[[_wb.datasets]]
= Data Sets


A *data set* is basically a set of columns populated with some rows, a matrix of data composed of timestamps, texts and numbers.
A data set can be stored in different systems: a database, an excel file, in memory or in a lot of other different systems.
On the other hand, a *data set definition* tells {CENTRAL} modules how such data can be accessed, read and parsed.

Notice, it's very important to make crystal clear the difference between a data set and its definition since {CENTRAL} does not take care of storing any data, it just provides an standard way to define access to those data sets regardless where the data is stored.

Let's take for instance the data stored in a remote database.
A valid data set could be, for example, an entire database table or the result of an SQL query.
In both cases, the database will return a bunch of columns and rows.
Now, imagine we want to get access to such data to feed some charts in a new {CENTRAL} page.
First thing is to create and register a data set definition in order to indicate the following:

* where the data set is stored,
* how can be accessed, read and parsed and
* what columns contains and of which type.


This chapter introduces the available {CENTRAL} tools for registering and handling data set definitions and how this definitions can be consumed in other {CENTRAL} modules like, for instance, the Page Editor.

[NOTE]
====
For simplicity sake we will be using the term _data set_ to refer to the actual data set definitions as _Data set_ and _Data set definition_ can be considered synonyms under the data set authoring context.
====

[[_wb.datasetauthoringperspective]]
== Data Set Authoring Page


Everything related to the authoring of data sets can be found under the _Data Set Authoring_ page which is accessible from the following top level menu entry: __Extensions>Data Sets__, as shown in the following screenshot.

.Data Set Authoring Page
image::Workbench/Authoring/DataSets/DataSetAuthoringPerspective.png[align="center"]


The center panel, shows a welcome screen, whilst the left panel contains the _Data Set Explorer_ listing all the data sets available

[NOTE]
====
This page is only intended to Administrator users, since defining data sets can be considered a low level task.
====

[[_wb.datasetexplorer]]
== Data Set Explorer


The _Data Set Explorer_ list the data sets present in the system.
Every time the user clicks on the data set it shows a brief summary alongside the following information:

.Data Set Explorer
image::Workbench/Authoring/DataSets/DataSetExplorer.png[align="center"]


* (1) A button for creating a new Data set
* (2) The list of currently available Data sets
* (3) An icon that represents the Data set's provider type (Bean, SQL, CSV, etc)
* (4) Details of current cache and refresh policy status
* (5) Details of current size on backend (unit as rows) and current size on client side (unit in bytes)
* (6) The button for editing the Data set. Once clicked the Data set editor screen is opened on the center panel


The next sections explains how to create, edit and fine tune data set definitions.

[[_wb.datasetcreation]]
== Data Set Creation


Clicking on the _New Data Set_ button opens a new screen from which the user is able to create a new data set definition in three steps:

* _Provider type selection_
+

Specify the kind of the remote storage system (BEAN, SQL, CSV, ElasticSearch)
* _Provider configuration_
+

Specify the attributes for being able to look up data from the remote system.
The configuration varies depending on the data provider type selected.
* _Data set columns & filter_
+

Live data preview, column types and initial filter configuration.


[[_wb.datasetcreationtypeselection]]
=== Step 1: Provider type selection


Allows the user's specify the type of data provider of the data set being created.

This screen lists all the current available data provider types and helper popovers with descriptions.
Each data provider is represented with a descriptive image:

.Provider type selection
image::Workbench/Authoring/DataSets/DataSetDefTypeSelection.png[align="center"]


Four types are currently supported:

* Bean (Java class) - To generate a data set directly from Java
* SQL - For getting data from any ANSI-SQL compliant database
* CSV - To upload the contents of a remote or local CSV file
* Elastic Search - To query and get documents stored on Elastic Search nodes as data sets


Once a type is selected, click on Next button to continue with the next workflow step.

[[_wb.datasetcreationconfiguration]]
=== Step 2: Configuration

.CSV Configuration
image::Workbench/Authoring/DataSets/DataSetDefConfigScreen.png[align="center"]


The provider type selected in the previous step will determine which configuration settings the system asks for.

.Configuration screen per data set type
image::Workbench/Authoring/DataSets/DataSetDefConfigTypes.png[align="center"]


[NOTE]
====
The UUID attribute is a read only field as it's generated by the system.
It's only intended for usage in API calls or specific operations.
====

[[_wb.datasetcreationpreview]]
=== Step 3: Data set columns and preview


After clicking on the _Test_ button (see previous step), the system executes a data set lookup test call in order to check if the remote system is up and the data is available.
If everything goes ok the user will see the following screen:

.Data set preview
image::Workbench/Authoring/DataSets/DataSetDefLivePreview.png[align="center"]


This screen shows a live data preview along with the columns the user wants to be part of the resulting data set.
The user can also navigate through the data and apply some changes to the data set structure.
Once finished, we can click on the _Save_ button in order to register the new data set definition.

We can also change the configuration settings at any time just by going back to the configuration tab.
We can repeat the _Configuration>Test>Preview_ cycle as may times as needed until we consider it's ready to be saved.

*Columns*

In the _Columns_ tab area the user can select what columns are part of the resulting data set definition.

.Data set columns
image::Workbench/Authoring/DataSets/DataSetDefColumns.png[align="center"]


* (1) To add or remove columns. Select only those columns you want to be part of the resulting data set
* (2) Use the drop down image selector to change the column type


A data set may only contain columns of any of the following 4 types:

* Label - For text values supporting group operations (similar to the SQL "group by" operator) which means you can perform data lookup calls and get one row per distinct value.
* Text - For text values NOT supporting group operations. Typically for modeling large text columns such as abstracts, descriptions and the like.
* Number - For numeric values. It does support aggregation functions on data lookup calls: sum, min, max, average, count, disctinct.
* Date - For date or timestamp values. It does support time based group operations by different time intervals: minute, hour, day, month, year, ...


No matter which remote system you want to retrieve data from, the resulting data set will always return a set of columns of one of the four types above.
There exists, by default, a mapping between the remote system column types and the data set types.
The user is able to modify the type for some columns, depending on the data provider and the column type of the remote system.
The system supports the following changes to column types:

* Label <> Text - Useful when we want to enable/disable the categorization (grouping) for the target column. For instance, imagine a database table called "document" containing a large text column called "abstract". As we do not want the system to treat such column as a "label" we might change its column type to "text". Doing so, we are optimizing the way the system handles the data set and
* Number <> Label - Useful when we want to treat numeric columns as labels. This can be used for instance to indicate that a given numeric column is not a numeric value that can be used in aggregation functions. Despite its values are stored as numbers we want to handle the column as a "label". One example of such columns are: an item's code, an appraisal id., ...


[NOTE]
====
BEAN data sets do not support changing column types as it's up to the developer to decide which are the concrete types for each column.
====

*Filter*

A data set definition may define a filter.
The goal of the filter is to leave out rows the user does not consider necessary.
The filter feature works on any data provider type and it lets the user to apply filter operations on any of the data set columns available.

.Data set filter
image::Workbench/Authoring/DataSets/DataSetDefFilter.png[align="center"]


While adding or removing filter conditions and operations, the preview table on central area is updated with live data that reflects the current filter status.

There exists two strategies for filtering data sets and it's also important to note that choosing between the two have important implications.
Imagine a dashboard with some charts feeding from a expense reports data set where such data set is built on top of an SQL table.
Imagine also we only want to retrieve the expense reports from the "London" office.
You may define a data set containing the filter "office=London" and then having several charts feeding from such data set.
This is the recommended approach.
Another option is to define a data set with no initial filter and then let the individual charts to specify their own filter.
It's up to the user to decide on the best approach.

Depending on the case it might be better to define the filter at a data set level for reusing across other modules.
The decision may also have impact on the performance since a filtered cached data set will have far better performance than a lot of individual non-cached data set lookup requests.
(See the next section for more information about caching data sets).

[NOTE]
====
Notice, for SQL data sets, the user can use both the filter feature introduced or, alternatively, just add custom filter criteria to the SQL sentence.
Although, the first approach is more appropriated for non technical users since they might not have the required SQL language skills.
====

[[_wb.datasetdefeditor]]
== Data set editor


To edit an existing data set definition go the data set explorer, expand the desired data set definition and click on the _Edit_ button.
This will cause a new editor panel to be opened and placed on the center of the screen, as shown in the next screenshot:

.Data set definition editor
image::Workbench/Authoring/DataSets/DataSetDefEditor.png[align="center"]


.Editor selector
image::Workbench/Authoring/DataSets/DataSetDefEditorSelector.png[align="center"]


* Save - To validate the current changes and store the data set definition.
* Delete - To remove permanently from storage the data set definition. Any client module referencing the data set may be affected.
* Validate - To check that all the required parameters exists and are correct, as well as to validate the data set can be retrieved with no issues.
* Copy - To create a brand new definition as a copy of the current one.


[NOTE]
====
Data set definitions are stored in the underlying GIT repository as JSON files.
Any action performed is registered in the repository logs so it is possible to audit the change log later on.
====

[[_wb.datasetadvancedsettings]]
== Advanced settings


In the _Advanced settings_ tab area the user can specify caching and refresh settings.
Those are very important for making the most of the system capabilities thus improving the performance and having better application responsive levels.

.Advanced settings
image::Workbench/Authoring/DataSets/DataSetDefAdvanced.png[align="center"]


* (1) To enable or disable the client cache and specify the maximum size (bytes).
* (2) To enable or disable the backend cache and specify the maximum cache size (number of rows).
* (3) To enable or disable automatic refresh for the Data set and the refresh period.
* (4) To enable or disable the refresh on stale data setting.


Let's dig into more details about the meaning of these settings.

[[_wb.datasetcachesettings]]
== Caching


The system provides caching mechanisms out-of-the-box for holding data sets and performing data operations using in-memory strategies.
The use of these features brings a lot of advantages, like reducing the network traffic, remote system payload, processing times etc.
On the other hand, it's up to the user to fine tune properly the caching settings to avoid hitting performance issues.

Two cache levels are supported:

* Client level
* Backend level


The following diagram shows how caching is involved in any data set operation:

.Data set caching
image::Workbench/Authoring/DataSets/DataSetCacheArchitecture.png[align="center"]


Any data look up call produces a resulting data set, so the use of the caching techniques determines where the data lookup calls are executed and where the resulting data set is located.

*Client cache*

If ON then the data set involved in a look up operation is pushed into the web browser so that all the components that feed from this data set *do not need to perform any requests to the backend* since data set operations are resolved at a client side:

* The data set is stored in the web browser's memory
* The client components feed from the data set stored in the browser
* Data set operations (grouping, aggregations, filters and sort) are processed within the web browser, by means of a Javascript data set operation engine.


If you know beforehand that your data set will remain small, you can enable the client cache.
It will reduce the number of backend requests, including the requests to the storage system.
On the other hand, if you consider that your data set will be quite big, disable the client cache so as to not hitting with browser issues such as slow performance or intermittent hangs.

*Backend cache*

Its goal is to provide a caching mechanism for data sets on backend side.

This feature allows to *reduce the number of requests to the remote storage system* , by holding the data set in memory and performing group, filter and sort operations using the in-memory engine.

It's useful for data sets that do not change very often and their size can be considered acceptable to be held and processed in memory.
It can be also helpful on low latency connectivity issues with the remote storage.
On the other hand, if your data set is going to be updated frequently, it's better to disable the backend cache and perform the requests to the remote storage on each look up request, so the storage system is in charge of resolving the data set lookup request.

[NOTE]
====
BEAN and CSV data providers relies by default on the backend cache, as in both cases the data set must be always loaded into memory in order to resolve any data lookup operation using the in-memory engine.
This is the reason why the backend settings are not visible in the Advanced settings tab.
====

[[_wb.datasetrefreshsettings]]
== Refresh


The refresh feature allows for the invalidation of any cached data when certain conditions are meet.

.Refresh settings
image::Workbench/Authoring/DataSets/DataSetDefRefreshSettings.png[align="center"]


* (1) To enable or disable the refresh feature.
* (2) To specify the refresh interval.
* (3) To enable or disable data set invalidation when the data is outdated.


The data set refresh policy is tightly related to data set caching, detailed in previous section.
This invalidation mechanism determines the cache life-cycle.

Depending on the nature of the data there exist three main use cases:

* *Source data changes predictable* - Imagine a database being updated every night. In that case, the suggested configuration is to use a "refresh interval = 1 day" and disable "refresh on stale data". That way, the system will always invalidate the cached data set every day. This is the right configuration when we know in advance that the data is going to change.
* *Source data changes unpredictable* - On the other hand, if we do not know whether the database is updated every day, the suggested configuration is to use a "refresh interval = 1 day" and enable "refresh on stale data". If so the system, before invalidating any data, will check for modifications. On data modifications, the system will invalidate the current stale data set so that the cache is populated with fresh data on the next data set lookup call.
* *Real time scenarios* - In real time scenarios caching makes no sense as data is going to be updated constantly. In this kind of scenarios the data sent to the client has to be constantly updated, so rather than enabling the refresh settings (remember this settings affect the caching, and caching is not enabled) it's up to the clients consuming the data set to decide when to refresh. When the client is a dashboard then it's just a matter of modifying the refresh settings in the Displayer Editor configuration screen and set a proper refresh period, "refresh interval = 1 second" for example.
