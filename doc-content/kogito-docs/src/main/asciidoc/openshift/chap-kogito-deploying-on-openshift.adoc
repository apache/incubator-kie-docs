[id="chap-kogito-deploying-on-openshift"]
= Deploying {PRODUCT} services on OpenShift
ifdef::context[:parent-context: {context}]
:context: kogito-deploying-on-openshift

// Purpose statement for the assembly
[role="_abstract"]
As a developer of business processes and decisions, you can deploy {PRODUCT} services on {OPENSHIFT} for cloud implementation. The {PRODUCT} Operator automates many of the deployment steps for you or guides you through the deployment process. You can use the {PRODUCT} command-line interface (CLI) to interact with the {PRODUCT} Operator for deployment tasks.

.Prerequisites
* {OPENSHIFT} 4.3 or later is installed.
* The OpenShift project for the deployment is created.

// Modules - concepts, procedures, refs, etc.
[id="con-kogito-on-ocp_{context}"]
== {PRODUCT} on {OPENSHIFT}

[role="_abstract"]
You can deploy {PRODUCT} services on {OPENSHIFT} for cloud implementation. In this architecture, {PRODUCT} services are deployed as OpenShift pods that you can scale up and down individually to provide as few or as many containers as required for a particular service. You can use standard OpenShift methods to manage the pods and balance the load.

To help you deploy your services on OpenShift, {PRODUCT} provides an operator and a command-line interface (CLI):

* *{PRODUCT} Operator*: An operator that guides you through the deployment process. The {PRODUCT} Operator is based on the https://sdk.operatorframework.io/[Operator SDK] and automates many of the deployment steps for you. For example, when you give the operator a link to the Git repository that contains your application, the operator can automatically configure the components required to build your project from source and deploy the resulting services.
* *{PRODUCT} command-line interface (CLI)*: A CLI tool that enables you to interact with the {PRODUCT} Operator for deployment tasks. The {PRODUCT} CLI also enables you to deploy {PRODUCT} services from source instead of relying on custom resources and YAML files. You can use the {PRODUCT} CLI as a command-line alternative for deploying {PRODUCT} services without the {OPENSHIFT} web console.

[id="proc-kogito-enabling-probes_{context}"]
== Probes for {PRODUCT} services on {OPENSHIFT}

[role="_abstract"]
The probes in {OPENSHIFT} and Kubernetes verify that an application is working or it needs to be restarted. For {PRODUCT} services on Quarkus and Spring Boot, probes interact with the application using an HTTP request, defaulting to the endpoints that are exposed by an extension. Therefore, to run your {PRODUCT} services on {OPENSHIFT} or Kubernetes, you must import the extensions to enable the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness, readiness, and startup probes] for your application.

=== Enabling probes for Quarkus-based {PRODUCT} services on {OPENSHIFT}

You can enable the probes for the Quarkus-based {PRODUCT} services on {OPENSHIFT}.

.Procedure
. In a command terminal, navigate to the `pom.xml` file of your {PRODUCT} project and add the following dependency for the https://quarkus.io/guides/microprofile-health[Quarkus `smallrye-health`] extension:

.SmallRye Health dependency for Quarkus applications on {OPENSHIFT}
[source,xml]
----
<dependencies>
  <dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-smallrye-health</artifactId>
  </dependency>
</dependencies>
----

=== Enabling probes for Spring Boot-based {PRODUCT} services on {OPENSHIFT}

You can enable the probes for the Spring Boot-based {PRODUCT} services on {OPENSHIFT}.

.Procedure
. In a command terminal, navigate to the `pom.xml` file of your {PRODUCT} project and add the following https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/reference/html/production-ready-features.html[Spring Boot actuator dependency]:

.Spring Boot actuator dependency for Spring Boot applications on {OPENSHIFT}
[source,xml]
----
<dependencies>
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
  </dependency>
</dependencies>
----

=== Setting custom probes for {PRODUCT} services on {OPENSHIFT}

You can also configure the custom endpoints for the liveness, readiness, and startup probes.

.Procedure
. Define the probes in the `KogitoRuntime` YAML file of your {PRODUCT} project, as shown in the following example:

[source,yaml,subs="attributes+"]
.Example {PRODUCT} service custom resource with custom probe endpoints
----
apiVersion: app.kiegroup.org/v1beta1 # Kogito API for this service
kind: KogitoRuntime
metadata:
  name: process-quarkus-example # Application name
spec:
  replicas: 1
  probes:
    livenessProbe:
      httpGet:
        path: /probes/live # Liveness endpoint
        port: 8080
    readinessProbe:
      httpGet:
        path: /probes/ready # Readiness endpoint
        port: 8080
    startupProbe:
      tcpSocket:
        port: 8080
----
////
[id="proc-kogito-deploying-on-ocp-console_{context}"]
== Deploying {PRODUCT} services on OpenShift using the OpenShift web console

[role="_abstract"]
After you create your {PRODUCT} services as part of a business application, you can use the {OPENSHIFT} web console to deploy your services. The {PRODUCT} Operator page in the OpenShift web console guides you through the deployment process. The {PRODUCT} Operator is based on the https://sdk.operatorframework.io/[Operator SDK] and automates many of the deployment steps for you. For example, when you give the operator a link to the Git repository that contains your application, the operator can automatically configure the components required to build your project from source and deploy the resulting services.

.Prerequisites
* The application with your {PRODUCT} services is in a Git repository that is reachable from your OpenShift environment.
* You have access to the OpenShift web console with `cluster-admin` permissions.
* (Quarkus only) The `pom.xml` file of your {PRODUCT} project contains the following dependency for the https://quarkus.io/guides/microprofile-health[Quarkus `smallrye-health` extension]. This extension enables the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness, readiness, and startup probes] that are required for Quarkus-based projects on {OPENSHIFT} or Kubernetes.
+
.SmallRye Heath dependency for Quarkus applications on {OPENSHIFT}
[source,xml]
----
<dependencies>
  <dependency>
    <groupId>io.quarkus</groupId>
    <artifactId>quarkus-smallrye-health</artifactId>
  </dependency>
</dependencies>
----
* (Spring Boot only) The `pom.xml` file of your {PRODUCT} project contains the following https://docs.spring.io/spring-boot/docs/2.3.0.RELEASE/reference/html/production-ready-features.html[Spring Boot actuator dependency]. This dependency enables the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness, readiness, and startup probes] that are required for Spring Boot-based projects on {OPENSHIFT} or Kubernetes.
+
.Spring Boot actuator dependency for Spring Boot applications on {OPENSHIFT}
[source,xml]
----
<dependencies>
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
  </dependency>
</dependencies>
----

.Procedure
. In the OpenShift web console, go to *Operators* -> *OperatorHub* in the left menu, search for and select *Kogito*, and follow the on-screen instructions to install the latest operator version.
. After you install the {PRODUCT} Operator, in the OpenShift web console, go to *Operators* -> *Installed Operators* and select *Kogito*.
. In the operator page, select the *Kogito Build* tab and click *Create KogitoBuild* to create the {PRODUCT} build definition.
+
.Create a {PRODUCT} build definition
image::kogito/openshift/kogito-ocp-create-build.png[Image of Kogito build page in web console]
. In the application window, use the *Form View* or *YAML View* to configure the build definition.
+
--
At a minimum, define the application configurations shown in the following example YAML file:

.Example YAML definition for an application with {PRODUCT} build
[source,yaml,subs="attributes+"]
----
apiVersion: app.kiegroup.org/v1beta1 # {PRODUCT} API for this service
kind: KogitoBuild  # Application type
metadata:
  name: example-quarkus  # Application name
  namespace: kogito  # OpenShift project namespace
spec:
  type: RemoteSource
  gitSource:
    uri: 'https://github.com/kiegroup/kogito-examples'  # Git repository containing application (uses default branch)
    contextDir: process-quarkus-example  # Git folder location of application
----

[NOTE]
====
If you have configured an internal Maven repository, you can use it as a Maven mirror service and specify the Maven mirror URL in your {PRODUCT} build definition to substantially shorten build time:

[source,yaml]
----
spec:
  mavenMirrorURL: http://nexus3-nexus.apps-crc.testing/repository/maven-public/
----

For more information about internal Maven repositories, see the https://maven.apache.org/guides/introduction/introduction-to-repositories.html[Apache Maven] documentation.
====
--
. After you define your application data, click *Create* to generate the {PRODUCT} build.
+
--
Your application is listed in the *{PRODUCT}Builds* page:

.New {PRODUCT} build instance
image::kogito/openshift/kogito-ocp-create-build-listed.png[Image of Kogito build listed in web console]

You can select the application name to view or modify application settings and YAML details:

.View {PRODUCT} build details
image::kogito/openshift/kogito-ocp-build-details.png[Image of Kogito service details page in web console]

--
. In the operator page, select the *Kogito Service* tab and click *Create KogitoRuntime* to create the {PRODUCT} service definition.
+
.Create a {PRODUCT} service definition
image::kogito/openshift/kogito-ocp-create-runtime.png[Image of Kogito service page in web console]
. In the application window, use the *Form View* or *YAML View* to configure the service definition.
+
--
At a minimum, define the application configurations shown in the following example YAML file:

.Example YAML definition for an application with {PRODUCT} services
[source,yaml,subs="attributes+"]
----
apiVersion: app.kiegroup.org/v1beta1 # {PRODUCT} API for this service
kind: KogitoRuntime  # Application type
metadata:
  name: example-quarkus  # Application name
  namespace: kogito  # OpenShift project namespace
----

--
. After you define your application data, click *Create* to generate the {PRODUCT} service.
+
--
Your application is listed in the {PRODUCT} service page:

.New {PRODUCT} service instance
image::kogito/openshift/kogito-ocp-create-runtime-listed.png[Image of Kogito service listed in web console]

You can select the application name to view or modify application settings and YAML details:

.View {PRODUCT} service details
image::kogito/openshift/kogito-ocp-runtime-details.png[Image of Kogito service details page in web console]

--
. In the left menu of the web console, go to *Builds* -> *Builds* to view the status of your application build.
+
--
You can select a specific build to view build details:

.View {PRODUCT} service build details
image::kogito/openshift/kogito-ocp-app-build-details.png[Image of Kogito service build details page in web console]

[NOTE]
====
For every {PRODUCT} service that you create for OpenShift deployment, two builds are generated and listed in the *Builds* page in the web console: a traditional runtime build and a Source-to-Image (S2I) build with the suffix `-builder`. The S2I mechanism builds the application in an OpenShift build and then passes the built application to the next OpenShift build to be packaged into the runtime container image. The {PRODUCT} S2I build configuration also enables you to build the project directly from a Git repository on the OpenShift platform.
====

--
. After the application build is complete, go to *Workloads* -> *Deployments* to view the application deployments, pod status, and other details.
+
--
You can select the application name to increase or decrease the pod count or modify deployment settings:

.View {PRODUCT} service deployment details
image::kogito/openshift/kogito-ocp-service-deployment-details.png[Image of Kogito service deployment details page in web console]
--
. After your {PRODUCT} service is deployed, in the left menu of the web console, go to *Networking* -> *Routes* to view the access link to the deployed application.
+
--
You can select the application name to view or modify route settings:

.View {PRODUCT} service route details
image::kogito/openshift/kogito-ocp-service-route-details.png[Image of Kogito service route details page in web console]

With the application route, you can integrate your {PRODUCT} services with your business automation solutions as needed.
--
////

////

[id="con-kogito-operator-installation_{context}"]
== {PRODUCT} Operator installation

<Start here>

[id="proc-kogito-operator-installation-openshift_{context}"]
=== Installing the {PRODUCT} Operator on OpenShift

<Start here>

[id="proc-kogito-operator-installation-kubernetes-engine_{context}"]
=== Installing the {PRODUCT} Operator on OpenShift Kubernetes Engine

<Start here>

[id="proc-kogito-operator-installation-kubernetes_{context}"]
=== Installing the {PRODUCT} Operator on Kubernetes

<Start here>

[id="proc-kogito-operator-verify-installation_{context}"]
== Verifying the {PRODUCT} Operator installation

<Start here>

[id="proc-kogito-cli-installation_{context}"]
== Installing the {PRODUCT} CLI

<Start here>

[id="ref-kogito-cli-installation-troubleshooting_{context}"]
=== {PRODUCT} CLI installation troubleshooting

<Start here>
////

[id="ref-kogito-cli-operations_{context}"]
=== Supported commands and operations in {PRODUCT} CLI

[role="_abstract"]
The {PRODUCT} command-line interface (CLI) supports the following operations on Linux, Mac, and Windows operating systems. The syntax of some operations might vary for Windows users, such as a backslash (`\`) instead of a forward slash (`/`).

.Supported commands and operations in the {PRODUCT} CLI
[cols="30%,35%,35%", options="header"]
|===
|Operation
|{PRODUCT} CLI command
|Example

|Create an OpenShift project for {PRODUCT} services.
|`{PRODUCT_INIT} new-project __PROJECT_NAME__`
|`{PRODUCT_INIT} new-project kogito-travel-agency`

|Connect the {PRODUCT} Operator to an existing OpenShift project with {PRODUCT} services.
|`{PRODUCT_INIT} use-project __PROJECT_NAME__`
|`{PRODUCT_INIT} use-project kogito-travel-agency`

|Install the Infinispan infrastructure for process data persistence in {PRODUCT} services.
|`{PRODUCT_INIT} install infra __INFINISPAN_INFRA_NAME__ --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan`
|`{PRODUCT_INIT} install infra kogito-infinispan-infra --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan`

|Install the Apache Kafka infrastructure for messaging in {PRODUCT} services.
|`{PRODUCT_INIT} install infra __KAFKA_INFRA_NAME__ --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka`
|`{PRODUCT_INIT} install infra kogito-kafka-infra --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka`

|Install the {PRODUCT} Data Index Service for data management in {PRODUCT} services and provision the Data Index Service to connect to the specified Infinispan and Kafka infrastructures.
|`{PRODUCT_INIT} install data-index --infra __INFINISPAN_INFRA_NAME__ --infra __KAFKA_INFRA_NAME__`
|`{PRODUCT_INIT} install data-index --infra kogito-infinispan-infra --infra kogito-kafka-infra`

|Install the {PRODUCT} Jobs Service for job scheduling in {PRODUCT} services and provision the Jobs Service to connect to the specified Infinispan and Kafka infrastructures.
|`{PRODUCT_INIT} install jobs-service --infra __INFINISPAN_INFRA_NAME__ --infra __KAFKA_INFRA_NAME__`
|`{PRODUCT_INIT} install jobs-service --infra kogito-infinispan-infra --infra kogito-kafka-infra`

|Install the {PRODUCT} Management Console for managing process instance details in {PRODUCT} services.
|`{PRODUCT_INIT} install mgmt-console`
|`{PRODUCT_INIT} install mgmt-console`

|Install the {PRODUCT} Trusty Service to store tracing events and provide advanced analytical capabilities in {PRODUCT} services, and provision the Trusty Service to connect to the specified Infinispan and Kafka infrastructures.
|`{PRODUCT_INIT} install trusty --infra __INFINISPAN_INFRA_NAME__ --infra __KAFKA_INFRA_NAME__`
|`{PRODUCT_INIT} install trusty --infra kogito-infinispan-infra --infra kogito-kafka-infra`

|Install the {PRODUCT} Explainability Service to analyze decisions made in {PRODUCT} services and provision the Explainability Service to connect to the specified Kafka infrastructure.
|`{PRODUCT_INIT} install explainability --infra __KAFKA_INFRA_NAME__`
|`{PRODUCT_INIT} install explainability --infra kogito-kafka-infra`

|Install the {PRODUCT} Task Console for viewing and interacting with user tasks in {PRODUCT} process services.
|`{PRODUCT_INIT} install task-console`
|`{PRODUCT_INIT} install task-console`

|Install {PRODUCT} supporting services or infrastructure components for a specified {PRODUCT} project. Use this syntax if you did not use the `new-project` or `use-project` commands to connect the {PRODUCT} Operator to a specified project.
|`{PRODUCT_INIT} install __KOGITO_SUPPORTING_SERVICE__ --infra __KOGITO_INFRA_NAME__ -p __PROJECT_NAME__`

`{PRODUCT_INIT} install infra __KOGITO_INFRA_NAME__ __KOGITO_INFRA_DETAILS__ -p __PROJECT_NAME__`

|`{PRODUCT_INIT} install data-index --infra kogito-infinispan-infra --infra kogito-kafka-infra -p kogito-travel-agency`

`{PRODUCT_INIT} install infra kogito-infinispan-infra --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan -p kogito-travel-agency`

`{PRODUCT_INIT} install infra kogito-kafka-infra --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka -p kogito-travel-agency`

|Create a {PRODUCT} service definition from a local source or a Git repository and deploy the service. In a local directory source, if the `pom.xml` file is present, then the entire directory is zipped and uploaded to OCP and the build is initiated. However, if the `pom.xml` is not present, then only supported extension files (including `.dmn`, `.drl`, `.bpmn`, `.bpmn2`, `.properties`, `.sw.json`, and `.sw.yaml`) are uploaded from the directory to initiate the build. In a binary build configuration, this command creates the service definition but does not deploy the service.
|`{PRODUCT_INIT} deploy-service __SERVICE_NAME__`

`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ __GIT_REPOSITORY_URL__ --context-dir __PROJECT_DIRECTORY__`
|`{PRODUCT_INIT} deploy-service travels`

`{PRODUCT_INIT} deploy-service travels \https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels`

|Enable Infinispan persistence and Apache Kafka messaging for a {PRODUCT} service during deployment. Use this command if you installed the relevant infrastructures using the {PRODUCT} Operator. In a binary build configuration, this command creates the service definition but does not deploy the service.
|`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ --infra __INFINISPAN_INFRA_NAME__ --infra __KAFKA_INFRA_NAME__`

`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ __GIT_REPOSITORY_URL__ --context-dir __PROJECT_DIRECTORY__ --infra __INFINISPAN_INFRA_NAME__ --infra __KAFKA_INFRA_NAME__`
|`{PRODUCT_INIT} deploy-service travels --infra kogito-infinispan-infra --infra kogito-kafka-infra`

`{PRODUCT_INIT} deploy-service travels \https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels --infra kogito-infinispan-infra --infra kogito-kafka-infra`

|Create a {PRODUCT} service definition from a local or Git source and deploy the service using a native build.
|`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ --native`

`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ __GIT_REPOSITORY_URL__ --context-dir __PROJECT_DIRECTORY__ --native`
|`{PRODUCT_INIT} deploy-service travels --native`

`{PRODUCT_INIT} deploy-service travels \https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels --native`

|Upload a {PRODUCT} service file, such as a Decision Model and Notation (DMN) or Business Process Model and Notation (BPMN) file, or a file directory with multiple files to an OpenShift Cluster and trigger a new Source-to-Image (S2I) build. For single files, you can specify a local file system path or Git repository URL. For file directories, you can specify a local file system path only.
|`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ __PATH_TO_FILE_OR_DIR__`

`{PRODUCT_INIT} deploy-service __SERVICE_NAME__ __GIT_FILE_URL__`

|`kogito deploy-service travels /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2`

`kogito deploy-service travels /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels`

`kogito deploy-service travels \https://github.com/kiegroup/kogito-examples/blob/stable/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2`

|Delete a {PRODUCT} service.
|`{PRODUCT_INIT} delete-service __SERVICE_NAME__`
|`{PRODUCT_INIT} delete-service travels`
|===

[id="con-kogito-operator-deployment-options_{context}"]
== OpenShift build options with the {PRODUCT} Operator and CLI

[role="_abstract"]
The {PRODUCT} Operator and command-line interface (CLI) support the following options for building and deploying {PRODUCT} services on {OPENSHIFT}:

* Git source build and deployment
* Binary build and deployment
* File build and deployment
ifdef::KOGITO-COMM[]
* Native build and deployment
endif::[]

{PRODUCT} provides the relevant Source-to-Image (S2I) build configuration depending the deployment option that you use.

NOTE: For all deployment options, you must be logged in to the relevant OpenShift cluster using the `oc login` command.

=== Git source build and deployment

In most use cases, you can use the standard runtime build and deployment method to deploy {PRODUCT} services on OpenShift from a Git repository source, as shown in the following examples. These examples are based on the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application.

.Example {PRODUCT} service deployment from existing namespace
[source,subs="attributes+,+quotes"]
----
// Uses the provisioned namespace in your OpenShift cluster
$ {PRODUCT_INIT} use-project __PROJECT_NAME__

// Deploys a new {PRODUCT} service from a Git source
$ {PRODUCT_INIT} deploy-service travels https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels
----

The {PRODUCT} Operator uses the default branch in the specified Git repository, usually `master`.

Alternatively, you can generate a new namespace in your cluster during deployment:

.Example {PRODUCT} service deployment from new namespace
[source,subs="attributes+,+quotes"]
----
// Creates a new namespace in your cluster
$ {PRODUCT_INIT} new-project __NEW_PROJECT_NAME__

// Deploys a new {PRODUCT} service from a Git source
$ {PRODUCT_INIT} deploy-service travels https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels
----

NOTE: If you are developing or testing your {PRODUCT} service locally, you can use the binary build or file build option to build and deploy from a local source instead of from a Git repository.

=== Binary build and deployment

OpenShift builds can require extensive amounts of time. As a faster alternative for building and deploying your {PRODUCT} services on OpenShift, you can use a binary build. In a binary build, you build the application locally and push the built application to an OpenShift `BuildConfig` configuration to be packaged into the runtime container image.

The following example creates a {PRODUCT} service from a local directory, builds the project binaries, and deploys the binary build to OpenShift. This example is based on the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application.

.Example {PRODUCT} service deployment from binary build
[source,subs="attributes+,+quotes"]
----
// Creates the {PRODUCT} service
$ cd ~/kogito-travel-agency/extended/travels
$ kogito deploy-service travels

// Builds the project and generates binary resources in `target` folder
$ mvn clean package

// Deploys to OpenShift using binary build
$ oc start-build travels-binary --from-dir=target/

Uploading directory "target/" as binary input for the build ...
....
Uploading finished
build.build.openshift.io/travels-1 started
----

In this example, the following resources are generated in the `target` folder to prepare for the binary build:

* `quarkus-app/app/travels.jar`: Standard JAR file that contains the classes and resources of the project.
* `quarkus-app/quarkus`: Contains the generated resources required by the Quarkus application to run.
* `quarkus-app/lib`: Directory that contains the project dependencies.
* `quarkus-app/quarkus-run.jar`: Executable JAR file for the project. Note that `quarkus-run.jar` is not an uber-JAR file, and also requires the folders `quarkus-app/app`, `quarkus-app/quarkus`, and `quarkus-app/lib`.

NOTE: The {PRODUCT} images are handling the binary build directly from the `target` folder, and the content of the `target/quarkus-app` folder is copied directly.

After the binary build is complete, the result is pushed to the `travels` Image Stream that was created by the {PRODUCT} Operator and triggers a new deployment.

=== File build and deployment

You can build and deploy your {PRODUCT} services from a single file, such as a Decision Model and Notation (DMN), Business Process Model and Notation (BPMN or BPMN2), Drools Rule Language (DRL), or properties file, or from a directory with multiple files. You can specify a single file from your local file system path or from a Git repository URL, or specify a file directory from a local file system path only. When you upload the file or directory to an OpenShift cluster, a new Source-to-Image (S2I) build is automatically triggered.

NOTE: You cannot upload a file directory from a Git repository. The file directory must be in your local file system. However, you can upload single files from either a Git repository or your local file system.

The following examples upload a single BPMN file from a local directory or Git repository to an OpenShift cluster for an S2I build. These examples are based on the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application.

.Example {PRODUCT} service deployment from a local file
[source,subs="attributes+,+quotes"]
----
$ kogito deploy-service travels /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2

File found: /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2.
...
The requested file(s) was successfully uploaded to OpenShift, a build with this file(s) should now be running. To see the logs, run 'oc logs -f bc/kogito-travel-agency-builder -n kogito'
----

.Example {PRODUCT} service deployment from a Git repository file
[source,subs="attributes+,+quotes"]
----
$ kogito deploy-service travels https://github.com/kiegroup/kogito-examples/blob/stable/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2

Asset found: travels.bpmn2.
...
The requested file(s) was successfully uploaded to OpenShift, a build with this file(s) should now be running. To see the logs, run 'oc logs -f bc/kogito-travel-agency-builder -n kogito'
----

As instructed in the terminal output, you can run the following command to see the build logs:

.Example build log for BPMN file build
[source]
----
$ oc logs -f bc/kogito-travel-agency-builder -n kogito

Receiving source from STDIN as file travels.bpmn2
Using docker-registry.default.svc:5000/openshift/kogito-builder as the s2i builder image
----

The following examples upload multiple files within a local directory to an OpenShift cluster for an S2I build:

.Example {PRODUCT} service deployment from multiple files in a local directory
[source,subs="attributes+,+quotes"]
----
$ kogito deploy-service travels /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels

The provided source is a dir, packing files.
File(s) found: [/tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/flightBooking.bpmn2 /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/hotelBooking.bpmn2 /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2].
...
The requested file(s) was successfully uploaded to OpenShift, a build with this file(s) should now be running. To see the logs, run 'oc logs -f bc/travels-builder -n kogito'
----

For every valid file type (DMN, BPMN, BPMN2, DRL) in the specified directory, the {PRODUCT} CLI compresses the files and uploads them to the OpenShift cluster. Any other unsupported file types are not uploaded. To ensure that other file types are uploaded, consider using a source build or a binary build.

If you need to update an uploaded file or directory after you create the build, you can use the `oc start-build` command to re-upload the file or directory, as shown in the following example. An S2I build cannot identify only changed files in a directory, so you must re-upload the entire directory of files to update the build.

.Example command to re-upload a single local file to update the S2I build
----
$ oc start-build kogito-travel-agency-builder --from-file tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels/travels.bpmn2
----

.Example command to re-upload multiple files from a local directory to update the S2I build
----
$ oc start-build kogito-travel-agency-builder --from-dir tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels
----

If a build fails, use the OpenShift environment variable https://docs.openshift.com/container-platform/4.3/builds/basic-build-operations.html#builds-basic-access-build-verbosity_basic-build-operations[`BUILD_LOGLEVEL`] with the desired level as part of your deployment command, as shown in the following example:

.Example command to troubleshoot failed build from directory
[source]
----
$ kogito --verbose deploy-service travels /tmp/kogito-examples/kogito-travel-agency/extended/travels/src/main/resources/org/acme/travels --build-env BUILD_LOGLEVEL=5
----

ifdef::KOGITO-COMM[]
=== Native build and deployment

NOTE: Native build and deployment is for Quarkus only and requires GraalVM or Mandrel.

By default, the {PRODUCT} Operator and CLI build services with traditional `java` compilers to save time and resources. The final generated artifact is a JAR file for the relevant runtime (defaults to Quarkus) with dependencies in the image user's home directory, such as `/home/kogito/bin/lib`.

However, for {PRODUCT} services on Quarkus with GraalVM or Mandrel (required), the {PRODUCT} Operator and CLI can build the services in native mode for direct binary execution as native code. Building in native mode uses a very low footprint on the runtime, although many resources are consumed during build time.

For more information about native build performance, see the GraalVM https://www.graalvm.org/docs/examples/java-performance-examples/[Performance examples for Java].

For more information about ahead-of-time (AOT) compilation, see the https://www.graalvm.org/docs/reference-manual/aot-compilation/[GraalVM Native Image] documentation.

The following examples build a {PRODUCT} service on Quarkus in native mode using the `--native` parameter. These examples are based on the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application.

.Example {PRODUCT} service native build from a local source directory
[source,subs="attributes+,+quotes"]
----
$ cd ~/kogito-travel-agency/extended/travels
$ {PRODUCT_INIT} deploy-service travels --native
----

.Example {PRODUCT} service native build from a Git repository source directory
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} deploy-service travels https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels --native
----

In {PRODUCT} Operator tests, native builds take approximately 10 minutes and the build pod can consume up to 10GB of RAM and 1.5 CPU cores.

By default, a {PRODUCT} project does not contain resource requests or limits. As a result, a native build might be terminated due to insufficient memory. To prevent this behavior, you can create a minimum memory request configuration for the {PRODUCT} application build, ensuring that the build pod is allocated on an OpenShift node with enough free memory. The side effect of this configuration is that OpenShift prioritizes the build pod.

For more information about OpenShift pod prioritization based on pod requests and limits, see https://docs.okd.io/3.11/dev_guide/compute_resources.html#quality-of-service-tiers[Quality of Service Tiers] in the OpenShift documentation.

The following example is a memory request configuration for the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application:

.Example memory request configuration for `kogito-travel-agency`
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoBuild
metadata:
  name: travels
  namespace: kogito
spec:
  type: RemoteSource
  gitSource:
    contextDir: kogito-travel-agency/extended/travels
    uri: "https://github.com/kiegroup/kogito-examples/"
  resources:
    requests:
        memory: "4Gi"
----

IMPORTANT: Ensure that you have these resources available on your OpenShift nodes when you run native builds. If the resources are not available, the S2I build fails. You can verify currently allocated and total resources of your nodes by using the command `oc describe nodes` invoked with `admin` permission.

You can limit the maximum heap space for the JVM used for a native build. You can apply the limitation by setting the `quarkus.native.native-image-xmx` property in the `application.properties` file of your {PRODUCT} project. In this case, the build pod requires roughly `-Xmx` plus 2 GB of memory. The `-Xmx` value depends on the complexity of the application. For example, for the https://github.com/kiegroup/kogito-examples/tree/stable/process-quarkus-example[`process-quarkus-example`] example application, the `-Xmx` value `2g` is sufficient, resulting in the builder pod consuming up to 4.2 GB of memory.

You can also set resource limits for a native build pod. In the https://github.com/kiegroup/kogito-examples/tree/stable/process-quarkus-example[`process-quarkus-example`] application, 80 percent of the memory limit is used for heap space in the JVM responsible for the native build. If the computed heap space limit for the JVM is less than 1024 MB, then all the memory from resource limits is used.

The following example is a memory limit configuration for the https://github.com/kiegroup/kogito-examples/tree/stable/process-quarkus-example[`process-quarkus-example`] example application:

.Example memory request configuration for `process-quarkus-example`
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoBuild
metadata:
  name: process-quarkus-example
  namespace: kogito
spec:
  type: RemoteSource
  gitSource:
    contextDir: process-quarkus-example
    uri: "https://github.com/kiegroup/kogito-examples/"
  resources:
    limits:
        memory: "4Gi"
----
endif::[]

[id="proc-kogito-build-native_{context}"]
==== Building a native image with {PRODUCT}

[role="_abstract"]
You can build an application as a native executable in {PRODUCT} using GraalVM or Mandrel.

.Prerequisites
* JDK 11 or later is installed.
* Apache Maven 3.6.2 or later is installed.
* Optional: https://quarkus.io/guides/cli-tooling[Quarkus CLI] is installed.
+
NOTE: Native build and deployment is Quarkus-only and requires GraalVM or Mandrel.

.Procedure
. Create a {PRODUCT} project using the Maven plugin or Quarkus CLI.
+
--
.Example project creation using Quarkus Maven plugin
[source,subs="attributes+"]
----
mvn io.quarkus:quarkus-maven-plugin:create \
   -DprojectGroupId=com.company \
   -DprojectArtifactId=sample-kogito \
   -Dextensions="kogito"
----

.Example project creation using Quarkus CLI
[source,subs="attributes+"]
----
quarkus create -x kogito sample-kogito
----

In the previous example, an application named `sample-kogito` is created. The `sample-kogito` application is pre-configured to support the native executables.

For more information about creating a {PRODUCT} project, see {URL_CREATING_RUNNING}#chap-kogito-creating-running[{CREATING_RUNNING}].
--

. Build a native image using Maven or Quarkus CLI.
+
--
.Example of building a native image using Maven plugin
[source]
----
$ mvn verify -Dnative
# running
$ ./target/sample-kogito-1.0-SNAPSHOT-runner
----

.Example of building a native image using the Quarkus CLI
[source]
----
$ quarkus build --native
# running
$ ./target/sample-kogito-1.0-SNAPSHOT-runner
----
--

. Build a native executable using a container image or Mandrel as follows:
+
--
* You can build a native executable using a container builder image, in which you do not need to insrall GraalVM. However, a container that contains GraalVM is pulled automatically. Note that the native executable targets a Linux kernel by default. For example:

.Example of building a native image using a container image
[source]
----
$ mvn verify -Dnative -Dquarkus.native.container-build
# alternatively
$ quarkus build --native -Dquarkus.native.container-build=true
----

* Alternatively, run the following command to use Mandrel instead of GraalVM to build a native image using container image:

.Example of building a native image using a container image (Mandrel)
[source]
----
$ mvn verify -Dnative -Dquarkus.native.container-build -Dquarkus.native.builder-image=quay.io/quarkus/ubi-quarkus-mandrel:21.1-java11
# alternatively
$ quarkus build --native -Dquarkus.native.container-build -Dquarkus.native.builder-image=quay.io/quarkus/ubi-quarkus-mandrel:21.1-java11
----

For more information about building a native image, see https://quarkus.io/guides/building-native-image#container-runtime[Quarkus - Building a Native Executable].
--

. Create a container image using Quarkus.
+
For more information about creating a container image using Quarkus, see https://quarkus.io/guides/container-image[extensions for building (and pushing) container images].

. Deploy the container image using `kubctl` or `oc`.

[id="con-kogito-operator-deployment-configs_{context}"]
== {PRODUCT} service properties configuration

[role="_abstract"]
When a {PRODUCT} service is deployed, a `configMap` resource is created for the `application.properties` configuration of the {PRODUCT} service.

The name of the `configMap` resource consists of the name of the {PRODUCT} service and the suffix `-properties`, as shown in the following example:

.Example `configMap` resource generated during {PRODUCT} service deployment
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: kogito-travel-agency-properties
data:
  application.properties : |-
    property1=value1
    property2=value2
----

The `application.properties` data of the `configMap` resource is mounted in a volume to the container of the {PRODUCT} service. Any runtime properties that you add to the `application.properties` section override the default application configuration properties of the {PRODUCT} service.

When the `application.properties` data of the `configMap` is changed, a rolling update modifies the deployment and configuration of the {PRODUCT} service.

////
[id="con-kogito-services-exposure_{context}"]
== {PRODUCT} services exposure in Openshift and Kubernetes

<Start here>

[id="proc-kogito-services-exposure-routes_{context}"]
=== Exposing {PRODUCT} services using routes

<Start here>

[id="proc-kogito-services-exposure-nginx-ingress_{context}"]
=== Exposing Kogito services using NGINX Ingress

<Start here>

[id="con-custom-kogito-service-deployment-options_{context}"]
== Custom {PRODUCT} service deployment options with {PRODUCT} Operator and CLI

<Start here>

[id="proc-custom-kogito-service-deployment-kubernetes-openshift-cluster_{context}"]
=== Deploying a custom {PRODUCT} service in Kubernetes or OpenShift cluster

<Start here>

==== Deploying a custom {PRODUCT} service in a Kubernetes cluster using custom configurations

<Start here>
////

[id="proc-kogito-service-deployment-openshift-web-console_{context}"]
=== Deploying a {PRODUCT} service using OpenShift web console

[role="_abstract"]
After you create your {PRODUCT} services as part of a business application, you can use the {OPENSHIFT} web console to deploy your services. The {PRODUCT} Operator page in the OpenShift web console guides you through the deployment process. The {PRODUCT} Operator is based on the https://sdk.operatorframework.io/[Operator SDK] and automates many of the deployment steps for you. For example, when you give the operator a link to the Git repository that contains your application, the operator can automatically configure the components required to build your project from source and deploy the resulting services.

.Prerequisites
* The application with your {PRODUCT} services is in a Git repository that is reachable from your OpenShift environment.
* You have access to the OpenShift web console with `cluster-admin` permissions.
* (Quarkus only) The `pom.xml` file of your {PRODUCT} project contains the following dependency for the https://quarkus.io/guides/microprofile-health[Quarkus `smallrye-health` extension]. This extension enables the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness and readiness probes] that are required for Quarkus-based projects on OpenShift or Kubernetes.
+
.SmallRye Heath dependency for Quarkus applications on OpenShift
[source,xml]
----
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-smallrye-health</artifactId>
</dependency>
----

.Procedure
. In the OpenShift web console, go to *Operators* -> *OperatorHub* in the left menu, search for and select *Kogito*, and follow the on-screen instructions to install the latest operator version.
. After you install the {PRODUCT} Operator, in the OpenShift web console, go to *Operators* -> *Installed Operators* and select *Kogito*.
. In the operator page, select the *Kogito Build* tab and click *Create KogitoBuild* to create the {PRODUCT} build definition.
+
.Create a {PRODUCT} build definition
image::kogito/openshift/kogito-ocp-create-build.png[Image of Kogito build page in web console]
. In the application window, use the *Form View* or *YAML View* to configure the build definition.
+
--
At a minimum, define the application configurations shown in the following example YAML file:

.Example YAML definition for an application with {PRODUCT} build
[source,yaml,subs="attributes+"]
----
apiVersion: app.kiegroup.org/v1beta1 # {PRODUCT} API for this service
kind: KogitoBuild  # Application type
metadata:
  name: example-quarkus  # Application name
  namespace: kogito  # OpenShift project namespace
spec:
  type: RemoteSource
  gitSource:
    uri: 'https://github.com/kiegroup/kogito-examples'  # Git repository containing application (uses default branch)
    contextDir: process-quarkus-example  # Git folder location of application
----

[NOTE]
====
If you have configured an internal Maven repository, you can use it as a Maven mirror service and specify the Maven mirror URL in your {PRODUCT} build definition to substantially shorten build time:

[source,yaml]
----
spec:
  mavenMirrorURL: http://nexus3-nexus.apps-crc.testing/repository/maven-public/
----

For more information about internal Maven repositories, see the https://maven.apache.org/guides/introduction/introduction-to-repositories.html[Apache Maven] documentation.
====
--
. After you define your application data, click *Create* to generate the {PRODUCT} build.
+
--
Your application is listed in the *{PRODUCT}Builds* page:

.New {PRODUCT} build instance
image::kogito/openshift/kogito-ocp-create-build-listed.png[Image of Kogito build listed in web console]

You can select the application name to view or modify application settings and YAML details:

.View {PRODUCT} build details
image::kogito/openshift/kogito-ocp-build-details.png[Image of Kogito service details page in web console]

--
. In the operator page, select the *Kogito Service* tab and click *Create KogitoRuntime* to create the {PRODUCT} service definition.
+
.Create a {PRODUCT} service definition
image::kogito/openshift/kogito-ocp-create-runtime.png[Image of Kogito service page in web console]
. In the application window, use the *Form View* or *YAML View* to configure the service definition.
+
--
At a minimum, define the application configurations shown in the following example YAML file:

.Example YAML definition for an application with {PRODUCT} services
[source,yaml,subs="attributes+"]
----
apiVersion: app.kiegroup.org/v1beta1 # {PRODUCT} API for this service
kind: KogitoRuntime  # Application type
metadata:
  name: example-quarkus  # Application name
  namespace: kogito  # OpenShift project namespace
----

--
. After you define your application data, click *Create* to generate the {PRODUCT} service.
+
--
Your application is listed in the {PRODUCT} service page:

.New {PRODUCT} service instance
image::kogito/openshift/kogito-ocp-create-runtime-listed.png[Image of Kogito service listed in web console]

You can select the application name to view or modify application settings and YAML details:

.View {PRODUCT} service details
image::kogito/openshift/kogito-ocp-runtime-details.png[Image of Kogito service details page in web console]

--
. In the left menu of the web console, go to *Builds* -> *Builds* to view the status of your application build.
+
--
You can select a specific build to view build details:

.View {PRODUCT} service build details
image::kogito/openshift/kogito-ocp-app-build-details.png[Image of Kogito service build details page in web console]

[NOTE]
====
For every {PRODUCT} service that you create for OpenShift deployment, two builds are generated and listed in the *Builds* page in the web console: a traditional runtime build and a Source-to-Image (S2I) build with the suffix `-builder`. The S2I mechanism builds the application in an OpenShift build and then passes the built application to the next OpenShift build to be packaged into the runtime container image. The {PRODUCT} S2I build configuration also enables you to build the project directly from a Git repository on the OpenShift platform.
====

--
. After the application build is complete, go to *Workloads* -> *Deployments* to view the application deployments, pod status, and other details.
+
--
You can select the application name to increase or decrease the pod count or modify deployment settings:

.View {PRODUCT} service deployment details
image::kogito/openshift/kogito-ocp-service-deployment-details.png[Image of Kogito service deployment details page in web console]
--
. After your {PRODUCT} service is deployed, in the left menu of the web console, go to *Networking* -> *Routes* to view the access link to the deployed application.
+
--
You can select the application name to view or modify route settings:

.View {PRODUCT} service route details
image::kogito/openshift/kogito-ocp-service-route-details.png[Image of Kogito service route details page in web console]

With the application route, you can integrate your {PRODUCT} services with your business automation solutions as needed.
--

[id="proc-kogito-deploying-on-ocp-kogito-cli_{context}"]
=== Deploying {PRODUCT} services on OpenShift or Kubernetes using the {PRODUCT} CLI

[role="_abstract"]
The {PRODUCT} command-line interface (CLI) enables you to interact with the {PRODUCT} Operator for deployment tasks. The {PRODUCT} CLI also enables you to deploy {PRODUCT} services from source instead of relying on custom resources and YAML files. You can use the {PRODUCT} CLI as a command-line alternative for deploying {PRODUCT} services without the {OPENSHIFT} web console.

.Prerequisites
* The `oc` OpenShift CLI is installed and you are logged in to the relevant OpenShift cluster. For `oc` installation and login instructions, see the
ifdef::KOGITO-ENT[]
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html/cli_tools/openshift-cli-oc[OpenShift documentation].
endif::[]
ifdef::KOGITO-COMM[]
https://docs.openshift.com/container-platform/4.2/cli_reference/openshift_cli/getting-started-cli.html[OpenShift documentation].
endif::[]
* You have OpenShift permissions to create resources in a specified namespace.
* (Quarkus only) The `pom.xml` file of your {PRODUCT} project contains the following dependency for the https://quarkus.io/guides/microprofile-health[Quarkus `smallrye-health` extension]. This extension enables the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness and readiness probes] that are required for Quarkus-based projects on OpenShift or Kubernetes.
+
.SmallRye Heath dependency for Quarkus applications on OpenShift
[source,xml]
----
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-smallrye-health</artifactId>
</dependency>
----

.Procedure
. Go to the https://github.com/kiegroup/kogito-operator/releases[`{PRODUCT_INIT}-cloud-operator`] releases page in GitHub and download the latest version of the `{PRODUCT_INIT}-cli-_RELEASE_` binary file that is specific to your operating system.
. Extract the `{PRODUCT_INIT}-cli-_RELEASE_` binary file to a local directory:
+
--
* On Linux or Mac: In a command terminal, navigate to the directory where you downloaded the `{PRODUCT_INIT}-cli-_RELEASE_` binary file and enter the following command to extract the contents:
+
.Extract the {PRODUCT} CLI distribution
[source,subs="attributes+,+quotes"]
----
$ tar -xvf {PRODUCT_INIT}-cli-_RELEASE_.tar.gz
----

* On Windows: In your file browser, navigate to the directory where you downloaded the `{PRODUCT_INIT}-cli-_RELEASE_` binary file and extract the ZIP file.

The `{PRODUCT_INIT}` executable file appears.
--
. Move the extracted `{PRODUCT_INIT}` file to an existing directory in your `PATH` variable:
+
--
* On Linux or Mac: In a command terminal, enter the following command:
+
.Move the `{PRODUCT_INIT}` file
[source,subs="attributes+,+quotes"]
----
$ cp /__PATH_TO_{PRODUCT_INIT_CAP}__ /usr/local/bin
----

* On Windows: Update the relevant `PATH` environment variables in your system settings to include the path to the {PRODUCT} CLI folder. For example, on Windows 10, go to *Settings* -> *System Info* -> *Advanced System Settings* -> *Advanced* -> *Environment Variables* and in the *User* or *System* variables, add the path for the {PRODUCT} CLI folder to the `PATH` variable. Close and reopen your Windows command prompt to apply the changes.
--
. With the {PRODUCT} CLI now installed, enter the following commands to deploy your {PRODUCT} services on OpenShift from source:
+
--
NOTE: You must be logged in to the relevant OpenShift cluster using the `oc login` command.

.Example {PRODUCT} service deployment from existing namespace
[source,subs="attributes+,+quotes"]
----
// Uses the provisioned namespace in your OpenShift cluster
$ {PRODUCT_INIT} use-project __PROJECT_NAME__

// Deploys a new {PRODUCT} service from a Git source
$ {PRODUCT_INIT} deploy-service example-quarkus https://github.com/kiegroup/kogito-examples --context-dir process-quarkus-example
----

The {PRODUCT} Operator uses the default branch in the specified Git repository, usually `master`.

NOTE: The first time that you use the {PRODUCT} CLI to interact with a project or service, the {PRODUCT} Operator is automatically installed and used to execute the relevant tasks.

Alternatively, you can generate a new namespace in your cluster during deployment:

.Example {PRODUCT} service deployment from new namespace
[source,subs="attributes+,+quotes"]
----
// Creates a new namespace in your cluster
$ {PRODUCT_INIT} new-project __NEW_PROJECT_NAME__

// Deploys a new {PRODUCT} service from a Git source
$ {PRODUCT_INIT} deploy-service example-quarkus https://github.com/kiegroup/kogito-examples --context-dir process-quarkus-example
----

You can also combine the commands to create the namespace and deploy the service using the following abbreviated syntax:

.Abbreviated command for {PRODUCT} service deployment
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} deploy-service example-quarkus https://github.com/kiegroup/kogito-examples --context-dir process-quarkus-example --project __PROJECT_NAME__
----
--
////
[id="ref-kogito-service-deployment-openshift-web-console_{context}"]
=== Custom {PRODUCT} service deployment troubleshooting

<Start here>

[id="con-custom-kogito-service-integration-options_{context}"]
== Custom {PRODUCT} service integration options with {PRODUCT} Operator and CLI

<Start here>

[id="proc-custom-kogito-service-integration-infinispan_{context}"]
=== Integrating a custom {PRODUCT} service with Infinispan

<Start here>
////

==== Infinispan persistence in {PRODUCT} services

After you install the `KogitoInfra` custom resource to connect with the Infinispan infrastructure, to enable Infinispan persistence for a {PRODUCT} service using the {PRODUCT} Operator, use the `--infra __INFINISPAN_INFRA_NAME__` flag during deployment in the {PRODUCT} CLI or edit the `spec.infra` configuration in the `KogitoRuntime` custom resource:

.Example {PRODUCT} service deployment with Infinispan persistence enabled using the {PRODUCT} CLI
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} deploy-service travels --infra __INFINISPAN_INFRA_NAME__
----

.Example {PRODUCT} service custom resource with Infinispan persistence enabled
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: travels
spec:
  infra:
    - INFINISPAN_INFRA_NAME
----

If you set up a custom Infinispan cluster, you can refer to it in the `KogitoRuntime` custom resource by configuring the following environment variables and application properties:

.Required environment variables for a custom Infinispan cluster
[source]
----
ENABLE_PERSISTENCE=true

# On Quarkus
QUARKUS_INFINISPAN_CLIENT_AUTH_USERNAME
QUARKUS_INFINISPAN_CLIENT_AUTH_PASSWORD

# On Spring Boot
INFINISPAN_REMOTE_AUTH_USERNAME
INFINISPAN_REMOTE_AUTH_PASSWORD
----

.Required application properties for a custom Infinispan cluster
[source]
----
# On Quarkus
quarkus.infinispan-client.server-list=
quarkus.infinispan-client.use-auth=
quarkus.infinispan-client.sasl-mechanism=
quarkus.infinispan-client.auth-realm=

# On Spring Boot
infinispan.remote.server-list=
infinispan.remote.use-auth=
infinispan.remote.sasl-mechanism=
infinispan.remote.auth-realm=
----

[role="_additional-resources"]
.Additional resources
* {URL_CONFIGURING_KOGITO}#con-persistence_kogito-configuring[Persistence in {PRODUCT}]
* {URL_CONFIGURING_KOGITO}#con-data-index-service_kogito-configuring[{PRODUCT} Data Index Service]
* https://github.com/infinispan/infinispan-operator/blob/master/README.md[Infinispan Operator]

////
[id="proc-custom-kogito-service-integration-kafka_{context}"]
=== Integrating a custom {PRODUCT} service with Apache Kafka

<Start here>
////

==== Apache Kafka messaging in {PRODUCT} services

After you install the `KogitoInfra` custom resource to connect with the Apache Kafka infrastructure, to enable Kafka messaging for a {PRODUCT} service using the {PRODUCT} Operator, use the `--infra __KAFKA_INFRA_NAME__` flag during deployment in the {PRODUCT} CLI or edit the `spec.infra` configuration in the `KogitoRuntime` custom resource:

.Example {PRODUCT} service deployment with Kafka messaging enabled using the {PRODUCT} CLI
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} deploy-service travels --infra __KAFKA_INFRA_NAME__
----

.Example {PRODUCT} service custom resource with Kafka messaging enabled
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: travels
spec:
  infra:
    - KAFKA_INFRA_NAME
----

When you enable Kafka messaging, a variable named `KAFKA_BOOTSTRAP_SERVERS` is injected into the service container. On Quarkus, this is the default behavior when you use https://quarkus.io/guides/kafka-streams#topic-configuration[Kafka Client] 1.x or later. On Spring Boot, you might need to use a property substitution in the `application.properties` file, such as the following example:

.Example application property substitution for Kafka on Spring Boot
[source]
----
spring.kafka.bootstrap.servers=${KAFKA_BOOTSTRAP_SERVERS}
----

If the service container has any environment variables with the suffix `_BOOTSTRAP_SERVERS`, the variables are also injected by the value of the `KAFKA_BOOTSTRAP_SERVERS` variable.

For example, when you deploy the following {PRODUCT} service, the variables `MP_MESSAGING_INCOMING_TRAVELLERS_BOOTSTRAP_SERVERS` and `MP_MESSAGING_OUTGOING_PROCESSEDTRAVELLERS_BOOTSTRAP_SERVERS` are injected with the deployed Kafka service URL:

.Example {PRODUCT} service deployment with injected Kafka variable values
[source]
----
$ kogito deploy-service travels https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended --context-dir travels --infra kogito-kafka-infra \
--build-env MAVEN_ARGS_APPEND="-Pevents" \
-e MP_MESSAGING_INCOMING_TRAVELLERS_BOOTSTRAP_SERVERS
-e MP_MESSAGING_OUTGOING_PROCESSEDTRAVELLERS_BOOTSTRAP_SERVERS
----

If you set up a custom Kafka cluster, you can refer to it in the `KogitoRuntime` custom resource by configuring the following environment variable and application property:

.Required environment variable for a custom Kafka cluster
[source]
----
ENABLE_EVENTS=true
----

.Required application property for a custom Kafka cluster
[source]
----
# On Quarkus
kafka.bootstrap.servers=

# On Spring Boot
spring.kafka.bootstrap-servers=
----

[role="_additional-resources"]
.Additional resources
* {URL_CONFIGURING_KOGITO}#proc-messaging-enabling_kogito-configuring[Enabling messaging for {PRODUCT} services]
* {URL_CONFIGURING_KOGITO}#con-data-index-service_kogito-configuring[{PRODUCT} Data Index Service]

////
[id="proc-custom-kogito-service-integration-mongodb_{context}"]
=== Integrating a custom {PRODUCT} service with MongoDB

<Start here>
////

==== MongoDB persistence in {PRODUCT} services

After you install the `KogitoInfra` custom resource to connect with the MongoDB infrastructure, to enable MongoDB persistence for a {PRODUCT} service using the {PRODUCT} Operator, use the `--infra __MONGODB_INFRA_NAME__` flag during deployment in the {PRODUCT} CLI or edit the `spec.infra` configuration in the `KogitoRuntime` custom resource:

.Example {PRODUCT} service deployment with MongoDB persistence enabled using the {PRODUCT} CLI
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} deploy-service travels --infra __MONGODB_INFRA_NAME__
----

.Example {PRODUCT} service custom resource with MongoDB persistence enabled
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: travels
spec:
  infra:
    - MONGODB_INFRA_NAME
----

If you set up a custom MongoDB cluster and do not use a `KogitoInfra` resource, you can refer to it in the `KogitoRuntime` custom resource by configuring the following environment variables and application properties:

.Required environment variables for a custom MongoDB cluster
[source]
----
ENABLE_PERSISTENCE=true

# On Quarkus
QUARKUS_MONGODB_CREDENTIALS_AUTH_SOURCE
QUARKUS_MONGODB_CREDENTIALS_USERNAME
QUARKUS_MONGODB_CREDENTIALS_PASSWORD
QUARKUS_MONGODB_DATABASE

# On Spring Boot
SPRING_DATA_MONGODB_AUTHENTICATION_DATABASE
SPRING_DATA_MONGODB_USERNAME
SPRING_DATA_MONGODB_PASSWORD
SPRING_DATA_MONGODB_DATABASE
----

.Required application properties for a custom MongoDB cluster
[source]
----
# On Quarkus
quarkus.mongodb.connection-string=

# On Spring Boot
spring.data.mongodb.host=
spring.data.mongodb.port=
----

On Spring Boot, `spring.data.mongodb.uri` does not work simultaneously with host, port, and credentials properties (Spring Boot). Also, it is recommended to set the password in Kubernetes.

[role="_additional-resources"]
.Additional resources
* {URL_CONFIGURING_KOGITO}#con-persistence_kogito-configuring[Persistence in {PRODUCT}]
* {URL_CONFIGURING_KOGITO}#con-data-index-service_kogito-configuring[{PRODUCT} Data Index Service]
* https://github.com/mongodb/mongodb-kubernetes-operator/blob/master/README.md[MongoDB Operator]


[id="proc-custom-kogito-service-integration-knative-eventing_{context}"]
=== Integrating a custom {PRODUCT} service with Knative Eventing

Knative Eventing is a serverless platform you can use to create event producers and consumers for your applications. For more information about Knative Eventing, see https://knative.dev/docs/eventing/[Knative Eventing] and the https://github.com/knative/docs[Knative docs repository] on GitHub.

When you integrate a custom {PRODUCT} service with Knative Eventing you can create a Knative broker, bind that Knative broker with the {PRODUCT} Operator, and include a cross-reference link for Knative Eventing.

Knative events conform to the CloudEvents specification. For more information, see https://cloudevents.io/[CloudEvents].

You can use the Knative Eventing add-on to include your {PRODUCT} Services in the Knative Eventing platform. For more information about the Knative Eventing add-on, see https://github.com/kiegroup/kogito-runtimes/tree/main/addons/common/knative/eventing[Knative Eventing add-on] on GitHub.

To use all features, in addition to those available as part of the {PRODUCT} Knative Eventing Source Operator, you must integrate the custom {PRODUCT} service with Knative Eventing.

==== Creating a Knative Broker

Knative brokers and triggers provide an "event mesh" model that enables an event producer to deliver events to a broker. The broker then uniformly distributes the events to consumers by using triggers. For more information about brokers, see https://knative.dev/docs/eventing/broker/[Broker - Knative]. For more information about triggers, see https://knative.dev/docs/eventing/broker/triggers/[Triggers - Knative].

Consumers can register different types of events without needing to negotiate directly with event producers. You can use specific filter conditions to optimize event routing on the underlying platform.

To integrate your custom {PRODUCT} Service with Knative Eventing you need at least one broker that collects the events produced by your custom {PRODUCT} Service. You can create a broker by applying a YAML file by using `kubectl`.

.Prerequisites

* Administrative privileges are available in the target cluster.
* Knative Eventing is installed or the {OPENSHIFT} Serverless Platform is available.
+
For more information about installing Knative Eventing, see https://knative.dev/docs/install/[About installing Knative]. For more information about {OPENSHIFT} Serverless, see https://www.openshift.com/learn/topics/serverless[{OPENSHIFT} Serverless]

.Procedure

. Create a `<filename.yaml>` YAML file containing the following template:
+
[source]
----
apiVersion: eventing.knative.dev/v1
kind: Broker
metadata:
  name: <broker-name>
----
+
The YAML file creates a broker with the desired name in the current namespace. For more information about configuring broker options using YAML, see https://knative.dev/docs/eventing/broker/example-mtbroker/[Broker configuration example - Knative].

. To apply the YAML file, enter the following command:
+
[source]
----
kubectl apply -f <filename>.yaml>
----
+
Where `<filename>` is the name of the file you created in the previous step.

. Optional: To verify that the broker is working correctly, enter the following command:
+
[source]
----
kubectl -n <namespace> get broker <broker-name>
----
+
The information about your broker is displayed:
+
If the broker is working correctly, it shows a *READY* status of `True`, for example:
+
[cols="15%,15%,15%,40%,15%", options="header"]
|===
|NAME
|READY
|REASON
|URL
|AGE

|<broker-name>
|True
|
|http://broker-ingress.knative-eventing.svc.cluster.local/event-example/<broker-name>
|1m
|===
+
If the *READY* status is `False`, wait a few moments and then run the command again.

==== Binding Knative Broker with {PRODUCT} operator

. Create your {PRODUCT} project locally. For more information about Serverless Workflow, see https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-orchestrating-serverless[Orchestrating microservices with Serverless Workflow in Kogito].
. Build the image with your project. For an example project, see https://github.com/kiegroup/kogito-examples/tree/stable/kogito-quarkus-examples/process-knative-quickstart-quarkus[Process with Knative Eventing] on GitHub.
. Deploy the project as either a Knative Service or a regular deployment.
. Create the {PRODUCT} source CR, for example:
+
[source]
----
apiVersion: kogito.knative.dev/v1alpha1
kind: KogitoSource
metadata:
  name: my-kogito-source
spec:
  subject:
    apiVesion: serving.knative.dev/v1
    kind: Service
    # Put the name of your custom Kogito Service [3]
    name: my-kogito-service
  sink:
    ref:
      apiVersion: eventing.knative.dev/v1
      kind: Broker
      name: <broker-name>
----
. To deploy your source, enter the following command:
+
[source]
----
kubectl apply -f <path to the source>.yaml
----

The custom {KOGITO} Service is now configured to produce cloud events that will be collected by the broker.

If you want the custom {KOGITO} Service to consume Knative Events you can use Knative triggers. For more information, see https://docs.jboss.org/kogito/release/latest/html_single/#con-knative-eventing_kogito-developing-process-services[Knative Eventing in Kogito Services].

==== Using the {KOGITO} Knative add-on

You can use the {PRODUCT} add-on instead of manually configuring definitions of the YAML file descriptors for your custom {PRODUCT} service.

To use the {PRODUCT} Knative add-on to generate the needed YAML files for your custom {PRODUCT} service, you add the YAML file to your `pom.xml` configuration file and compile your project using the Knative profile.

https://github.com/kiegroup/kogito-examples/blob/main/kogito-quarkus-examples/serverless-workflow-order-processing/pom.xml#L105-L127[Example ]

.Procedure

. Add the the following example to your `pom.xml` file:
+
[source]
----
<profile>
  <id>knative</id>
  <properties>
    <!-- Shortcuts -->
    <namespace>default</namespace>
    <deploy>false</deploy>
    <!-- Quarkus Knative integration to build and deploy -->
    <quarkus.kubernetes.namespace>${namespace}</quarkus.kubernetes.namespace>
    <quarkus.kubernetes.deploy>${deploy}</quarkus.kubernetes.deploy>
    <quarkus.container-image.build>true</quarkus.container-image.build>
    <quarkus.profile>knative</quarkus.profile>
  </properties>
  <dependencies>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-kubernetes</artifactId>
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-container-image-jib</artifactId>
    </dependency>
  </dependencies>
</profile>
----
+
For more information, see https://github.com/kiegroup/kogito-examples/blob/main/kogito-quarkus-examples/serverless-workflow-order-processing/pom.xml#L105-L127[pom.xml example] in the _{PRODUCT} examples_ on GitHub.

. To create the `knative.yml` and `kogito.yml` YAML files, enter the following:
+
[source]
----
mvn clean install -Pknative -Dnamespace=<your namespace>
----

. To deploy the objects from the `knative.yml` and `kogito.yml` YAML files, enter the following command:
+
[source]
----
kubectl apply -f target/kubernetes/knative.yml,target/kubernetes/kogito.yml
----

////
[id="ref-kogito-service-integration-troubleshooting_{context}"]
=== Custom {PRODUCT} service integration troubleshooting

<Start here>

[id="con-kogito-operator-interaction-data-index-service_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Data Index Service

<Start here>

[id="proc-kogito-data-index-service-deployment_{context}"]
=== Deploying a {PRODUCT} Data Index Service

<Start here>

[id="ref-kogito-data-index-service-deployment-troubleshooting_{context}"]
=== {PRODUCT} Data Index Service deployment troubleshooting

<Start here>
////

[id="con-kogito-operator-with-postgresql_{context}"]
==== Postgresql persistence with {PRODUCT} service

[role="_abstract"]
The {PRODUCT} Operator does not manage Postgres instances.
If {PRODUCT} service persistence type is reactive(`kogito.persistence.type=postgresql`), then you can refer to it in the `KogitoRuntime` custom resource by configuring the following environment variables. Reactive persistence with `kogito.persistence.type=postgresql` only worked with Springboot.

.Required environment variables for a custom Postgresql persistence
[source]
----
# for Spring Boot
PGUSER
PGHOST
PGPASSWORD
PGDATABASE
PGPORT
----

.Example {PRODUCT} service custom resource with reactive Postgresql persistence
[source]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: process-postgresql-persistence-quarkus
spec:
  image: quay.io/kiegroup/process-postgresql-persistence-quarkus:persistence
  runtime: springboot
  env:
    - name: PGUSER
      value: postgresql
    - name: PGDATABASE
      value: runtimeapp
    - name: PGPASSWORD
      valueFrom:
        secretKeyRef:
          name: kogito-postgresql
          key: postgresql-password
    - name: PGHOST
      value: kogito-postgresql.default.svc.cluster.local
    - name: PGPORT
      value: '5432'
----

or with the connection string
[source]
----
kogito.persistence.postgresql.connection.uri
----

whereas if the persistence type is jdbc(`kogito.persistence.type=jdbc`), then you can refer to it in the `KogitoRuntime` custom resource by configuring the following environment variables:

.Required environment variables for a custom Postgresql persistence
[source]
----
# On Quarkus
QUARKUS_DATASOURCE_DB-KIND
QUARKUS_DATASOURCE_USERNAME
QUARKUS_DATASOURCE_PASSWORD
QUARKUS_DATASOURCE_JDBC_URL

# On Spring Boot
SPRING_DATASOURCE_USERNAME
SPRING_DATASOURCE_PASSWORD
SPRING_DATASOURCE_URL
----

.Example {PRODUCT} service custom resource with JDBC Postgresql persistence
[source]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: process-postgresql-persistence-quarkus
spec:
  image: quay.io/kiegroup/process-postgresql-persistence-quarkus:jdbc
  env:
    - name: QUARKUS_DATASOURCE_DB-KIND
      value: postgresql
    - name: QUARKUS_DATASOURCE_USERNAME
      value: postgres
    - name: QUARKUS_DATASOURCE_PASSWORD
      valueFrom:
        secretKeyRef:
          name: kogito-postgresql
          key: postgresql-password
    - name: QUARKUS_DATASOURCE_JDBC_URL
      value: jdbc:postgresql://kogito-postgresql.default.svc.cluster.local:5432/runtimeapp
----

=== {PRODUCT} Data Index Service properties configuration

When the {PRODUCT} Data Index Service is deployed, a `configMap` resource is created for the `application.properties` configuration of the Data Index Service.

The name of the `configMap` resource consists of the name of the Data Index Service and the suffix `-properties`, as shown in the following example:

.Example `configMap` resource generated during {PRODUCT} Data Index Service deployment
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: data-index-properties
data:
  application.properties : |-
    property1=value1
    property2=value2
----

The `application.properties` data of the `configMap` resource is mounted in a volume to the container of the Data Index Service. Any runtime properties that you add to the `application.properties` section override the default application configuration properties of the Data Index Service.

When the `application.properties` data of the `configMap` is changed, a rolling update modifies the deployment and configuration of the Data Index Service.

If your {PRODUCT} project uses the Infinispan or MongoDB Persistence {PRODUCT} add-on to enable Infinispan or MongoDB persistence explicitly, the {PRODUCT} Operator mounts a volume based on a `configMap` resource created for you during the deployment of the service. This `configMap` resource has the `-protobuf-files` suffix and lists the protobuf (https://developers.google.com/protocol-buffers/[protocol buffers]) files that your service generated during build time.

.Example `configMap` resource with protobuf files listed
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: example-quarkus-protobuf-files
  labels:
    kogito-protobuf: true
data:
  visaApplications.proto: |-
    syntax = "proto2";
    package org.acme.travels.visaApplications;
    import "kogito-index.proto";
    import "kogito-types.proto";
    option kogito_model = "VisaApplications";
    option kogito_id = "visaApplications";
    ...
----

When a new persistent {PRODUCT} service is deployed, a new set of protobuf files is generated and the Data Index Service creates a new `pod` instance that refers to the new volume.

Updated protobuf files are automatically refreshed by Kubernetes volumes, so if you add a new
property in your domain data, this data is reflected automatically in the Data Index Service without restarts.

If a {PRODUCT} service is removed, the associated protobuf files are also removed and are no longer visible in the Data Index Service, but the data is still persisted in Infinispan or MongoDB.

////
[id="con-kogito-operator-interaction-jobs-service_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Jobs Service

<Start here>

[id="proc-kogito-jobs-service-deployment_{context}"]
=== Deploying {PRODUCT} Jobs Service

<Start here>

[id="ref-kogito-jobs-service-deployment-troubleshooting_{context}"]
=== {PRODUCT} Jobs Service deployment troubleshooting

<Start here>
////

=== PostgreSQL persistence with the {PRODUCT} Data Index Service

Data Index Service does not initialize its database schema automatically. To initialize the database schema, you need to either enable automatic schema initialization using Hibernate or create the schema using the DDL script.

Automatic schema initialization using Hibernate can be enabled using the `quarkus.hibernate-orm.database.generation` property. For possible values of `quarkus.hibernate-orm.database.generation` property, see https://quarkus.io/guides/hibernate-orm#quarkus-hibernate-orm_quarkus.hibernate-orm.database.generation[Quarkus docs for database generation options]. The value can also be set as an environment variable `QUARKUS_HIBERNATE_ORM_DATABASE_GENERATION`.

.Example `DataIndex` resource with PostgreSQL persistence enabled (requires Kafka KogitoInfra to be available):
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoSupportingService
metadata:
  name: data-index
spec:
  serviceType: DataIndex
  infra:
    - kogito-kafka-infra
  env:
    - name: QUARKUS_DATASOURCE_DB-KIND
      value: postgresql
    - name: QUARKUS_DATASOURCE_USERNAME
      value: postgres
    - name: QUARKUS_DATASOURCE_PASSWORD
      valueFrom:
        secretKeyRef:
          name: kogito-postgresql
          key: postgresql-password
    - name: QUARKUS_DATASOURCE_JDBC_URL
      value: jdbc:postgresql://kogito-postgresql.default.svc.cluster.local:5432/kogito
    - name: QUARKUS_HIBERNATE_ORM_DATABASE_GENERATION
      value: update
----

=== Infinispan persistence with the {PRODUCT} Jobs Service

To enable Infinispan persistence for the {PRODUCT} Jobs Service, you install the Infinispan infrastructure and then you can use the `--infra __INFINISPAN_INFRA_NAME__` flag during deployment in the {PRODUCT} CLI or edit the `spec.infra` configuration in the `KogitoSupportingService` custom resource that contains the `JobsService` service type:

.Example Jobs Service deployment with Infinispan persistence enabled
[source,subs="attributes+,+quotes"]
----
$ {PRODUCT_INIT} install infra __INFINISPAN_INFRA_NAME__ --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan -p __PROJECT_NAME__
$ {PRODUCT_INIT} install jobs-service --infra __INFINISPAN_INFRA_NAME__
----

.Example `JobsService` resource with Infinispan persistence enabled
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoSupportingService
metadata:
  name: jobs-service
spec:
  serviceType: JobsService
  infra:
    - kogito-infinispan-infra
----

You can refine the Infinispan integration by setting the `spec.infra` property in the `KogitoSupportingService` custom resource that contains the `JobsService` service type. With this property set, the {PRODUCT} Operator ensures that the Jobs Service has access to the persistence infrastructure configurations.

=== {PRODUCT} Jobs Service properties configuration

When the {PRODUCT} Jobs Service is deployed, a `configMap` resource is created for the `application.properties` configuration of the Jobs Service.

The name of the `configMap` resource consists of the name of the Jobs Service and the suffix `-properties`, as shown in the following example:

.Example `configMap` resource generated during {PRODUCT} Jobs Service deployment
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: jobs-service-properties
data:
  application.properties : |-
    property1=value1
    property2=value2
----

The `application.properties` data of the `configMap` resource is mounted in a volume to the container of the Jobs Service. Any runtime properties that you add to the `application.properties` section override the default application configuration properties of the Jobs Service.

When the `application.properties` data of the `configMap` is changed, a rolling update modifies the deployment and configuration of the Jobs Service.

////
[id="con-kogito-operator-interaction-management-console_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Management Console

<Start here>

[id="proc-management-console-deployment_{context}"]
=== Deploying the {PRODUCT} Management Console

<Start here>

[id="ref-kogito-management-console-deployment-troubleshooting_{context}"]
=== {PRODUCT} Management Console deployment troubleshooting

<Start here>

[id="con-kogito-operator-interaction-task-console_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Task Console

<Start here>

[id="proc-task-console-deployment_{context}"]
=== Deploying the {PRODUCT} Task Console

<Start here>

[id="ref-kogito-task-console-deployment-troubleshooting_{context}"]
=== {PRODUCT} Task Console deployment troubleshooting

<Start here>

[id="con-kogito-operator-interaction-trusty-service_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Trusty Service

<Start here>

[id="proc-trusty-service-deployment_{context}"]
=== Deploying the {PRODUCT} Trusty Service

<Start here>

[id="ref-kogito-trusty-service-deployment-troubleshooting_{context}"]
=== {PRODUCT} Trusty Service deployment troubleshooting

<Start here>

[id="con-kogito-operator-interaction-explainability-service_{context}"]
== {PRODUCT} Operator interaction with the {PRODUCT} Explainability Service

<Start here>

[id="proc-explainability-service-deployment_{context}"]
=== Deploying {PRODUCT} Explainability Service

<Start here>

[id="ref-kogito-trusty-service-deployment-troubleshooting_{context}"]
=== {PRODUCT} Explainability Service deployment troubleshooting

<Start here>
////

[id="con-kogito-operator-interaction-prometheus-grafana_{context}"]
== {PRODUCT} Operator interaction with Prometheus and Grafana

[role="_abstract"]
{PRODUCT} provides a `monitoring-prometheus-addon` add-on that enables Prometheus metrics monitoring for {PRODUCT} services and generates Grafana dashboards that consume the default metrics exported by the add-on. The {PRODUCT} Operator uses the  https://github.com/coreos/prometheus-operator[Prometheus Operator] to expose the metrics from your {PRODUCT} project for Prometheus to scrape. Due to this dependency, the Prometheus Operator must be installed in the same namespace as your {PRODUCT} project.

When you deploy a {PRODUCT} service that uses the `monitoring-prometheus-addon` add-on and the  Prometheus Operator is installed, the {PRODUCT} Operator creates a `ServiceMonitor` custom resource to expose the metrics for Prometheus, as shown in the following example:

.Example `ServiceMonitor` resource for Prometheus
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: onboarding-service
  name: onboarding-service
  namespace: kogito
spec:
  endpoints:
  - path: /metrics
    targetPort: 8080
    scheme: http
  namespaceSelector:
    matchNames:
    - kogito
  selector:
    matchLabels:
      app: onboarding-service
----

You must manually configure your `Prometheus` custom resource that is managed by the Prometheus Operator to select the `ServiceMonitor` resource:

.Example `Prometheus` resource
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      app: onboarding-service
----

After you configure your Prometheus resource with the `ServiceMonitor` resource, you can see the endpoint being scraped by Prometheus in the *Targets* page in the Prometheus web console:

.Targets page in Prometheus web console
image::kogito/openshift/kogito-operator-prometheus-targets.png[Image of Kogito service targets view in Prometheus]

The metrics exposed by the {PRODUCT} service appear in the *Graph* view:

.Graph view in Prometheus web console
image::kogito/openshift/kogito-operator-prometheus-graph.png[Image of Kogito service graph view in Prometheus]

The {PRODUCT} Operator also creates a `GrafanaDashboard` custom resource defined by the https://operatorhub.io/operator/grafana-operator[Grafana Operator] for each of the Grafana dashboards generated by the add-on. The `app` label for the dashboards is the name of the deployed {PRODUCT} service. You must set the `dashboardLabelSelector` property of the `Grafana` custom resource according to the relevant {PRODUCT} service.

.Example `Grafana` resource
[source,yaml]
----
apiVersion: integreatly.org/v1alpha1
kind: Grafana
metadata:
  name: example-grafana
spec:
  ingress:
    enabled: true
  config:
    auth:
      disable_signout_menu: true
    auth.anonymous:
      enabled: true
    log:
      level: warn
      mode: console
    security:
      admin_password: secret
      admin_user: root
  dashboardLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values:
            - my-kogito-application
----

[role="_additional-resources"]
.Additional resources
* {URL_CONFIGURING_KOGITO}#proc-prometheus-metrics-monitoring_kogito-configuring[Enabling Prometheus metrics monitoring in {PRODUCT}]
* https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md[Prometheus Operator]
* https://operatorhub.io/operator/grafana-operator[Grafana Operator]

////
[id="ref-prometheus-grafana-integration-troubleshooting_{context}"]
=== Prometheus and Grafana integration troubleshooting
<Start here>
////

[id="proc-kogito-custom-truststore_{context}"]
=== Replacing {PRODUCT} services TrustStores

[role="_abstract"]
You can replace the default Java TrustStore that comes in {PRODUCT} images. Replacement of Java TrustStore is required when a given {PRODUCT} service makes HTTPS calls to services that require encrypted connections using a private https://dzone.com/articles/understanding-the-role-of-certificate-authorities[Certificate Authority (CA)].

.Prerequisites
* The Java TrustStore replaces the default one. You must add the trusted certificates to the default JKS. As an alternative, you can create a new TrustStore that consists of the required {PRODUCT} service certificates.

NOTE: Any {PRODUCT} service that is deployed in a given namespace can access the Java TrustStore if required.

To manipulate the Java TrustStore, you can use https://keystore-explorer.org/[KeyExplorer] or https://docs.oracle.com/javase/8/docs/technotes/tools/unix/keytool.html[keytool] that consists of JDK.

.Procedure
. Create a https://kubernetes.io/docs/tasks/configmap-secret/managing-secret-using-config-file/[Kubernetes Secret] based on the customized Java TrustStore:
+
--
.Example Kuberbetes Secret
[source]
----
$ oc create secret generic kogito-truststore --from-file=cacerts=<path to your JKS> --from-literal=keyStorePassword=<TrustStore Password>
----
--
+
You must define the secret keys as shown in the previous command. `cacerts` is used for the TrustStore file and `keyStorePassword` is used for the TrustStore password.

. Deploy the {PRODUCT} service that uses the TrustStore. For example:
+
--
.Example {PRODUCT} service deployment
[source,yaml]
----
piVersion: app.kiegroup.org/v1beta1
kind: KogitoRuntime
metadata:
  name: my-service
spec:
  replicas: 1
  image: quay.io/my-namespace/my-service:latest
  trustStoreSecret: kogito-truststore
----
--

. Alternatively, you can use {PRODUCT} CLI to deploy the {PRODUCT} service using `truststore-secret` as follows:
+
--
.Example {PRODUCT} service deployment using {PRODUCT} CLI
[source]
----
$ kogito deploy my-service https://mygitrepository/my-service.git --truststore-secret kogito-truststore
----
--

[id="proc-kogito-deploying-on-kubernetes_{context}"]
=== Deploying {PRODUCT} services on Kubernetes

[id="con-kogito-operator-examples_{context}"]
== Examples provided with {PRODUCT} Operator

Several examples are shipped with {KOGITO}. You can review the code for examples and modify it as necessary to suit your needs.

[id="con-kogito-travel-agency_{context}"]
=== Travel agency tutorial for {PRODUCT} services on OpenShift

[role="_abstract"]
The https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application in GitHub contains {PRODUCT} services related to travel booking. The purpose of this example application is to help you get started with deploying {PRODUCT} services on {OPENSHIFT}.

The example application illustrates many of the configuration options you can use whether you are deploying services locally or on {OPENSHIFT}, such as process persistence with Infinispan, messaging with Apache Kafka, and application data indexing with the {PRODUCT} Data Index Service.

For more information about this example application, see the `README` file in the application folder.

This tutorial demonstrates the following two related services in the `kogito-travel-agency` extended example application:

* https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended/travels[`travels`]: Service for booking a trip to a specified destination, including flight and hotel
* https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended/visas[`visas`]: Service for managing travel visas, if required

The following Business Model and Notation (BPMN) 2.0 process models are the core processes in these services:

.Travels core process
image::kogito/openshift/kogito-ocp-travel-agency-process.png[Image of travel agency example process]

.Visas core process
image::kogito/openshift/kogito-ocp-visas-process.png[Image of visas example process]

These two services communicate with each other through events. The travel agency service schedules specified travel plans and sends visa applications for travelers that require visas to visit a specified country. The visa service then evaluates any visa applications and responds with the visa approval or rejection.

The services expose REST API endpoints that are generated from the BPMN business process definitions in the services. Internally, the services communicate using Apache Kafka messaging. The logic to interact with Kafka to produce and consume messages is also generated from the BPMN process definitions.

.Tutorial objectives
* Deploy an application with advanced {PRODUCT} services, including supporting services and infrastructure.
* Deploy {PRODUCT} infrastructures (Infinispan, Kafka, and Data Index Service) using the {PRODUCT} Operator and {PRODUCT} CLI.
* Deploy {PRODUCT} service definitions using the {PRODUCT} CLI.
* Use binary builds to deploy {PRODUCT} services on OpenShift.

.Prerequisites
* https://code.visualstudio.com/[VSCode] 1.46.0 or later is installed.
* The *{PRODUCT} Bundle* VSCode extension is installed and enabled in your VSCode IDE.
* {OPENSHIFT} 4.3 or later is installed.
* The `oc` OpenShift CLI is installed. For `oc` installation instructions, see the
ifdef::KOGITO-ENT[]
https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html/cli_tools/openshift-cli-oc[OpenShift documentation].
endif::[]
ifdef::KOGITO-COMM[]
https://docs.openshift.com/container-platform/4.2/cli_reference/openshift_cli/getting-started-cli.html[OpenShift documentation].
endif::[]
* You have access to the OpenShift web console with `cluster-admin` permissions.
* The {PRODUCT} command-line interface (CLI) is installed from the latest https://github.com/kiegroup/kogito-operator/releases[{PRODUCT} CLI distribution].
* Git is installed.
* JDK 11 or later is installed. (https://www.graalvm.org/[GraalVM] is recommended.)
* Apache Maven 3.6.2 or later is installed.
* (Quarkus only) The `pom.xml` file of your {PRODUCT} project contains the following dependency for the https://quarkus.io/guides/microprofile-health[Quarkus `smallrye-health` extension]. This extension enables the https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes[liveness and readiness probes] that are required for Quarkus-based projects on OpenShift or Kubernetes.
+
.SmallRye Heath dependency for Quarkus applications on OpenShift
[source,xml]
----
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-smallrye-health</artifactId>
</dependency>
----

[id="proc-kogito-travel-agency-clone-repo_{context}"]
==== Cloning the {PRODUCT} examples Git repository

[role="_abstract"]
For this travel agency tutorial, you need local access to the example services, so you must first clone the https://github.com/kiegroup/kogito-examples[`{PRODUCT_INIT}-examples`] Git repository to your local system.

.Procedure
In a command terminal, navigate to a directory where you want to store the {PRODUCT} example applications and enter the following command to clone the repository:

.Cloning the {PRODUCT} examples repository
[source]
----
$ git clone https://github.com/kiegroup/kogito-examples.git
----

The cloned `{PRODUCT_INIT}-examples` repository contains various types of {PRODUCT} services on Quarkus or Spring Boot to help you develop your own applications.

For this travel agency tutorial, you need the `kogito-travel-agency` extended example application, which contains the following services:

* https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended/travels[`travels`]: Service for booking a trip to a specified destination, including flight and hotel
* https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended/visas[`visas`]: Service for managing travel visas, if required

[id="proc-kogito-travel-agency-configure-ocp_{context}"]
==== Configuring access to your OpenShift environment

[role="_abstract"]
To complete the travel agency tutorial, you must ensure that you have proper access to both the {OPENSHIFT} web console and to the `oc` CLI.

NOTE: You can use different types of OpenShift 4.x environments, such as a full OpenShift cluster or a small https://github.com/code-ready/crc[CodeReady Containers] environment. However, the OpenShift environment must have access to the public Internet in order to be able to pull in the required container images and build artifacts.

.Procedure
. Log in to the OpenShift web console and in the upper-right corner of the screen, select your profile and click *Copy Login Command*.
. In the new window that appears, log in again to re-authenticate your user and then click *Display Token*.
. Copy the `oc login` command and enter it in a command terminal:
+
--
.Example `oc` CLI login token
[source,subs="+quotes"]
----
$ oc login --token=__OPENSHIFT_TOKEN__ --server=https://__WEB_CONSOLE_SERVER__
----

If your authentication fails or you do not have `cluster-admin` permissions, contact your OpenShift administrator.
--

[id="proc-kogito-travel-agency-create-ocp-project_{context}"]
==== Creating an OpenShift project and installing the {PRODUCT} Operator using the {PRODUCT} CLI

[role="_abstract"]
To set up an example application with {PRODUCT} services for deployment on {OPENSHIFT}, you must create a project (namespace) in OpenShift in which you can install the application and the {PRODUCT} Operator. The {PRODUCT} Operator is based on the https://sdk.operatorframework.io/[Operator SDK] and automates many of the deployment steps for you. The first time that you use the {PRODUCT} CLI to interact with a project or service, the {PRODUCT} Operator is automatically installed and used to execute the relevant tasks.

You can create the project and install the {PRODUCT} Operator using the OpenShift web console or using the {PRODUCT} CLI. This example uses the {PRODUCT} CLI.

.Procedure
In a command terminal, enter the following command to create an OpenShift project for the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application using the {PRODUCT} CLI:

.Creating the OpenShift project
[source]
----
$ kogito new-project kogito-travel-agency
Project `kogito-travel-agency` created successfully
----

If you do not have `cluster-admin` permissions and another user created the `kogito-travel-agency` project for you, you can alternatively use the following command to connect the {PRODUCT} CLI tooling to the existing project:

.Connecting to an existing OpenShift project
[source]
----
$ kogito use-project kogito-travel-agency
Project set to 'kogito-travel-agency'
----

The `kogito new-project` and `kogito use-project` commands automatically install the *{PRODUCT} Operator* if it is not installed already.

If your {PRODUCT} project requires persistence and messaging infrastructures, you can use the left menu of the OpenShift web console to navigate to *Operators* -> *OperatorHub* and install the https://github.com/infinispan/infinispan-operator[Infinispan Operator] for persistence and the https://strimzi.io/docs/latest/[Strimzi Operator] for Apache Kafka clusters and messaging. You can also install these operators manually using the https://infinispan.org/infinispan-operator/master/operator.html[Infinispan Operator Guide] or the https://strimzi.io/docs/operators/master/quickstart.html[Strimzi Quick Start guide]. The {PRODUCT} Operator uses these operators to create the needed persistence and messaging infrastructures.

After you create the OpenShift project using the {PRODUCT} CLI and install the {PRODUCT} Operator, the operator is listed with any other installed operators in the OpenShift web console in *Operators* -> *Installed Operators*:

.Installed operators in web console
image::kogito/openshift/kogito-ocp-installed-operators.png[Image of installed operators in web console]

[id="proc-kogito-travel-agency-enable-persistence_{context}"]
==== Installing the Infinispan persistence infrastructure for {PRODUCT} services on OpenShift

[role="_abstract"]
{PRODUCT} supports runtime persistence for process data in your services. {PRODUCT} persistence is based on https://infinispan.org/[Infinispan] and enables you to configure key-value storage definitions to persist data, such as active nodes and process instance variables, so that the data is preserved across application restarts.

The {PRODUCT} Operator uses the https://github.com/infinispan/infinispan-operator[Infinispan Operator] to deploy the Infinispan infrastructure in a {PRODUCT} project. For optimal {PRODUCT} deployment on OpenShift, install the Infinispan Operator and enable Infinispan persistence for your {PRODUCT} services. You can install the Infinispan infrastructure using the {PRODUCT} Operator page in the OpenShift web console or using the {PRODUCT} CLI.

This example uses the {PRODUCT} CLI to install the Infinispan infrastructure and the {PRODUCT} Operator page in the web console to verify that the infrastructure is enabled.

.Prerequisites
* The https://github.com/infinispan/infinispan-operator[Infinispan Operator] is installed in the same OpenShift namespace as your {PRODUCT} project. You can install the Infinispan Operator using the *Operators* -> *OperatorHub* page in the OpenShift web console or manually as described in the https://infinispan.org/infinispan-operator/master/operator.html[Infinispan Operator Guide].

* https://infinispan.org/infinispan-operator/master/operator.html#minimal_crd-start[An Infinispan Server resource] is deployed in the same OpenShift namespace as your {PRODUCT} project. You can try one of the examples as described in the https://infinispan.org/infinispan-operator/master/operator.html#creating_minimal_clusters-start[Infinispan Operator Guide]. For this procedure, set `kogito-infinispan` as the name of the Infinispan Server resource.

.Procedure
. In a command terminal, enter the following command to install the Infinispan infrastructure for the {PRODUCT} services:
+
.Installing Infinispan infrastructure
[source]
----
$ kogito install infra kogito-infinispan-infra --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan
----
. In the OpenShift web console, use the left menu to navigate to the following windows to verify the installed Infinispan infrastructure:

* *Operators* -> *Installed Operators* -> *{PRODUCT}* -> *{PRODUCT} Infra*: A new `kogito-infinispan-infra` custom resource is listed.
+
.{PRODUCT} infrastructure resource for Infinispan
image::kogito/openshift/kogito-ocp-infra.png[Image of Kogito Infra page in web console]
* *Operators* -> *Installed Operators* -> *Infinispan* -> *Infinispan Cluster*: A `kogito-infinispan` custom resource is listed.
+
.Infinispan cluster resource
image::kogito/openshift/kogito-ocp-infinispan-infra.png[Image of Infinispan Cluster page in web console]
* *Workloads* -> *Stateful Sets*: A `kogito-infinispan` stateful set is deployed.
+
.Stateful set for Infinispan
image::kogito/openshift/kogito-ocp-stateful-sets-infinispan.png[Image of Stateful Sets page in web console]

[id="proc-kogito-travel-agency-enable-messaging_{context}"]
==== Installing the Kafka messaging infrastructure for {PRODUCT} services on OpenShift

[role="_abstract"]
{PRODUCT} supports the https://github.com/eclipse/microprofile-reactive-messaging[MicroProfile Reactive Messaging] specification for messaging in your services. {PRODUCT} messaging is based on https://kafka.apache.org/[Apache Kafka] and enables you to configure messages as either input or output of business process execution.

The {PRODUCT} Operator uses the https://strimzi.io/[Strimzi Operator] to deploy and manage the Kafka infrastructure in a {PRODUCT} project. For optimal {PRODUCT} deployment on OpenShift, install the Strimzi Operator and enable Kafka messaging for your {PRODUCT} services. You can install the Kafka infrastructure using the {PRODUCT} Operator page in the OpenShift web console or using the {PRODUCT} CLI.

This example uses the {PRODUCT} CLI to auto-configure an existing Kafka infrastructure and the {PRODUCT} Operator page in the web console to verify that the infrastructure is enabled.

.Prerequisites
* The https://strimzi.io/[Strimzi Operator] is installed in the same OpenShift namespace as your {PRODUCT} project. You can install the Strimzi Operator using the *Operators* -> *OperatorHub* page in the OpenShift web console or manually as described in the https://strimzi.io/docs/operators/master/quickstart.html[Strimzi Quick Start guide].

* https://strimzi.io/docs/operators/master/overview.html#configuration-points-broker_str[A Kafka Cluster resource] is deployed in the same OpenShift cluster as your {PRODUCT} project. You can try one of the examples as described in the https://strimzi.io/docs/operators/master/quickstart.html#proc-kafka-cluster-str[Strimzi Guide]. For this procedure, set `kogito-kafka` as the name of the Kafka Cluster resource.

.Procedure
. In a command terminal, enter the following command to install the Kafka infrastructure for the {PRODUCT} services:
+
.Installing Kafka infrastructure
[source]
----
$ kogito install infra kogito-kafka-infra --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka
----
. In the OpenShift web console, use the left menu to navigate to the following windows to verify the installed Kafka infrastructure:

* *Operators* -> *Installed Operators* -> *{PRODUCT}* -> *{PRODUCT} Infra*: A new `kogito-kafka-infra` custom resource is listed.
+
.Kafka enabled
image::kogito/openshift/kogito-ocp-infra-kafka.png[Image of Kogito infra details in web console]
* *Operators* -> *Installed Operators* -> *Strimzi* -> *Kafka*: A `kogito-kafka` custom resource is listed.
+
.Kafka custom resource
image::kogito/openshift/kogito-ocp-kafka-infra.png[Image of Kafkas page in web console]
* *Workloads* -> *Stateful Sets*: `kogito-kafka-kafka` and `kogito-kafka-zookeeper` stateful sets are deployed.
+
.Stateful sets for Kafka
image::kogito/openshift/kogito-ocp-stateful-sets-kafka.png[Image of Stateful Sets page in web console]

[id="proc-kogito-travel-agency-enable-data-index_{context}"]
==== Installing the {PRODUCT} Data Index Service for {PRODUCT} services on OpenShift

[role="_abstract"]
{PRODUCT} provides a Data Index Service that stores all {PRODUCT} events related to processes, tasks, and domain data. The Data Index Service uses Kafka messaging to consume https://cloudevents.io/[CloudEvents] messages from {PRODUCT} services, and then indexes the returned data for future GraphQL queries and stores the data in the Infinispan persistence store. The Data Index Service is at the core of all {PRODUCT} search, insight, and management capabilities.

The {PRODUCT} Operator uses the Data Index Service for data management in a {PRODUCT} project. For optimal {PRODUCT} deployment on OpenShift, enable the Data Index Service for your {PRODUCT} services. You can install the Data Index Service using the {PRODUCT} Operator page in the OpenShift web console (under *{PRODUCT} Supporting Service* with `serviceType` set to `DataIndex`) or using the {PRODUCT} CLI.

This example uses the {PRODUCT} CLI to install the Data Index Service and the {PRODUCT} Operator page in the web console to verify that the service is enabled.

.Prerequisites
* You have installed the following Infinispan persistence and Kafka messaging infrastructure components for your {PRODUCT} services, as shown in the following example commands:
+
.Installing Infinispan and Kafka infrastructure components
[source]
----
$ kogito install infra kogito-infinispan-infra --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan
$ kogito install infra kogito-kafka-infra --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka
----

.Procedure
. In a command terminal, enter the following command to install the {PRODUCT} Data Index Service for the {PRODUCT} services:
+
--
.Installing Data Index Service
[source]
----
$ kogito install data-index --infra kogito-infinispan-infra --infra kogito-kafka-infra
----

When you enter this command, the {PRODUCT} Operator provisions the Data Index Service to connect to the specified infrastructures.
--
. In the OpenShift web console, use the left menu to navigate to the following windows to verify the installed Data Index Service:

* *Operators* -> *Installed Operators* -> *{PRODUCT}* -> *{PRODUCT} Supporting Service*: A new `kogito-data-index` custom resource is listed.
+
.Data Index Service resource
image::kogito/openshift/kogito-ocp-data-index.png[Image of Kogito Data Index page in web console]
* *Workloads* -> *Deployments*: A new `kogito-data-index` deployment is listed.
+
.Data Index Service deployment
image::kogito/openshift/kogito-ocp-data-index-deployment.png[Image of Data Index deployment page in web console]
* *Networking* -> *Routes*: A new `kogito-data-index` route is listed.
+
--
.Data Index Service route
image::kogito/openshift/kogito-ocp-data-index-route.png[Image of Data Index route page in web console]

You can click the *Location* URL to view the {PRODUCT} Data Index Service GraphQL interface (GraphiQL) and enter GraphQL queries for stored data.

.Example GraphQL query in GraphiQL interface for Data Index Service
image::kogito/openshift/kogito-ocp-data-index-graphql2.png[Image of Data Index GraphiQL interface]
--

[id="proc-kogito-travel-agency-create-services_{context}"]
==== Creating {PRODUCT} service definitions on OpenShift using the {PRODUCT} CLI

[role="_abstract"]
After you set up the required infrastructures for your application, you can create the {PRODUCT} service definitions and provision the OpenShift resources required for deployment with a binary build. You can create the service definitions using the OpenShift web console or using the {PRODUCT} CLI.

This example uses the {PRODUCT} CLI to create the service definitions for the https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application and uses the {PRODUCT} Operator page in the web console to verify that the services are created.

The travel agency example application includes the following key OpenShift resources:

* `BuildConfig`: Configures the application to support a binary build in addition to a traditional OpenShift build for deployment. In a binary build, you build the application locally and push the built application to the OpenShift build to be packaged into the runtime container image. A binary build enables services to be deployed faster than a traditional OpenShift build and deployment.
* `ImageStream`: Defines the set of container images identified by tags.
* `Deployments`: Describes the desired state of the application as a pod template.
* `Service`: Functions as a Kubernetes-internal load balancer to serve the application pods.
* `Route`: Exposes the `Service` at a host name.

.Prerequisites
* You have installed the following Infinispan persistence and Kafka messaging infrastructure components for your {PRODUCT} services, and you have installed the {PRODUCT} Data Index Service accordingly, as shown in the following example commands:
+
.Installing Infinispan, Kafka, and Data Index Service components
[source]
----
$ kogito install infra kogito-infinispan-infra --kind Infinispan --apiVersion infinispan.org/v1 --resource-name kogito-infinispan
$ kogito install infra kogito-kafka-infra --kind Kafka --apiVersion kafka.strimzi.io/v1beta2 --resource-name kogito-kafka
$ kogito install data-index --infra kogito-infinispan-infra --infra kogito-kafka-infra
----

.Procedure
. In a command terminal, navigate to the `kogito-travel-agency` extended example application and enter the following commands to create {PRODUCT} service definitions for the `travels` and `visas` services with Infinispan persistence and Kafka messaging enabled:
+
--
.Creating the travels service with persistence and messaging enabled
[source]
----
$ kogito deploy-service travels --infra kogito-infinispan-infra --infra kogito-kafka-infra
----

.Creating the visas service with persistence and messaging enabled
[source]
----
$ kogito deploy-service visas --infra kogito-infinispan-infra --infra kogito-kafka-infra
----

When the deployment configuration of this service is generated, the {PRODUCT} Operator automatically configures the environment variables to point to the location of the Kafka and Infinispan environments that you deployed previously. For Kafka messaging, the operator sets the incoming and outgoing messaging channels and properties as needed. For Infinispan persistence, the operator sets the authorization configuration based on the credentials generated by the Infinispan Operator.

NOTE: You can also provide a Git repository location to create your services remotely instead of creating your services from a local source. However, this example uses local applications to demonstrate how to prepare the {PRODUCT} project on a development machine for a direct push to the cloud.

--
. In the OpenShift web console, use the left menu to navigate to *Operators* -> *Installed Operators* -> *{PRODUCT}* -> *{PRODUCT} Service* and verify the new `travels` and `visas` services:
+
--
.New travel agency and visas services listed
image::kogito/openshift/kogito-ocp-create-app-listed-agency.png[Image of travels and visas services listed in web console]

The new services are available but not yet deployed on OpenShift until you build and deploy the service projects from source using a binary build.
--

[id="proc-kogito-travel-agency-deploy-binary_{context}"]
==== Deploying {PRODUCT} services on OpenShift using a binary build

[role="_abstract"]
OpenShift builds can require extensive amounts of time. As a faster alternative for building and deploying your {PRODUCT} services on OpenShift, you can use a binary build. In a binary build, you build the application locally and push the built application to an OpenShift `BuildConfig` configuration to be packaged into the runtime container image.

The https://github.com/kiegroup/kogito-examples/tree/stable/kogito-travel-agency/extended[`kogito-travel-agency`] extended example application includes a `BuildConfig` configuration to support a binary build in addition to traditional building for deployment.

[NOTE]
====
{PRODUCT} also supports Source-to-Image (S2I) builds, which build the application in an OpenShift build and then pass the built application to the next OpenShift build to be packaged into the runtime container image. The {PRODUCT} S2I build configuration also enables you to build the project directly from a Git repository on the OpenShift platform.

However, this example uses the local applications to demonstrate how to prepare the {PRODUCT} project on a development machine for a direct push to the cloud.
====

.Prerequisites
* You have created the {PRODUCT} service definitions and provisioned the OpenShift resources required for deployment with a binary build, as described in xref:proc-kogito-travel-agency-create-services_kogito-deploying-on-openshift[].

.Procedure
. In a command terminal, navigate to the `kogito-travel-agency/extended/travels` example service and build the project using Maven:
+
--
.Building the local travels project
[source]
----
$ cd kogito-travel-agency/extended/travels
$ mvn clean package
----

This command builds the project in standard JDK mode to package the application as a runner JAR file and include any dependencies in a `lib` folder.

ifdef::KOGITO-COMM[]
NOTE: Alternatively, you can also build the project in native mode (requires GraalVM and SubstrateVM) to build and compile the application into a native executable for your system.
endif::[]

--
The following resources are generated in the `target/quarkus-app` folder in preparation for deployment from binary build:

* `quarkus-app/app/travels.jar`: Standard JAR file that contains the classes and resources of the project.
* `quarkus-app/quarkus`: Contains the generated resources required by the Quarkus application to run.
* `quarkus-app/lib`: Directory that contains the project dependencies.
* `quarkus-app/quarkus-run.jar`: Executable JAR file for the project. Note that `quarkus-run.jar` is not an uber-JAR file, and also requires the folders `quarkus-app/app`, `quarkus-app/quarkus`, and `quarkus-app/lib`.
. From the same `kogito-travel-agency/extended/travels` directory location where you built the project, enter the following command to deploy the travels service to OpenShift using a binary build:
+
--
.Deploying to OpenShift using binary build
[source]
----
$ oc start-build travels-binary --from-dir=target/

Uploading directory "target/" as binary input for the build ...
....
Uploading finished
build.build.openshift.io/travels-1 started
----

NOTE: The {PRODUCT} images are handling the binary build directly from the `target` folder, and the content of the `target/quarkus-app` folder is copied directly.

You can use the following command to check the logs of the builder pod if needed:

.Checking logs of builder pod
[source]
----
$ oc logs -f build/travels-binary-1
----

After the binary build is complete, the result is pushed to the `travels` Image Stream that was created by the {PRODUCT} Operator and triggers a new deployment.
--
. In the OpenShift web console, use the left menu to navigate to the following windows to verify the deployed service:
+
--
* *Workloads* -> *Deployments*: Select the `travels` deployment to view the application deployment details, pod status, and other details.
+
.Travels deployment details
image::kogito/openshift/kogito-ocp-app-deployment-details-agency.png[Image of travels service deployment details page in web console]
* *Networking* -> *Routes*: Select the *Location* URL for the `travels` route to view the main page of the {PRODUCT} travel agency application.
+
.Travel agency application interface
image::kogito/openshift/kogito-ocp-travel-agency-app.png[Image of Travel Agency application main page]

After you verify that the travel agency application is deployed, repeat the same steps to deploy the visas application.
--
. In a command terminal, navigate to the `kogito-travel-agency/extended/visas` example service and build the project using Maven:
+
--
.Building the local visas project
[source]
----
$ cd kogito-travel-agency/extended/visas
$ mvn clean package
----
--
. Deploy the visas service to OpenShift using a binary build:
+
--
.Deploying to OpenShift using binary build
[source]
----
$ oc start-build visas-binary --from-dir=target/

Uploading directory "target/" as binary input for the build ...
....
Uploading finished
build.build.openshift.io/visas-1 started
----

You can use the following command to check the logs of the builder pod if needed:

.Checking logs of builder pod
[source]
----
$ oc logs -f build/visas-binary-1
----

After the binary build is complete, the result is pushed to the `visas` Image Stream that was created by the {PRODUCT} Operator and triggers a new deployment.
--
. In the OpenShift web console, use the left menu to navigate to the following windows to verify the deployed service:
+
--
* *Workloads* -> *Deployment Configs*: Select the `visas` deployment to view the application deployment configurations, pod status, and other details.
+
.Visas deployment details
image::kogito/openshift/kogito-ocp-app-deployment-details-visas.png[Image of visas service deployment details page in web console]
* *Networking* -> *Routes*: Select the *Location* URL for the `visas` route to view the main page of the {PRODUCT} visas application.
+
.Visas application interface
image::kogito/openshift/kogito-ocp-visas-app.png[Image of Visas application main page]

--

[id="proc-kogito-travel-agency-interacting_{context}"]
==== Interacting with the deployed travel agency services on OpenShift

[role="_abstract"]
After you deploy the example travel agency services on {OPENSHIFT}, you can interact with the application interfaces to create a new travel plan.

You can also use a REST client or curl utility to send a REST request, such as the following example request body:

.Example REST request body to add a traveler and trip
[source,json]
----
{
  "traveller": {
    "firstName": "Jan",
    "lastName": "Kowalski",
    "email": "jan@email.com",
    "nationality": "Polish",
    "address": {
      "street": "Polna",
      "city": "Krakow",
      "zipCode": "32-000",
      "country": "Poland"
    }
  },
  "trip": {
    "country": "US",
    "city": "New York",
    "begin": "2019-11-04T00:00:00.000+02:00",
    "end": "2019-11-07T00:00:00.000+02:00"
  }
}
----

The travels service enables users to book a trip to a certain destination, including flight and hotel. A rule set determines whether a visa is required for the specified destination. The visa approval logic is then implemented as needed by the visas service.

For this tutorial, use the application interfaces for the travels and visas services to book a trip from one country to another and approve the required visa.

.Prerequisites
* You have deployed your {PRODUCT} services on OpenShift using a binary build, as described in xref:proc-kogito-travel-agency-deploy-binary_kogito-deploying-on-openshift[].

.Procedure
. In the OpenShift web console, use the left menu to navigate to *Networking* -> *Routes* and select the *Location* URL for the `travels` route to view the main page of the {PRODUCT} travel agency application:
+
--
.Routes for available services in web console
image::kogito/openshift/kogito-ocp-app-routes-agency.png[Image of Routes page in web console]

.Travel agency application interface
image::kogito/openshift/kogito-ocp-travel-agency-app.png[Image of Travel Agency application main page]
--
. In the travel agency application interface, click *Plan new trip*, enter details for a trip from one country to another, and click *Book your trip* to finish.
+
--
Ensure that the *Nationality* is different from the destination country so that a visa is required.

This example uses a traveler from Poland who is traveling to the United States:

.Book a new trip
image::kogito/openshift/kogito-travel-agency-plan-new-trip.png[Image of new trip details]

The new trip is displayed in the main page of the application interface:

.New trip listed in main page of application interface
image::kogito/openshift/kogito-travel-agency-new-trip-created.png[Image of new trip in application main page]
--
. Next to the new trip, click *Tasks* to view the pending tasks for that trip.
+
.Tasks for the new trip
image::kogito/openshift/kogito-travel-agency-task-visaapplication.png[Image of new trip tasks]

. Next to the *VisaApplication* task, click *Apply*, enter random passport details in the application window, and click *Submit application*.
+
.Submit visa application
image::kogito/openshift/kogito-travel-agency-task-visaapplication-form.png[Image of new trip tasks]
. In the OpenShift web console, use the left menu to navigate to *Networking* -> *Routes* and select the *Location* URL for the `visas` route to view the main page of the {PRODUCT} visas application.
+
--
The traveler visa application is displayed in the visas service interface.

.New visa application listed in visas service interface
image::kogito/openshift/kogito-visas-application.png[Image of new visa application in visas service interface]
--
. Next to the listed visa application, click *Approval* -> *Approve* to approve the visa application.
+
.Approve visa application
image::kogito/openshift/kogito-visas-task-approval.png[Image of visa application approval]
. Return to the travel agency application interface, and next to the approved trip, click *Tasks* and then click *Complete* for the pending *ConfirmTravel* task to complete the trip:
+
--
.Trip listed and visa approved
image::kogito/openshift/kogito-travel-agency-visa-approved.png[Image of approved trip]

.Complete trip confirmation task
image::kogito/openshift/kogito-travel-agency-task-confirm-travel.png[Image of pending completion task]

.Trip confirmed and complete
image::kogito/openshift/kogito-travel-agency-travel-complete.png[Image of completed trip]

Now that the application activity is complete, you can inspect the data that was created and stored by the {PRODUCT} Data Index Service that you configured for your application. In this case, you can search for data from booked trips and visa applications (`Travels` and `VisaApplications`) or for data from the underlying processes and tasks (`ProcessInstances` and `UserTaskInstances`).
--
. In the OpenShift web console, use the left menu to navigate to *Networking* -> *Routes* and select the *Location* URL for the `kogito-data-index` route to view the {PRODUCT} Data Index Service GraphQL interface (GraphiQL):
+
.GraphiQL interface for Data Index Service
image::kogito/openshift/kogito-data-index-graphiql.png[Image of Data Index GraphiQL interface]
. In the GraphiQL interface, enter any of the following GraphQL queries to retrieve stored application data:

* Retrieve data from booked trips (`Travels`):
+
--
.Example query
[source]
----
{ Travels {
  id, traveller {
    email
    firstName
    lastName
    nationality
  }, trip {
    begin
    city
    country
    end
    visaRequired
  }
} }
----

.Example response
image::kogito/openshift/kogito-data-index-graphiql-travels.png[Image of GraphQL query and response for travels]
--
* Retrieve data from booked trips by traveler last name (`like: "Chop*"`):
+
--
.Example query
[source]
----
{ Travels (where: {traveller: {lastName: {like: "Chop*"}}}) {
  id, traveller {
    email
    firstName
    lastName
    nationality
  }, trip {
    begin
    city
    country
    end
    visaRequired
  }
} }
----
--
* Retrieve data from visa applications (`VisaApplications`):
+
--
.Example query
[source]
----
{ VisaApplications {
  visaApplication {
    approved
    city
    country
    duration
    firstName
    lastName
    nationality
    passportNumber
  }
} }
----

.Example response
image::kogito/openshift/kogito-data-index-graphiql-visaapplications.png[Image of GraphQL query and response for visa applications]
--
* Retrieve data from process instances (`ProcessInstances`):
+
--
.Example query
[source]
----
{ ProcessInstances {
  id,
  processId,
  processName,
  state,
  nodes {
    name,
    type,
    enter,
    exit
  }
} }
----

.Example response
image::kogito/openshift/kogito-data-index-graphiql-process-instances.png[Image of GraphQL query and response for process instances]
--
* Retrieve data from user task instances (`UserTaskInstances`):
+
--
.Example query
[source]
----
{ UserTaskInstances {
  name,
  priority,
  processId,
  processInstanceId
} }
----

.Example response
image::kogito/openshift/kogito-data-index-graphiql-usertask-instances.png[Image of GraphQL query and response for user task instances]
--
////
=== {PRODUCT} Github bot with serverless workflow

<Start here>

=== {PRODUCT} temperature conversion with serverless workflow

<Start here>

=== {PRODUCT} onboarding services

<Start here>
////

[id="con-kogito-operator-architecture_{context}"]
== {PRODUCT} Operator architecture

[role="_abstract"]
The {PRODUCT} Operator is written in https://golang.org/[Go] and is built with the https://sdk.operatorframework.io/[Operator SDK]. The {PRODUCT} Operator uses the https://kubernetes.io/docs/concepts/overview/kubernetes-api/[Kubernetes API] for most of the deployment tasks that the operator facilitates and for other internal operations.

The {PRODUCT} Operator uses the following custom resources to deploy {PRODUCT} domain-specific services (the services that you develop), {PRODUCT} supporting services, and middleware infrastructure components:

* `KogitoBuild`: Defines the build configurations for {PRODUCT} domain-specific services deployed on OpenShift
* `KogitoRuntime`: Defines the deployment configurations for {PRODUCT} domain-specific services deployed on Kubernetes or OpenShift from a container image in an image registry
* `KogitoSupportingService`: Defines the deployment configurations for {PRODUCT} supporting services, such as the {PRODUCT} Data Index Service, Jobs Service, and Management Console
* `KogitoInfra`: Defines the relevant middleware infrastructure component or third-party operator for the {PRODUCT} service, such as Infinispan and MongoDB for persistence or Apache Kafka for messaging

[role="_additional-resources"]
.Additional resources
* https://github.com/kiegroup/kogito-operator[{PRODUCT} Operator source]
* https://github.com/kiegroup/kogito-operator/tree/master/examples[{PRODUCT} Operator deployment examples]
* https://sdk.operatorframework.io/docs/[Operator SDK documentation]
* https://github.com/operator-framework/operator-sdk/tree/master/example[Operator SDK examples]

=== {PRODUCT} Operator dependencies on third-party operators

The {PRODUCT} Operator uses the following third-party operators to auto-configure the {PRODUCT} service infrastructure components:

* *https://github.com/infinispan/infinispan-operator[Infinispan Operator]*: Used to interact with Infinispan Server instances for process data persistence in the {PRODUCT} services
* *https://github.com/strimzi/strimzi-kafka-operator[Strimzi Operator]*: Used to interact with Apache Kafka clusters with Zookeeper for messaging in {PRODUCT} services
* *https://github.com/keycloak/keycloak-operator[Keycloak Operator]*: Used to interact with Keycloak server instances for security and single sign-on capabilities in {PRODUCT} services
* *https://github.com/mongodb/mongodb-kubernetes-operator[MongoDB Operator]*: Used to interact with MongoDB instances for process data persistence in {PRODUCT} services

NOTE: You can find a list of third-party operators tested using the {PRODUCT} Operator on https://github.com/kiegroup/kogito-operator/tree/v1.10.0#kogito-operator-tested-integrations[{PRODUCT} Operator README]


When you enable an infrastructure mechanism through `KogitoInfra` deployment, the {PRODUCT} Operator uses the relevant third-party operator to configure the infrastructure.

You must define your custom infrastructure resource and link it in the `KogitoInfra`. You can specify your custom infrastructure resource in the `spec.resource.name` and `spec.resource.namespace` configurations:

.Example {PRODUCT} infrastructure resource for custom messaging
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoInfra
metadata:
  name: my-kafka-infra
spec:
  resource:
    apiVersion: kafka.strimzi.io/v1beta2
    kind: Kafka
    name: my-kafka-instance
    namespace: my-namespace
----

In this example, the `KogitoInfra` custom resource connects to the Kafka cluster named `my-kafka-instance` from the `my-namespace` for event messaging.

Similarly, you can define a `KogitoInfra` resource for MongoDB to allow your processes to connect to it. In this case, you also need to define extra properties into the `KogitoInfra` resource for the {PRODUCT} Operator to interact with the MongoDB instance:

.Example {PRODUCT} infrastructure resource for MongoDB
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoInfra
metadata:
  name: my-mongodb-infra
spec:
  resource:
    apiVersion: mongodb.com/v1
    kind: MongoDB
    name: custom-mongodb
    namespace: my-namespace
  infraProperties:
    username: kogitouser
    database: kogitodatabase
----

The {PRODUCT} Data Index Service similarly depends on Infinispan and Kafka infrastructure components. Without Infinispan or MongoDB persistence and Kafka messaging, the Data Index Service cannot function properly. However, you can specify whether the Data Index Service uses the general infrastructure components that the {PRODUCT} Operator generates or a custom alternative for that component.

For example, by default, the `KogitoSupportingService` custom resource that contains the `DataIndex` service type specifies the `infra` configuration for both Infinispan and Kafka in order to use the corresponding infrastructure components:

.Default `DataIndex` resource configuration with Infinispan persistence and messaging
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoSupportingService
metadata:
  name: data-index
spec:
  serviceType: DataIndex
  replicas: 1
  infra:
     - kogito-infinispan-infra
     - kogito-kafka-infra
----

You typically create `kogito-infinispan-infra` and `kogito-kafka-infra` custom resources before configuring the `KogitoSupportingService` custom resource that contains the `DataIndex` service type.

You can also connect the {PRODUCT} Data Index Service to a MongoDB instance instead:

.Default `DataIndex` resource configuration with MongoDB persistence and messaging
[source,yaml]
----
apiVersion: app.kiegroup.org/v1beta1
kind: KogitoSupportingService
metadata:
  name: data-index
spec:
  serviceType: DataIndex
  replicas: 1
  infra:
     - my-mongodb-infra
     - kogito-kafka-infra
----

=== {PRODUCT} Operator core package structure

The {PRODUCT} Operator uses the following core packages. Your understanding of the {PRODUCT} Operator package structure can help you use the operator more effectively or contribute to the development of the operator.

.{PRODUCT} Operator core package structure
image::kogito/openshift/kogito-operator-packages.png[Image of Kogito Operator package layout]

The following list describes the function and interaction of these core packages:

* `cmd`: Contains the operator entry point and CLI implementation
** `manager`: Serves as the entry point for the {PRODUCT} Operator image
** `kogito`: Provides the implementation for the {PRODUCT} CLI
* `test`: Contains the implementation for Behavior Driven Development (BDD) tests based on https://github.com/cucumber/godog[Godog] (by Cucumber for Go)
** `config`: Provides the configuration for BDD tests
** `features`: Defines the features for BDD tests
** `framework`: Provides the support API framework to interact with other operator components
** `steps`: Defines BDD test steps
* `pkg`: Contains the implementation for the {PRODUCT} Operator
** `apis`: Defines the custom resource definition types for the resources that are managed by the {PRODUCT} Operator
** `client`: Serves as a wrapper for the Kubernetes and OpenShift clients
** `controller`: Defines the business logic for how the {PRODUCT} Operator responds to changes to the resources that are managed by the operator
** `framework`: Provides the common code related to any Kubernetes operator for all controllers
** `infrastructure`: Provides the common code related to the {PRODUCT} Operator infrastructure for all controllers, such as external endpoints among the services that are managed by the operator
** `logger`: Provides the implementation for the common logger for all other packages, based on https://github.com/uber-go/zap[Zap] (by Uber Go)
** `util`: Provides the common https://golang.org/[Go] utilities used across the project

To explore {PRODUCT} Operator packages or contribute to the operator development, see the https://github.com/kiegroup/kogito-operator[{PRODUCT} Operator source] repository in GitHub.
