[id="chap-kogito-configuring"]
= Configuring {PRODUCT} supporting services and runtime capabilities
ifdef::context[:parent-context: {context}]
:context: kogito-configuring

// Purpose statement for the assembly
[role="_abstract"]
As a developer of business processes and decisions, you can configure {PRODUCT} supporting services and runtime properties for advanced use cases with your {PRODUCT} services.

// Modules - concepts, procedures, refs, etc.
[id="con-kogito-supporting-services-and-configuration_{context}"]
== {PRODUCT} supporting services and runtime configuration

[role="_abstract"]
{PRODUCT} supporting services consist of middleware infrastructure services and other dedicated services that help you build additional functionality in the {PRODUCT} domain-specific services that you develop.

{PRODUCT} supports the following key middleware infrastructure services:

* Infinispan persistence
* Apache Kafka reactive messaging

{PRODUCT} also provides the following dedicated services:

* {PRODUCT} Data Index Service indexing and querying
* {PRODUCT} Jobs Service job scheduling

The {PRODUCT} runtime supports various configuration options for these supporting services and for other capabilities, such as the following examples:

* Custom event listeners
* Prometheus metrics monitoring
* Process instance management

These supporting services, runtime configurations, and {PRODUCT} add-on components enable you to optimize your {PRODUCT} domain-specific services for your business automation requirements.

[id="ref-kogito-runtime-properties_{context}"]
== {PRODUCT} runtime properties quick reference

[role="_abstract"]
The following table serves as a quick reference for commonly used runtime configuration properties supported by {PRODUCT}. You can define these properties in the `src/main/resources/application.properties` file of the relevant {PRODUCT} project or by using the `-D` prefix during application start-up.

NOTE: Some of these properties might require accompanying dependencies in the relevant {PRODUCT} project to enable the specified capability. For more information about dependency requirements, review the sections of the {PRODUCT} configuration documentation that relate to that property.

.Common runtime properties in {PRODUCT}
[cols="15%,45%,40%"]
|===
|Relevance |Property |Description

.3+|Events
|`kogito.events.processinstances.enabled`
a|Determines whether runtime events are published for process instances, either `true` or `false`

Default value: `true`

Example: `kogito.events.processinstances.enabled=true`

a|`kogito.events.usertasks.enabled`
|Determines whether runtime events are published for user task instances, either `true` or `false`

Default value: `true`

Example: `kogito.events.usertasks.enabled=true`

a|`kogito.events.variables.enabled`
|Determines whether runtime events are published for process instances variables, either `true` or `false`

Default value: `true`

Example: `kogito.events.variables.enabled=true`

a|`kogito.messaging.as-cloudevents`
|Determines whether messages (sent or received through message events) are published in CloudEvents format, either `true` of `false`

Example: `kogito.messaging.as-cloudevents=true`

.3+|Infinispan persistence
a|`quarkus.infinispan-client.server-list`

For Spring Boot: `infinispan.remote.server-list`
a|Defines the location where an Infinispan Server is running, typically used to connect your application to Infinispan for persistence

Example: `quarkus.infinispan-client.server-list=localhost:11222`

For Spring Boot: `infinispan.remote.server-list=127.0.0.1:11222`

a|`quarkus.infinispan-client.auth-username`

`quarkus.infinispan-client.auth-password`
|Identifies the Infinispan user name and password to authenticate Infinispan persistence capabilities in the relevant application, if required, such as in the {PRODUCT} Data Index Service

Examples:

`quarkus.infinispan-client.auth-username=admin`

`quarkus.infinispan-client.auth-password=admin123`

|`kogito.persistence.infinispan.template`
|Defines an optional template name of the Infinispan cache configuration to be used to persist process instance data

Example: `kogito.persistence.infinispan.template=MyTemplate`

|Kafka messaging
a|Incoming:

`mp.messaging.incoming.kogito_incoming_stream.connector`

`mp.messaging.incoming.kogito_incoming_stream.topic` (Optional, defaults to `kogito_incoming_stream`)

`mp.messaging.incoming.kogito_incoming_stream.value.deserializer`

Outgoing:

`mp.messaging.outgoing.kogito_outgoing_stream.connector`

`mp.messaging.outgoing.kogito_outgoing_stream.topic` (Optional, defaults to `kogito_outgoing_stream`)

`mp.messaging.outgoing.kogito_outgoing_stream.value.serializer`

Spring Boot:

`kafka.bootstrapAddress`

`kogito.addon.cloudevents.kafka.kogito_incoming_stream` (Optional, defaults to `kogito_incoming_stream`)

`kogito.addon.cloudevents.kafka.kogito_outgoing_stream` (Optional, defaults to `kogito_outgoing_stream`)


a|Defines the connector, topic, and deserializer for the incoming and outgoing messages and channels for reactive messaging with Apache Kafka

Examples for incoming:

`mp.messaging.incoming.kogito_incoming_stream.connector=smallrye-kafka`

`mp.messaging.incoming.kogito_incoming_stream.topic=travellers`

`mp.messaging.incoming.kogito_incoming_stream.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer`

Examples for outgoing:

`mp.messaging.outgoing.kogito_outgoing_stream.connector=smallrye-kafka`

`mp.messaging.outgoing.kogito_outgoing_stream.topic=processedtravellers`

`mp.messaging.outgoing.kogito_outgoing_stream.value.serializer=org.apache.kafka.common.serialization.StringSerializer`

Examples for Spring Boot:

`kafka.bootstrapAddress=localhost:9092`

`kogito.addon.cloudevents.kafka.kogito_incoming_stream=travellers`

`kogito.addon.cloudevents.kafka.kogito_outgoing_stream=processedtravellers`


.7+|{PRODUCT} Jobs Service
|`kogito.service.url`
a|Defines the location where the {PRODUCT} service is deployed, typically used by the Jobs Service to find the source of the jobs

Example: `kogito.service.url=http://localhost:8080`

a|`kogito.jobs-service.url`
|Defines the callback URL that posts to a running {PRODUCT} Jobs Service

Example: `kogito.jobs-service.url=http://localhost:8085`

|`kogito.jobs-service.backoffRetryMillis`
a|(Specified in Jobs Service) Defines the retry back-off time in milliseconds between job execution attempts, in case the execution fails

Default value: `1000`

Example: `kogito.jobs-service.backoffRetryMillis=1000`

|`kogito.jobs-service.maxIntervalLimitToRetryMillis`
a|(Specified in Jobs Service) Defines the maximum interval in milliseconds when retrying to execute jobs, in case the execution fails

Default value: `60000`

Example: `kogito.jobs-service.maxIntervalLimitToRetryMillis=60000`

|`mp.messaging.outgoing.kogito-job-service-job-status-events.bootstrap.servers`
a|(Specified in Jobs Service) Identifies the Kafka bootstrap server address with the port used to publish events

Default value: `localhost:9092`

Example: `mp.messaging.outgoing.kogito-job-service-job-status-events.bootstrap.servers=localhost:9092`

|`mp.messaging.outgoing.kogito-job-service-job-status-events.topic`
a|(Specified in Jobs Service) Defines the name of the Kafka topic where the events are published

Default value: `kogito-jobs-events`

Example: `mp.messaging.outgoing.kogito-job-service-job-status-events.topic=kogito-jobs-events`

|RESTEasy
|`resteasy.jaxrs.scan-packages`
a|(Spring Boot only) Lists comma-separated package names that contain REST endpoint Java classes. Sub-packages are automatically scanned. Wildcard notation is supported. Packages generated by DMN namespaces typically start with `http`.

Example: `resteasy.jaxrs.scan-packages=org.kie.kogito.example,http{asterisk}`

|REST endpoint generation
|`kogito.generate.rest`
a|Disables the REST endpoints generation globally.

Example: `kogito.generate.rest=false`

|REST endpoint generation for a specific resource
a|
* `kogito.generate.rest.decisions`
* `kogito.generate.rest.predictions`
* `kogito.generate.rest.processes`
* `kogito.generate.rest.rules`

a|Disables the REST endpoints generation for a specific resource.

Example: In your project, which contains `.dmn` and `.bpmn` files, you can disable only the decision REST endpoints using `kogito.generate.decisions=false` property .

|Data Index service protobuf generation
|`kogito.persistence.data-index.proto.generation`
a|Disables the generation of protobuf files used by Data Index service. The protobuf files are used for domain-specific queries when the storage supports, such as Infinispan and MongoDB. In other cases, the protobuf files are not necessary and can be skipped. The default value of this property is `true`.

Example: `kogito.persistence.data-index.proto.generation=false` to disable the generation of a protobuf file.

|{PRODUCT} runtime protobuf marshaller generation
|`kogito.persistence.proto.marshaller`
a|Disables the generation of domain-specific protobuf marshaller to serialize process variables. When setting this property to `false`, {PRODUCT} leverages Java serialization to persist process variables. The default value of this property is `true`.

NOTE: The types of process variables must implement `java.io.Serializable` when protobuf marshaller generation is disabled.

NOTE: Java serialization strategy does not work when using native build.

Example: `kogito.persistence.data-index.proto.generation=false` to disable the generation of a protobuf file.

|===

[id="con-kogito-runtime-events_{context}"]
== {PRODUCT} runtime events

[role="_abstract"]
A runtime event is record of a significant change of state in the application domain at a point in time. {PRODUCT} emits runtime events as a result of successfully executed requests, or _units of work_, in a process instance or task instance in a process. {PRODUCT} can use these events to notify third parties about changes to the BPMN process instance and its data.

=== Process instance events

For every executed process instance, an event is generated that contains information for that instance, such as the following information:

* Process instance metadata, such as the process definition ID, process instance ID, process instance state, and other identifying information
* Node instances that have been triggered during the execution
* Variables used and the current state of variables after the execution

These events provide a complete view of the process instances being executed and can be consumed by an event listener, such as a `ProcessEventListener` configuration.

If multiple processes are executed within a single request (unit of work), each process instance is given a dedicated event.

The following event is an example process instance event generated after the request was executed successfully:

.Example process instance event
[source,json]
----
{
  "specversion": "0.3",
  "id": "f52af50c-4fe2-4581-9184-7ad48137fb3f",
  "source": null,
  "type": "ProcessInstanceEvent",
  "time": "2019-08-05T17:47:49.019494+02:00[Europe/Warsaw]",
  "data": {
    "id": "c1aced49-399b-4938-9071-b2ffa3fb7045",
    "parentInstanceId": null,
    "rootInstanceId": null,
    "processId": "deals",
    "processName": "SubmitDeal",
    "startDate": 1565020069015,
    "endDate": null,
    "state": 1,
    "nodeInstances": [
      {
        "id": "a8fe24c4-27a5-4869-85df-16e9f170f2c4",
        "nodeId": "2",
        "nodeDefinitionId": "CallActivity_1",
        "nodeName": "Call a deal",
        "nodeType": "SubProcessNode",
        "triggerTime": 1565020069015,
        "leaveTime": null
      },
      {
        "id": "7a3bf1b1-b167-4928-969d-20bddf16c87a",
        "nodeId": "1",
        "nodeDefinitionId": "StartEvent_1",
        "nodeName": "StartProcess",
        "nodeType": "StartNode",
        "triggerTime": 1565020069015,
        "leaveTime": 1565020069015
      }
    ],
    "variables": {
      "name": "my fancy deal",
      "traveller": {
        "firstName": "John",
        "lastName": "Doe",
        "email": "jon.doe@example.com",
        "nationality": "American",
        "address": {
          "street": "main street",
          "city": "Boston",
          "zipCode": "10005",
          "country": "US"
        }
      }
    }
  },
  "kogitoprocinstanceid": "c1aced49-399b-4938-9071-b2ffa3fb7045",
  "kogitoparentprociid": null,
  "kogitorootprociid": null,
  "kogitoprocid": "deals",
  "kogitoprocist": "1"
}
----

The event is in https://cloudevents.io/[CloudEvents] format so that it can be consumed efficiently by other entities.

The event data also includes the following extensions to enable event routing based on the event metadata without requiring the body of the event:

* `kogitoprocinstanceid`
* `kogitoparentprociid`
* `kogitorootprociid`
* `kogitoprocid`
* `kogitoprocist`

=== User task instance events

If an executed request (unit of work) in a process instance interacts with a user task, an event is generated for that user task and contains information for the task instance, such as the following information:

* Task metadata, such as the task description, priority, start and complete dates, and other identifying information
* Task input and output data
* Task assignments, such as the task owner, potential users and groups, business administrator and business administrator groups, or excluded users
* Task reference name that should be used to interact with the task using the {PRODUCT} service endpoints

The following event is an example user task instance event generated after the relevant request was executed successfully:

.Example user task instance event
[source,json]
----
{
  "data": {
    "adminGroups": [],
    "adminUsers": [],
    "excludedUsers": [],
    "id": "4d899471-19dd-485d-b7f4-b313185d430d",
    "inputs": {
      "Locale": "en-UK",
      "trip": {
        "begin": "2019-09-22T22:00:00Z[UTC]",
        "city": "Boston",
        "country": "US",
        "end": "2019-09-26T22:00:00Z[UTC]",
        "visaRequired": true
      },
      "TaskName": "VisaApplication",
      "NodeName": "Apply for visa",
      "Priority": "1",
      "Skippable": "true",
      "traveller": {
        "address": {
          "city": "Krakow",
          "country": "Poland",
          "street": "Polna",
          "zipCode": "12345"
        },
        "email": "jan.kowalski@email.com",
        "firstName": "Jan",
        "lastName": "Kowalski",
        "nationality": "Polish"
      }
    },
    "outputs": {},
    "potentialGroups": [],
    "potentialUsers": [],
    "processId": "travels",
    "processInstanceId": "63c297cb-f5ac-4e20-8254-02f37bd72b80",
    "referenceName": "VisaApplication",
    "startDate": "2019-09-16T15:22:26.658Z[UTC]",
    "state": "Ready",
    "taskName": "Apply for visa",
    "taskPriority": "1"
  },
  "id": "9c340cfa-c9b6-46f2-a048-e1114b077a7f",
  "kogitoprocid": "travels",
  "kogitoprocinstanceid": "63c297cb-f5ac-4e20-8254-02f37bd72b80",
  "kogitousertaskiid": "4d899471-19dd-485d-b7f4-b313185d430d",
  "kogitousertaskist": "Ready",
  "source": "http://localhost:8080/travels",
  "specversion": "0.3",
  "time": "2019-09-16T17:22:26.662592+02:00[Europe/Berlin]",
  "type": "UserTaskInstanceEvent"
}
----

The event data also includes the following extensions to enable event routing based on the event metadata without requiring the body of the event:

* `kogitousertaskiid`
* `kogitousertaskist`
* `kogitoprocinstanceid`
* `kogitoprocid`

=== Event publishing

{PRODUCT} generates events only when at least one publisher is configured. A {PRODUCT} service environment can have many event publishers that publish these events into different channels.

By default, {PRODUCT} includes the following message-based event publishers, depending on your application framework:

* *For Quarkus*: https://smallrye.io/smallrye-reactive-messaging/[Reactive Messaging] for sending events using Apache Kafka, Apache Camel, Advanced Message Queuing Protocol (AMQP), or MQ Telemetry Transport (MQTT)
* *For Spring Boot*: https://spring.io/projects/spring-kafka[Spring for Apache Kafka] for sending events using Kafka

To enable or disable event publishing, you can adjust the following properties in the `src/main/resources/application.properties` file in your {PRODUCT} project:

* `kogito.events.processinstances.enabled`: Enables or disables publishing process instance events (default: `true`)
* `kogito.events.usertasks.enabled`: Enables or disables publishing user task instance events (default: `true`)
* `kogito.events.variables.enabled`: Enables or disables publishing process instances variables events (default: `true`)

To develop additional event publishers, you can implement the `org.kie.kogito.event.EventPublisher` implementation and include the required annotations for JavaBeans discovery.

////
//@comment: Excluded for now because not yet supported in Kogito. Will be in its own topic. (Stetson, 1 Apr 2020)
## Registering work item handlers

To be able to use custom service tasks a work item handler must be registered. Once the work item handler is implemented to can be either packaged in the application itself or as dependency of the application.

`WorkItemHandlerConfig` class should be created to provide custom work item handlers. It must implement `org.kie.kogito.process.WorkItemHandlerConfig` although recommended is to always extend the default implementation (`org.kie.kogito.process.impl.DefaultWorkItemHandlerConfig`) to benefit from the out of the box provided handlers as well.

[source, java]
----
@ApplicationScoped
public class CustomWorkItemHandlerConfig extends DefaultWorkItemHandlerConfig {{
    register("MyServiceTask", new MyServiceWorkItemHandler());
}}
----

NOTE: These classes are meant to be injectable so ensure you properly annotate the class (`@ApplicationScoped`/`@Component`) so they can be found and registered.

You can also take advantage of lifecycle method like `@PostConstruct` and `@PreDestroy` to manage your handlers.
////

// tag::proc-messaging-enabling[]
[id="proc-messaging-enabling_{context}"]
=== Enabling Kafka messaging for {PRODUCT} services

[role="_abstract"]
{PRODUCT} supports the https://github.com/eclipse/microprofile-reactive-messaging[MicroProfile Reactive Messaging] specification for messaging in your services. You can enable messaging to configure message events as either input or output of business process execution.

For example, the following `handle-travelers.bpmn2` process uses messaging start and end events to communicate with travelers:

.Example process with messaging start and end events
image::kogito/bpmn/bpmn-messaging-example.png[Image of message-based process]

In this example, the message start and end events require the following information:

* Message name that maps to the channel that delivers messages
* Message payload that maps to a process instance variable

.Example message configuration for start event
image::kogito/bpmn/bpmn-messaging-start-event.png[Image of message start event data]

.Example message configuration for end event
image::kogito/bpmn/bpmn-messaging-end-event.png[Image of message end event data]

For this procedure, the messaging is based on https://kafka.apache.org/[Apache Kafka] as the event publisher, so you must have Kafka installed in order to enable messaging. Your marshalling configuration depends on the messaging solution that you use.

.Prerequisites
* https://kafka.apache.org/[Apache Kafka] is installed and includes any required topics. For information about Kafka installation and configuration, see the https://kafka.apache.org/documentation/[Apache Kafka documentation].

.Procedure
. Add the following dependencies to the `pom.xml` file of your {PRODUCT} project:
+
--
.On Quarkus
[source, xml]
----
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-smallrye-reactive-messaging-kafka</artifactId>
</dependency>
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-cloudevents</artifactId>
</dependency>
----

.On Spring Boot
[source,xml]
----
<dependency>
  <groupId>org.springframework.kafka</groupId>
  <artifactId>spring-kafka</artifactId>
</dependency>
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-springboot-cloudevents</artifactId>
</dependency>
<dependency>
  <groupId>com.fasterxml.jackson.core</groupId>
  <artifactId>jackson-databind</artifactId>
</dependency>
----
--
. Configure the incoming and outgoing messaging channels and properties:
+
--
* *On Quarkus*: Add the following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure the incoming and outgoing messages and channels:
+
.Configure incoming and outgoing messages and channels
[source]
----
mp.messaging.incoming.kogito_incoming_stream.connector=smallrye-kafka
mp.messaging.incoming.kogito_incoming_stream.topic=travellers
mp.messaging.incoming.kogito_incoming_stream.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
mp.messaging.outgoing.kogito_outgoing_stream.connector=smallrye-kafka
mp.messaging.outgoing.kogito_outgoing_stream.topic=processedtravellers
mp.messaging.outgoing.kogito_outgoing_stream.value.serializer=org.apache.kafka.common.serialization.StringSerializer
----
+
Replace `travellers` with the name of the message start event.
Replace `processedtravellers` with the name of the message end event.
+
[NOTE]
====
To prevent execution errors due to long wait times with messaging, you can also use the following property to disable waiting for message completion:

.Disable message wait time
[source]
----
mp.messaging.outgoing.[channel-name].waitForWriteCompletion=false
----
====

* *On Spring Boot*: Add the following property to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure the messaging channel:
+
.Configure messaging channel
[source]
----
kafka.bootstrapAddress=localhost:9092
kogito.addon.cloudevents.kafka.kogito_incoming_stream=travellers
kogito.addon.cloudevents.kafka.kogito_outgoing_stream=processedtravellers
----
+
--

NOTE: As an alternative to enabling Kafka messaging explicitly in {PRODUCT} services, you can use the {PRODUCT} Operator to install the Kafka infrastructure and enable messaging for the service during deployment on OpenShift. For more information about enabling Kafka messaging with the {PRODUCT} Operator, see {URL_DEPLOYING_ON_OPENSHIFT}#con-kogito-operator-with-kafka_kogito-deploying-on-openshift[_{DEPLOYING_ON_OPENSHIFT}_].

For example {PRODUCT} services with Kafka messaging, see the following example applications in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-kafka-quickstart-quarkus[`process-kafka-quickstart-quarkus`]: Example on Quarkus
* https://github.com/kiegroup/kogito-examples/tree/stable/process-kafka-quickstart-springboot[`process-kafka-quickstart-springboot`]: Example on Spring Boot
// end::proc-messaging-enabling[]

[id="proc-events-mongodb-debezium_{context}"]
=== Applying outbox pattern in {PRODUCT} events using MongoDB and Debezium

[role="_abstract"]
To avoid the data inconsistencies in process services, the {PRODUCT} services must update the process data and publish events within a unit of work entirely, but not partially. However, Apache Kafka does not support distributed transactions. In order to resolve this problem, you can apply the outbox pattern with {PRODUCT} service.

When you store the data of a process service in a unit of work, you can also add the records that represent the events to be sent into the outbox tables in the same transaction. After that, a separate message relay process publishes the events that are added into the outbox tables to Kafka broker asynchronously. For more information about the outbox pattern, see https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/[Debezium blog].

https://debezium.io/[Debezium] is useful for message relay, which comes with `Change Data Capture` connectors for various databases, such as MongoDB. It captures new entries in MongoDB collections and stream those entries to Apache Kafka.

{PRODUCT} enables you to store the data of a process and events of the same unit of work in a single transaction using MongoDB.

For this procedure, a {PRODUCT} service is configured to store the process data and events in a single transaction using MongoDB. Also, a Debezium MongoDB connector is configured and started to stream the events from MongoDB collections to Apache Kafka topics.

.Prerequisites
* https://kafka.apache.org/[Apache Kafka] is installed and includes required topics. For information about Kafka installation and configuration, see the https://kafka.apache.org/documentation/[Apache Kafka documentation].
* https://www.mongodb.com/[MongoDB] version 4.4 or later is installed, started, and `oplog` is enabled. For information about MongoDB setting up for Debezium, see the https://debezium.io/documentation/reference/connectors/mongodb.html#setting-up-mongodb[MongoDB connector documentation].
* https://debezium.io/[Debezium] version 1.7 or later is installed and started. For information about Debezium installation and configuration, see the https://debezium.io/documentation/reference/[Debezium documentation].

.Procedure
. Add the following dependencies to the `pom.xml` file of your {PRODUCT} project to store {PRODUCT} events to MongoDB:
+
--
.On Quarkus
[source, xml]
----
<dependency>
    <groupId>org.kie.kogito</groupId>
    <artifactId>kogito-addons-quarkus-events-mongodb</artifactId>
</dependency>
----

.On Spring Boot
[source,xml]
----
<dependency>
    <groupId>org.kie.kogito</groupId>
    <artifactId>kogito-addons-springboot-events-mongodb</artifactId>
</dependency>
----
--

. Configure the MongoDB database and collections for {PRODUCT} services to store the {PRODUCT} events to the MongoDB collections:
+
--
Add properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure the MongoDB database and collections as shown in the following example:

.Properties to configure MongoDB database
[source]
----
kogito.events.database=kogito
kogito.events.processinstances.collection=kogitoprocessinstancesevents
kogito.events.usertasks.collection=kogitousertaskinstancesevents
kogito.events.variables.collection=kogitovariablesevents
----
--

. Enable MongoDB persistence for {PRODUCT} services to use MongoDB as a runtime persistence. For more information, see xref:proc-mongodb-persistence-enabling_kogito-configuring[].

. Build your {PRODUCT} project and start the {PRODUCT} service.
+
--
[NOTE]
====
Ensure that you enable MongoDB transaction by adding the following property to the `src/main/resources/application.properties` file in your {PRODUCT} project:

.Property to enable MongoDB transaction
[source]
----
kogito.persistence.transaction.enabled=true
----
====
When you add the previous property to your {PRODUCT} project, the process data and events that are generated in the same unit of work, are stored to MongoDB in the same transaction. This procedure avoids potential inconsistencies.
--

. Start Debezium MongoDB connector to stream {PRODUCT} events from MongoDB collections to Kafka topics:
+
--
Follow the https://debezium.io/documentation/reference/connectors/mongodb.html#mongodb-deploying-a-connector[Debezium MongoDB connector deployment instructions] to install the Debezium MongoDB connector, configure the connector, and start the connector.

Use the following example configuration to configure the Debezium MongoDB connector, which captures the events from MongoDB collections and stream the events to Kafka topics:

.Example configuration for Debezium MongoDB connector
[source]
----
{
  "name": "kogito-connector",
  "config": {
    "connector.class" : "io.debezium.connector.mongodb.MongoDbConnector",
    "tasks.max" : "1",
    "mongodb.hosts" : "rs0/mongodb:27017",
    "mongodb.name" : "dbserver1",
    "mongodb.user" : "debezium",
    "mongodb.password" : "dbz",
    "database.include" : "kogito",
    "database.history.kafka.bootstrap.servers" : "kafka:9092",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "collection.include.list": "kogito.kogitoprocessinstancesevents,kogito.kogitousertaskinstancesevents,kogito.kogitovariablesevents",
    "transforms": "unwrap,reroute",
    "transforms.unwrap.type": "io.debezium.connector.mongodb.transforms.ExtractNewDocumentState",
    "transforms.unwrap.array.encoding": "array",
    "transforms.unwrap.drop.tombstones": "false",
    "transforms.unwrap.delete.handling.mode": "drop",
    "transforms.unwrap.operation.header": "false",
    "transforms.reroute.type": "io.debezium.transforms.ByLogicalTableRouter",
    "transforms.reroute.topic.regex": "(.*)kogito(.*)events(.*)",
    "transforms.reroute.topic.replacement": "kogito-$2-events",
    "transforms.reroute.key.enforce.uniqueness": "false",
    "skipped.operations": "u,d",
    "tombstones.on.delete": "false"
  }
}
----

In the previous example configuration, replace the server host, port, replica set, database, and collections information according to your MongoDB, Kafka installation, and {PRODUCT} project configuration.
--

You can also use the following example applications for consistency in a {PRODUCT} service using MongoDB and Debezium:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-outbox-mongodb-quarkus[`process-outbox-mongodb-quarkus`]: Example on Quarkus
* https://github.com/kiegroup/kogito-examples/tree/stable/process-outbox-mongodb-springboot[`process-outbox-mongodb-springboot`]: Example on Spring Boot

[id="proc-event-listeners-registering_{context}"]
=== Registering event listeners

[role="_abstract"]
You can register custom event listeners to detect and publish events that are not published by {PRODUCT} by default. Your custom event listener configuration must implement the relevant implementation for either processes or rules.

.Procedure
. Create an event listener configuration class for either process or rule events, such as a `ProcessEventListenerConfig` class or a `RuleEventListenerConfig` class.
. In your event listener configuration class, extend the default implementation of the configuration class as part of your listener definition:
+
--
* Implementation for process events: `org.kie.kogito.process.impl.DefaultProcessEventListenerConfig`
* Implementation for rule events: `org.drools.core.config.DefaultRuleEventListenerConfig`

.Example process event listener with extended default implementation
[source, java]
----
@ApplicationScoped
public class ProcessEventListenerConfig extends DefaultProcessEventListenerConfig {

    public ProcessEventListenerConfig() {
        super(new CustomProcessEventListener());
    }
}
----

.Example rule event listener with extended default implementation
[source, java]
----
@ApplicationScoped
public class RuleEventListenerConfig extends DefaultRuleEventListenerConfig {

    public RuleEventListenerConfig() {
        super(new CustomRuleEventListener());
    }
}
----

NOTE: These configuration classes must be injectable, so ensure that you properly annotate the classes, such as with the annotations `@ApplicationScoped` or `@Component`, so that they can be found and registered.

Alternatively, you can implement the relevant event listener interface instead of extending the default implementation, but this approach excludes other listeners provided by {PRODUCT}.

* Interface for process events: `org.kie.kogito.process.ProcessEventListenerConfig`
* Interface for rule events: `org.kie.kogito.rules.RuleEventListenerConfig`
--
. After the event listener is configured, package the listener configuration class in the `src/main/java` folder of the relevant application or add it as dependency in the `pom.xml` file of the application to make the listener available.

== Metrics monitoring in {PRODUCT} services

[role="_abstract"]
{PRODUCT} supports metrics monitoring modules powered by https://micrometer.io/[Micrometer], so you can export your metrics to any monitoring system supported by Micrometer. The primary monitoring module in {PRODUCT} is based on https://prometheus.io/[Prometheus], which enables you to collect and store metrics related to your {PRODUCT} assets, and then visualize those metrics through a configured data-graphing tool such as https://grafana.com/[Grafana].

As an alternative to Prometheus, you can also use https://www.elastic.co/elastic-stack[Elasticsearch] metrics monitoring with your {PRODUCT} services, or you can subscribe to any Micrometer registry using an API exposed by the {PRODUCT} monitoring library.

[id="proc-prometheus-metrics-monitoring_{context}"]
=== Enabling Prometheus metrics monitoring in {PRODUCT}

[role="_abstract"]
https://prometheus.io/[Prometheus] is an open source systems monitoring toolkit that you can use with {PRODUCT} to collect and store metrics related to the execution of Business Process Model and Notation (BPMN) process models, business rules, and Decision Model and Notation (DMN) decision models. You can access the stored metrics through a REST API call to a configured application endpoint, through the Prometheus expression browser, or using a data-graphing tool such as https://grafana.com/[Grafana].

.Prerequisites
* Prometheus is installed. For information about downloading and using Prometheus, see the https://prometheus.io/docs/introduction/overview/[Prometheus documentation page].

.Procedure
. In your {PRODUCT} project, depending on the framework you are using, add one of the following dependencies to the `pom.xml` file to enable the Prometheus add-on:
+
--
.Add dependency for Prometheus Quarkus add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-monitoring-prometheus</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

.Add dependency for Prometheus Springboot add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-springboot-monitoring-prometheus</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

By default, all stored metrics are exported to the configured monitoring module. If you want to disable certain metrics from being exported, you can set any of the following properties in the `application.properties` file of your {PRODUCT} project:

.Enable or disable specific metrics from being exported
[source]
----
kogito.monitoring.rule.useDefault=false
kogito.monitoring.process.useDefault=false
kogito.monitoring.interceptor.useDefault=false
----
--

. In the `prometheus.yaml` file of your Prometheus distribution, add the following settings in the `scrape_configs` section to configure Prometheus to scrape metrics from your {PRODUCT} service:
+
--
.Example scrape configurations in `prometheus.yaml` file
[source,yaml,subs="+quotes"]
----
scrape_configs:
  job_name: 'kogito-metrics'
metrics_path: /metrics
static_configs:
  - targets: ["localhost:8080"]
----

Replace the values according to your {PRODUCT} service settings.
--
. In a command terminal, navigate to your {PRODUCT} project and run the project using your preferred run mode, such as development mode:
+
--
.On Quarkus
[source]
----
mvn clean compile quarkus:dev
----

.On Sprint Boot
[source]
----
mvn clean compile spring-boot:run
----

After you start your {PRODUCT} service, Prometheus begins collecting metrics and {PRODUCT} publishes the metrics to the configured REST API endpoint.
--
. To verify the metrics configuration, use a REST client or curl utility to send a `GET` request to the configured `/metrics` endpoint, such as `\http://localhost:8080/metrics` in this example:
+
--
.Example curl command to return Prometheus metrics
[source]
----
curl -X GET http://localhost:8080/metrics
----

.Example response
[source]
----
# HELP kogito_process_instance_completed_total Completed Process Instances
# TYPE kogito_process_instance_completed_total counter
# HELP kogito_process_instance_started_total Started Process Instances
# TYPE kogito_process_instance_started_total counter
kogito_process_instance_started_total{app_id="acme-travels",process_id="travels",} 1.0
# HELP kogito_work_item_duration_seconds Work Items Duration
# TYPE kogito_work_item_duration_seconds summary
# HELP drl_match_fired_nanosecond Drools Firing Time
# TYPE drl_match_fired_nanosecond histogram
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="1000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="2000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="3000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="4000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="5000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="6000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="7000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="8000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="9000000.0",} 1.0
drl_match_fired_nanosecond_bucket{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",le="+Inf",} 1.0
drl_match_fired_nanosecond_count{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",} 1.0
drl_match_fired_nanosecond_sum{identifier="acme-travels",rule_name="Brazilian citizens require visa to Australia",} 789941.0
# HELP kogito_process_instance_sla_violated_total Process Instances SLA Violated
# TYPE kogito_process_instance_sla_violated_total counter
# HELP kogito_process_instance_duration_seconds Process Instances Duration
# TYPE kogito_process_instance_duration_seconds summary
# HELP kogito_process_instance_running_total Running Process Instances
# TYPE kogito_process_instance_running_total gauge
kogito_process_instance_running_total{app_id="acme-travels",process_id="travels",} 1.0
----

If the metrics are not available at the defined endpoint, review and verify the {PRODUCT} and Prometheus configurations described in this section.

You can also interact with your collected metrics and application targets in the Prometheus expression browser at `http://__HOST:PORT__/graph` and `http://__HOST:PORT__/targets`, or integrate your Prometheus data source with a data-graphing tool such as Grafana:

.Prometheus expression browser with {PRODUCT} service targets
image::kogito/configuration/prometheus-expression-browser-targets.png[Image of targets in Prometheus expression browser]

.Grafana dashboard with {PRODUCT} service metrics
image::kogito/configuration/prometheus-grafana-data.png[Image of application metrics in Grafana]
--

[role="_additional-resources"]
.Additional resources
* https://prometheus.io/docs/prometheus/latest/getting_started/[Getting Started with Prometheus]
* https://prometheus.io/docs/visualization/grafana/[Grafana Support for Prometheus]
* https://grafana.com/docs/grafana/latest/features/datasources/prometheus/[Using Prometheus in Grafana]

[id="con-grafana-dashboards-metrics-monitoring_{context}"]
==== Grafana dashboards for default metrics in {PRODUCT}

[role="_abstract"]
If any of the Prometheus monitoring modules are imported as dependencies in the `pom.xml` file of your {PRODUCT} project, some Grafana dashboards that use the default metrics are generated under the folder `target/classes/META-INF/resources/monitoring/dashboards/` every time you compile your {PRODUCT} service.

Two types of dashboards are exported depending on the decision model used on the endpoints:

* *Operational dashboard*: This dashboard is generated for DMN and DRL endpoints and contains the following metrics:
** Total number of requests on the endpoint
** Average number of requests per minute on the endpoint
** Quantiles on the elapsed time to evaluate the requests
** Exception details
+
.Generated operational dashboard example
image::kogito/configuration/grafana-operational-dashboard.png[Generated operational dashboard]

* *Domain-specific dashboard*: Currently this dashboard is exported only for DMN endpoints. The domain-specific dashboard contains a graph for each type of decision in the DMN model. Only the built-in types `number`, `string`, and `boolean` are currently supported.
** If the output of the decision is a `number` type, the graph contains the quantiles for that metric on a sliding window of 3 minutes.
** If the output is a `boolean` or a `string` type, the graph contains the number of occurrences for each output on a 10-minute average.
+
.Generated domain-specific dashboard example
image::kogito/configuration/grafana-domain-dashboard.png[Generated domain specific dashboard]

NOTE: Generated dashboards for BPMN resources are currently not supported.

[id="con-grafana-custom-dashboards_{context}"]
==== Custom Grafana dashboards in {PRODUCT}

[role="_abstract"]
You can add custom dashboards that are defined as `json` files in your project. For format specification and more information, see https://grafana.com/docs/grafana/latest/dashboards/json-model/[Official documentation] page.

To add custom dashboards in your {PRODUCT} project, you must follow the following conventions:

* Dashboard files must be stored in `/src/main/resources/META-INF/dashboards` directory
* dashboard file names must start with `domain-dashoboard` (for domain specific dashboards) or `operational-dashboard` (for operational dashboards)
* Dashboard file names must end with `.json`
* Dashboard file names must not conflict with the auto-generated ones
* The `title` attribute of custom dashboards must not conflict with the auto-generated attributes

Custom dashboards are available in the Grafana panel along with the auto-generated dashboards.

[id="proc-disable-dashboards-metrics-monitoring_{context}"]
==== Disabling Grafana dashboards generation in {PRODUCT}

[role="_abstract"]
You can disable the generation of default dashboards in Prometheus metrics monitoring using the following properties:

.Properties to disable default dashboards
[source]
----
kogito.grafana.disabled.operational.dashboards
kogito.grafana.disabled.domain.dashboards
----

.Prerequisites
* Prometheus is installed. For information about downloading and using Prometheus, see the https://prometheus.io/docs/introduction/overview/[Prometheus documentation page].

.Procedure

. To disable Grafana dashboard, add a comma-separated list of dashboard identifiers to the following properties:
+
--
.Properties to disable Grafana dashboard
[source]
----
kogito.grafana.disabled.operational.dashboards
kogito.grafana.disabled.domain.dashboards
----

The `kogito.grafana.disabled.operational.dashboards` property disables the generation of operational dashboards and `kogito.grafana.disabled.domain.dashboards` property disables the generation of domain dashboards.

For example, a project containing the following resources uses the default configuration and generates the default dashboards:

.Example project resources
[source]
----
Hello.drl
LoanEligibility.dmn
Traffic Violation.dmn
----

The following are the default generated dashboards in {PRODUCT}:

.Example default generated dashboards for a project
[cols="50%,50%", options="header"]
|===
|Dashboard type |Dashboard name

.3+|Operational
|`Hello`
|`LoanEligibility`
|`Traffic Violation`

.3+|Domain
|`Hello`
|`LoanEligibility`
|`Traffic Violation`
|===

If you use the following configuration in the `application.properties` file, the generation of `Hello` and `Traffic Violation` operational dashboards, and `LoanEligibility` domain dashboard is avoided.

.Configuration for disabling specific dashboards
----
kogito.grafana.disabled.operational.dashboards=Hello,Traffic Violation
kogito.grafana.disabled.domain.dashboards=LoanEligibility
----

.Customized generation of dashboards in {PRODUCT}
[cols="50%,50%"]
|===
|Dashboard type |Dashboard name

.1+|Operational
|`LoanEligibility`
.2+|Domain
|`Hello`
|`Traffic Violation`
|===

[IMPORTANT]
====
The spaces between the dashboard identifiers are eliminated as follows:

.Example dashboard identifiers with spaces
[source]
----
Hello, Traffic Violation, LoanEligibility
----

.Example dashboard identifiers without spaces
[source]
----
"Hello"
"Traffic Violation"
"LoanEligibility"
----

However, the spaces within a dashboard identifier are maintained as follows:

.Example dashboard identifiers with spaces
[source]
----
Traffic Violation, Traffic    Violation, Tra ffic Violation
----

.Example dashboard identifiers without spaces
[source]
----
"Traffic Violation"
"Traffic    Violation"
"Tra ffic Violation"
----
====

--

[id="proc-elastic-metrics-monitoring_{context}"]
=== Enabling Elasticsearch metrics monitoring in {PRODUCT}

[role="_abstract"]
https://www.elastic.co/elastic-stack[Elasticsearch] is a distributed, open source search and analytics engine that you can use with {PRODUCT} to collect and store metrics as an alternative to Prometheus.

.Prerequisites
* Elastic is installed. For information about downloading and using Elastic, see the https://www.elastic.co/guide/index.html[Elastic Stack and Product Documentation].

.Procedure
. In your {PRODUCT} project, depending on the framework you are using, add one of the following dependencies to the `pom.xml` file to enable the Elastic add-on:
+
--
.Add dependency for Elastic Quarkus add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-monitoring-elastic</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

.Add dependency for Elastic Springboot add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-springboot-monitoring-elastic</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

By default, all stored metrics are exported to the configured monitoring module. If you want to disable certain metrics from being exported, you can set any of the following properties in the `application.properties` file of your {PRODUCT} project:

.Enable or disable specific metrics from being exported
[source]
----
kogito.monitoring.rule.useDefault=false
kogito.monitoring.process.useDefault=false
kogito.monitoring.interceptor.useDefault=false
----
--

. In the `application.properties` file of your {PRODUCT} project, edit the following properties as needed to configure the Elastic add-on:
+
--
.Application properties for Elastic monitoring add-on
[cols="30%,70%", options="header"]
|===
|Property
|Description

| `kogito.addon.monitoring.elastic.host`
| Specifies the host to send metrics to.

Default value: `\http://localhost:9200`

| `kogito.addon.monitoring.elastic.index`
| Specifies the index to store metrics in.

Default value: `micrometer-metrics`

| `kogito.addon.monitoring.elastic.step`
| Sets the interval at which metrics are sent to Elastic.

Default value: Every 1 minute

| `kogito.addon.monitoring.elastic.indexDateFormat`
| Specifies the index date format used for rolling indices. This is appended to the index name, separated by the `indexDateSeparator`.

Default value: `yyyy-MM`

| `kogito.addon.monitoring.elastic.timestampFieldName`
| Defines the name of the `timestamp` field.

Default value: `@timestamp`

| `kogito.addon.monitoring.elastic.autoCreateIndex`
|  Determines whether to create the index automatically if it does not exist.

Default value: `true`

| `kogito.addon.monitoring.elastic.userName`
| Specifies the Basic Authentication user name.

| `kogito.addon.monitoring.elastic.password`
|  Specifies the Basic Authentication password.

| `kogito.addon.monitoring.elastic.pipeline`
|  Specifies the ingest pipeline name.

| `kogito.addon.monitoring.elastic.indexDateSeparator`
| Specifies the separator between the index name and the date part.

Default value: `-` (hyphen)

| `kogito.addon.monitoring.elastic.documentType`
| Specifies the type to be used when writing metrics documents to an index. This configuration is only used with Elasticsearch versions 6 and earlier.

Default value: `doc`
|===
--

[id="proc-custom-micrometer-metrics-monitoring_{context}"]
=== Enabling metrics monitoring with a custom Micrometer registry in {PRODUCT}

As an alternative to using the metrics monitoring modules provided in {PRODUCT}, you can subscribe to any Micrometer registry using an API exposed by the {PRODUCT} monitoring library. For the complete list of monitoring systems supported by Micrometer, see the https://micrometer.io/docs[Micrometer Documentation].

As an example, this procedure demonstrates how you can export metrics to the https://micrometer.io/docs/registry/atlas[Micrometer Atlas] metrics database.

.Procedure
. In your {PRODUCT} project, depending on the framework you are using, add one of the following dependencies to the `pom.xml` file to enable the monitoring core add-on:
+
--
.Add dependency for monitoring core Quarkus add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-monitoring-core</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

.Add dependency for monitoring core Springboot add-on
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-springboot-monitoring-core</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

By default, all stored metrics are exported to the configured monitoring module. If you want to disable certain metrics from being exported, you can set any of the following properties in the `application.properties` file of your {PRODUCT} project:

.Enable or disable specific metrics from being exported
[source]
----
kogito.monitoring.rule.useDefault=false
kogito.monitoring.process.useDefault=false
kogito.monitoring.interceptor.useDefault=false
----

As an alternative, if you do not want to use the default listeners and the default metrics, you can import the core common module directly:

.Add dependency for monitoring core common module
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-monitoring-core</artifactId>
  <version>__KOGITO_VERSION__</version>
</dependency>
----

[NOTE]
====
To write custom metrics, you can create custom event listeners and then register your Micrometer `Meter` object to the global `CompositeMeterRegistry` using the method `getDefaultMeterRegistry` of the class `org.kie.kogito.monitoring.core.common.MonitoringRegistry`. For more information about registering custom event listeners in {PRODUCT}, see xref:proc-event-listeners-registering_kogito-configuring[].
====
--

. Depending on the registry that you want to use, add the relevant dependency to the `pom.xml` file of your {PRODUCT} project. For this example, the Atlas registry requires the following dependency:
+
.Add dependency for Atlas registry
[source,xml,subs="+quotes"]
----
<dependency>
  <groupId>io.micrometer</groupId>
  <artifactId>micrometer-registry-atlas</artifactId>
  <version>__MICROMETER_VERSION__</version>
</dependency>
----

. Subscribe to your registry using the method `addRegistry` of the class `org.kie.kogito.monitoring.core.common.MonitoringRegistry`. For example, on Quarkus, create the following `AtlasProvider.java` class in your {PRODUCT} project:
+
.Example Java class on Quarkus to subscribe to a custom Micrometer registry for {PRODUCT} core monitoring
[source,java]
----
import java.time.Duration;

import javax.annotation.PostConstruct;
import javax.inject.Singleton;

import com.netflix.spectator.atlas.AtlasConfig;
import io.micrometer.atlas.AtlasMeterRegistry;
import io.micrometer.core.instrument.Clock;
import io.quarkus.runtime.Startup;
import org.kie.kogito.monitoring.core.common.MonitoringRegistry;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@Singleton
@Startup
public class AtlasProvider {

    private AtlasMeterRegistry registry;
    private static final Logger logger = LoggerFactory.getLogger(AtlasProvider.class);

    private AtlasProvider() {
    }

    @PostConstruct
    public void setUp() {
        AtlasConfig atlasConfig = new AtlasConfig() {
            @Override
            public Duration step() {
                return Duration.ofSeconds(10);
            }

            @Override
            public String get(String k) {
                return null; // accept the rest of the defaults
            }
        };
        registry = new AtlasMeterRegistry(atlasConfig, Clock.SYSTEM);
        MonitoringRegistry.addRegistry(registry);
        registry.start();
        logger.info("Atlas registry added to monitoring addon and started.");
    }
}
----

// tag::con-persistence[]
[id="con-persistence_{context}"]
== Persistence in {PRODUCT} services

[role="_abstract"]
{PRODUCT} supports runtime persistence for preserving process data in your services, such as active process nodes and process instance variables, across application restarts.

For {PRODUCT} persistence, you can use one of the following supported persistence stores:

* https://infinispan.org/[*Infinispan*]: (Default) Persists data using configured key-value storage definitions
* https://www.mongodb.com/[*MongoDB*]: Persists data using a document-based format
* https://kafka.apache.org/documentation/streams/[*Kafka Streams*]: (Quarkus only) Persists data in Kafka clusters
* https://www.postgresql.org/[*PostgreSQL*]: Persists data in PostgreSQL

Runtime persistence is intended primarily for storing data that is required to resume workflow execution for a particular process instance. Persistence applies to both public and private processes that are not yet complete. Once a process completes, persistence is no longer applied. This persistence behavior means that only the information that is required to resume execution is persisted.

Node instances that are currently active or in wait states are persisted. When a process instance finishes execution but has not reached the end state (completed or aborted), the node instance data is persisted.

Persistence setup scripts for {PRODUCT} runtimes and {PRODUCT} runtimes supporting services are included in the https://repository.jboss.org/org/kie/kogito/kogito-ddl/{COMMUNITY_VERSION_FINAL}/[kogito-ddl-{COMMUNITY_VERSION_FINAL}-db-scripts.zip] artifact.

=== Persistence workflow in {PRODUCT}

In {PRODUCT}, a process instance is persisted when the process reaches a wait state, where the process does not execute anymore but has not reached the end state (completed or aborted).

For example, when a process reaches a user task or a catching signal event, the process instance pauses and the {PRODUCT} {PROCESS_ENGINE} takes a complete snapshot of the process, including the following data:

* Process instance metadata, such as process instance ID, process definition ID, state, description, and start date
* Process instance variables
* Active node instances, including local variables

Process instance metadata is persisted with a predefined protobuf (https://developers.google.com/protocol-buffers/[protocol buffers]) schema that is aware of the metadata and supports node instances that are in wait states.

Process instance and node instance variables are persisted based on the generated protobuf schema and generated marshallers. Custom data types are also persisted during execution.

For straight-through process instances that do not trigger any activity, persistence is not invoked and no data is stored.

Each process definition has its own cache for storing runtime information. The cache is based on the process definition ID and is named in the Infinispan Server or in the MongoDB Server. If no process cache exists, cache is automatically created in Infinispan or in MongoDB. This setup facilitates maintenance of process instance data and reduces concurrency on the cache instances.

=== Persisted process instance variables and data types

Persisted process variables, local variables, and other process data are stored with the process instance. The stored data is marshalled into bytes format so it can be transferred and persisted into the key-value storage definition for Infinispan or into document-based format for MongoDB. The marshalling and unmarshalling is implemented based on protobuf (https://developers.google.com/protocol-buffers/[protocol buffers]) and requires a schema and marshallers for handling a specified type of data.

{PRODUCT} generates both the protobuf schema (as PROTO files) and marshallers for persisting variables. The {PRODUCT} marshallers are based on the https://github.com/infinispan/protostream[ProtoStream] subproject of Infinispan.

When you build your {PRODUCT} project, {PRODUCT} scans all process definitions and extracts information about the data within the business assets. Based on the unique data types (regardless of how many processes reference a specified type), PROTO files are generated that build a complete schema for the application. These files are stored inside the `/META-INF/resources/persistence/protobuf/` folder of the generated JAR file (e.g. quarkus-apps/quarkus/generated-bytecode.jar) and also in the `target/classes/META-INF/resources/persistence/protobuf/` folder of your project after successful build.

.Example PROTO file generated by {PRODUCT} to persist process data
[source]
----
syntax = "proto2";
package org.kie.kogito.examples.demo.orders;
import "kogito-index.proto";
import "kogito-types.proto";
option kogito_model = "Orders";
option kogito_id = "demo.orders";

/* @Indexed */
message Order {
	option java_package = "org.kie.kogito.examples.demo.orders";
	/* @Field(store = Store.YES) @SortableField */
	optional string orderNumber = 1;
	/* @Field(store = Store.YES) @SortableField */
	optional bool shipped = 2;
	/* @Field(store = Store.YES) @SortableField */
	optional double total = 3;
}
/* @Indexed */
message Orders {
	option java_package = "org.kie.kogito.examples.demo.orders";
	/* @Field(store = Store.YES) @SortableField
 @VariableInfo(tags="") */
	optional string approver = 1;
	/* @Field(store = Store.YES) @SortableField */
	optional string id = 2;
	/* @Field(store = Store.YES) @SortableField
 @VariableInfo(tags="") */
	optional Order order = 3;
	/* @Field(store = Store.YES) @SortableField */
	optional org.kie.kogito.index.model.KogitoMetadata metadata = 4;
}
----

NOTE: Each PROTO file imports `kogito-types.proto` and `kogito-index.proto` files that automatically define the base types managed by {PRODUCT}.

Based on the PROTO files, marshallers are also generated and configured in the application so that whenever a particular data type is used in a process instance, the data is successfully marshalled and unmarshalled.

=== Supported data types for persisted variables

For optimal persistence with process data and variables, use Java objects as data types that represent your process variables. If you use other formats for data types, your data might not be persisted or your {PRODUCT} project might fail to compile.

{PRODUCT} currently supports the following data types for process variables:

.Supported data types
[cols="30%,70%", options="header"]
|===
|Data type |Description

|`java.lang.String`
|Basic text type

|`java.lang.Integer`
|Basic number type

|`java.lang.Long`
|Extended size number type

|`java.lang.Float`
|Basic floating point number type

|`java.lang.Double`
|Extended size floating point number type

|`java.util.Date`
|Basic date type

|Java object
|Custom data type built with multiple simple types

|Java object with a Java object
|Custom data type built with multiple simple types and includes another Java object

|Java object with a list of Java objects
|Custom data type built with multiple simple types and a list of Java objects, and can also contain another Java object
|===

=== Enabling optimistic locking with persistence

{PRODUCT} runtimes need to safely handle concurrent requests to shared instances such as process instances or tasks. {PRODUCT} handles these requests using persistence enabled optimistic locking for concurrency control with the version on record (using the version field in the database or the metadata version in the case of Infinispan). This feature is optional and can be activated only with persistence by adding the following properties to the src/main/resources/application.properties file in your {PRODUCT} project,
[source]
------
kogito.persistence.optimistic.lock=true
------

Below are the databases that support optimistic locking

* Infinispan
* MongoDB
* PostgreSQL (Reactive and JDBC options)

// end::con-persistence[]

// tag::proc-infinispan-persistence-enabling[]
[id="proc-infinispan-persistence-enabling_{context}"]
=== Enabling Infinispan persistence for {PRODUCT} services

[role="_abstract"]
You can enable persistence for your {PRODUCT} services using https://infinispan.org/[Infinispan] to persist data, such as active process nodes and process instance variables, so that the data is preserved across application restarts. For {PRODUCT} persistence with Infinispan, you must have a an Infinispan Server installed and running in order to enable persistence.

.Prerequisites
* https://infinispan.org/[Infinispan Server] 13.0.2 or later is installed and running. For information about Infinispan installation and configuration, see the https://infinispan.org/documentation/[Infinispan documentation].

.Procedure
. Add the following dependencies to the `pom.xml` file of your {PRODUCT} project:
+
--
.On Quarkus
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-persistence-infinispan</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
----

.On Spring Boot
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-persistence-infinispan</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
<dependency>
  <groupId>org.infinispan</groupId>
  <artifactId>infinispan-spring-boot-starter-remote</artifactId>
  <version>__INFINISPAN_SPRING_BOOT_VERSION__</version>
</dependency>
----
--
. Add following property to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure the connection to the Infinispan Server.
+
--
Replace the server host and port information according to your Infinispan Server installation.

.On Quarkus
[source]
----
quarkus.infinispan-client.server-list=localhost:11222
----

.On Spring Boot
[source]
----
infinispan.remote.server-list=127.0.0.1:11222
----
--

NOTE: As an alternative to enabling Infinispan persistence explicitly in {PRODUCT} services, you can use the {PRODUCT} Operator to install the Infinispan infrastructure and enable persistence for the service during deployment on OpenShift. For more information about enabling Infinispan persistence with the {PRODUCT} Operator, see {URL_DEPLOYING_ON_OPENSHIFT}#con-kogito-operator-with-infinispan_kogito-deploying-on-openshift[_{DEPLOYING_ON_OPENSHIFT}_].

For example {PRODUCT} services with Infinispan persistence, see the following example applications in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-infinispan-persistence-quarkus[`process-infinispan-persistence-quarkus`]: Example on Quarkus
* https://github.com/kiegroup/kogito-examples/tree/stable/process-infinispan-persistence-springboot[`process-infinispan-persistence-springboot`]: Example on Spring Boot
// end::proc-infinispan-persistence-enabling[]

// tag::proc-mongodb-persistence-enabling[]
[id="proc-mongodb-persistence-enabling_{context}"]
=== Enabling MongoDB persistence for {PRODUCT} services

[role="_abstract"]
As an alternative to using Infinispan for {PRODUCT} runtime persistence, you can enable persistence for your {PRODUCT} services using https://www.mongodb.com/[MongoDB]. MongoDB is a general purpose, document-based database that enables you to store data in JSON-like documents instead of key-value storage definitions in Infinispan-based persistence storage. For {PRODUCT} persistence with MongoDB, you must have a MongoDB Server installed and running in order to enable persistence.

.Prerequisites
* https://www.mongodb.com/try/download[MongoDB Server] 3.6 or later is installed and running. For information about MongoDB installation and configuration, see the https://docs.mongodb.com/manual/installation/[MongoDB documentation].

.Procedure
. Add the following dependencies to the `pom.xml` file of your {PRODUCT} project:
+
--
.On Quarkus
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-persistence-mongodb</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
----

.On Spring Boot
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-persistence-mongodb</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-data-mongodb</artifactId>
  <version>__MONGODB_SPRING_BOOT_VERSION__</version>
</dependency>
----
--
. Add following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure MongoDB persistence and to connect to the relevant MongoDB Server and database.
+
--
Replace the server host, port, and database information according to your MongoDB Server installation. By default, the database is named `kogito`.

.On Quarkus
[source]
------
kogito.persistence.type=mongodb
quarkus.mongodb.connection-string = mongodb://localhost:27017
quarkus.mongodb.database=kogito_db
------

.On Spring Boot
[source]
------
kogito.persistence.type=mongodb
spring.data.mongodb.uri=mongodb://localhost:27017
spring.data.mongodb.database=kogito_db
------

Optionally, enable MongoDB transactions by adding the following property:
[source]
------
kogito.persistence.transaction.enabled=true
------
--

After you enable MongoDB persistence for your {PRODUCT} services, you can use MongoDB tools such as https://www.mongodb.com/try/download/compass[MongoDB Compass] to query and review the process instance information and model variables from the database, as shown in the following example:

.Example persisted process instance
[source,json]
----
{
  "_id": {
    "$oid": "60d0dd7130a5c559252bbef2"
  },
  "processType": "RuleFlow",
  "processId": "dealreviews",
  "id": "4c0ba244-d99e-46e7-8404-51970bf47a40",
  "parentProcessInstanceId": "b4e666dc-7f61-4e09-9eb3-2e10bf1abc00",
  "description": "Deal Review",
  "state": 1,
  "startDate": "1624300913211",
  "signalCompletion": true,
  "rootProcessInstanceId": "b4e666dc-7f61-4e09-9eb3-2e10bf1abc00",
  "rootProcessId": "deals",
  "sla": {
    "slaCompliance": 0
  },
  "context": {
    "variable": [
      {
        "name": "deal",
        "dataType": "java.lang.String",
        "value": {
          "@type": "type.googleapis.com/google.protobuf.StringValue",
          "value": "my fancy deal"
        }
      },
      {
        "name": "traveller",
        "dataType": "org.acme.deals.Traveller",
        "value": {
          "@type": "org.kie.kogito/org.kie.kogito.app.Traveller",
          "address": {
            "city": "Boston",
            "country": "US",
            "street": "main street",
            "zipCode": "10005"
          },
          "email": "jon.doe@example.com",
          "firstName": "John",
          "lastName": "Doe",
          "nationality": "American"
        }
      }
    ],
    "nodeInstance": [
      {
        "id": "c3549554-9787-42d5-82d0-decff6a17930",
        "nodeId": "2",
        "content": {
          "@type": "type.googleapis.com/org.kie.kogito.serialization.process.protobuf.WorkItemNodeInstanceContent",
          "workItemId": "b87e9296-92c6-4e83-8dcf-6e62882e418b",
          "variable": [
            {
              "name": "ActorId",
              "dataType": "java.lang.String",
              "value": {
                "@type": "type.googleapis.com/google.protobuf.StringValue",
                "value": "john"
              }
            },
            {
              "name": "NodeName",
              "dataType": "java.lang.String",
              "value": {
                "@type": "type.googleapis.com/google.protobuf.StringValue",
                "value": "Review the deal"
              }
            },
            {
              "name": "Skippable",
              "dataType": "java.lang.String",
              "value": {
                "@type": "type.googleapis.com/google.protobuf.StringValue",
                "value": "true"
              }
            },
            {
              "name": "TaskName",
              "dataType": "java.lang.String",
              "value": {
                "@type": "type.googleapis.com/google.protobuf.StringValue",
                "value": "review"
              }
            },
            {
              "name": "deal",
              "dataType": "java.lang.String",
              "value": {
                "@type": "type.googleapis.com/google.protobuf.StringValue",
                "value": "my fancy deal"
              }
            },
            {
              "name": "traveller",
              "dataType": "org.acme.deals.Traveller",
              "value": {
                "@type": "org.kie.kogito/org.kie.kogito.app.Traveller",
                "address": {
                  "city": "Boston",
                  "country": "US",
                  "street": "main street",
                  "zipCode": "10005"
                },
                "email": "jon.doe@example.com",
                "firstName": "John",
                "lastName": "Doe",
                "nationality": "American"
              }
            }
          ],
          "phaseId": "active",
          "phaseStatus": "Ready",
          "name": "Human Task",
          "startDate": "1624300913214",
          "workItemData": {
            "@type": "type.googleapis.com/org.kie.kogito.serialization.process.protobuf.HumanTaskWorkItemData",
            "taskName": "review",
            "potUsers": [
              "john"
            ],
            "taskReferenceName": "Review the deal"
          }
        },
        "level": 1,
        "triggerDate": "1624300913213",
        "sla": {
          "slaCompliance": 0
        }
      }
    ],
    "iterationLevels": [
      {
        "id": "_3597E33A-1C00-41B3-924E-09EA47F79D93",
        "level": 1
      }
    ]
  },
  "completedNodeIds": [
    "_7E3D27D0-6644-4E55-8E41-9F68BD0D1327"
  ],
  "version": {
    "$numberLong": "1"
  }
}
----

For example {PRODUCT} services with MongoDB persistence, see the following example applications in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-mongodb-persistence-quarkus[`process-mongodb-persistence-quarkus`]: Example on Quarkus
* https://github.com/kiegroup/kogito-examples/tree/stable/process-mongodb-persistence-springboot[`process-mongodb-persistence-springboot`]: Example on Spring Boot
// end::proc-mongodb-persistence-enabling[]

// tag::proc-kafka-streams-persistence-enabling[]
[id="proc-kafka-streams-persistence-enabling_{context}"]
=== Enabling Kafka Streams persistence for {PRODUCT} services

[role="_abstract"]
For Quarkus-based {PRODUCT} services, as an alternative to using Infinispan or MongoDB for {PRODUCT} runtime persistence, you can enable persistence using https://kafka.apache.org/documentation/streams/[Kafka Streams]. Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in Kafka clusters. For {PRODUCT} persistence with Kafka Streams, you must have Kafka installed and running in order to enable persistence.

NOTE: Kafka Streams persistence is supported for only Quarkus-based {PRODUCT} services.

.Prerequisites
* https://kafka.apache.org/[Apache Kafka] is installed and includes any required topics. For information about Kafka installation and configuration, see the https://kafka.apache.org/documentation/[Apache Kafka documentation].

.Procedure
. Add the following dependency to the `pom.xml` file of your {PRODUCT} project:
+
--
.Project dependency to enable Kafka Streams persistence (Quarkus only)
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-persistence-kafka</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
----
--
. Add the following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure Kafka persistence and to connect to the relevant Kafka server.
+
--
Replace the server host and port information according to your Kafka installation.

.Application properties to enable Kafka Streams persistence (Quarkus only)
[source]
------
kogito.persistence.type=kafka
kafka.bootstrap.servers=localhost:9092
------
--

For an example {PRODUCT} service with Kafka persistence, see the following example application in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-kafka-persistence-quarkus[`process-kafka-persistence-quarkus`]: Example on Quarkus
// end::proc-kafka-streams-persistence-enabling[]

// tag::proc-postgresql-persistence-enabling[]
[id="proc-postgresql-persistence-enabling_{context}"]
=== PostgreSQL persistence for {PRODUCT} services

[role="_abstract"]
As an alternative to using Infinispan or MongoDB for {PRODUCT} runtime persistence, you can enable persistence for your {PRODUCT} services using https://www.postgresql.org/[PostgreSQL]. PostgreSQL is a powerful, open source object-relational database system that uses and extends SQL language. Also, PostgreSQL is recognized and provided as a service (SAAS) by different cloud providers. For {PRODUCT} persistence with PostgreSQL, you must have PostgreSQL installed and running to enable persistence.

==== Enabling PostgreSQL persistence using reactive driver for {PRODUCT} services

You can enable PostgreSQL persistence for your {PRODUCT} services using reactive driver.

.Prerequisites
* https://www.postgresql.org/[PostgreSQL] is installed. For information about PostgreSQL installation and configuration, see https://www.postgresql.org/docs/current/[PostgreSQL documentation].

.On Quarkus

Add the following dependencies to the `pom.xml` file of your {PRODUCT} project:

--
.Project dependency to enable PostgreSQL persistence
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-persistence-postgresql</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-reactive-pg-client</artifactId>
</dependency>
----
--

Add the following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure PostgreSQL and to connect to the PostgreSQL server.
+
--
Replace the server host, port, credentials, and other information according to your PostgreSQL installation.

.Application properties to enable PostgreSQL persistence
[source]
------
kogito.persistence.type=postgresql
kogito.persistence.auto.ddl=true
kogito.persistence.query.timeout.millis=10000
quarkus.datasource.reactive.url=postgresql://localhost:5432/kogito
quarkus.datasource.username=kogito-user
quarkus.datasource.password=kogito-pass
------

* `url`: This parameter can be customized according to the https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING/[connection string specification]. For a complete list of settings for Quarkus reactive datasource, see https://quarkus.io/guides/reactive-sql-clients#reactive-datasource[reactive datasource in Quarkus].
* `auto.ddl`: This parameter indicates the application to run the scripts that create necessary tables. Ensure that you have the required permissions to run the script, which is convenient for testing and validation purposes. However, you can use your infrastructure to run the scripts in a controlled environment.
* `millis`: This parameter is used to control the waiting time for the application to respond to each executed query. The default value of `millis` is `10000`.

.On Spring Boot

Add the following dependency to the `pom.xml` file of your {PRODUCT} project:
+
--
.Project dependency to enable PostgreSQL persistence
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-quarkus-persistence-postgresql</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
----
--

Add the following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure PostgreSQL and to connect to the PostgreSQL server.

Replace the server host, port, credentials, and other information according to your PostgreSQL installation.

.Application properties to enable PostgreSQL persistence
[source]
------
kogito.persistence.type=postgresql
kogito.persistence.postgresql.connection.uri=postgresql://kogito-user:kogito-pass@localhost:5432/kogito
kogito.persistence.auto.ddl=true
kogito.persistence.query.timeout.millis=10000
------

* `uri`: This parameter can be customized according to the https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING/[connection string specification].
* `auto.ddl`: This parameter indicates the application to run the scripts that create necessary tables. Ensure that you have the required permissions to run the script, which is convenient for testing and validation purposes. However, you can use your infrastructure to run the scripts in a controlled environment.
* `millis`: This parameter is used to control the waiting time for the application to respond to each executed query. The default value of `millis` is `10000`.

Alternatively, you can define the connection properties using the environment variables as shown in the following example:

.Environment variables to enable PostgreSQL persistence
[source]
------
export PGUSER=kogito-user
export PGPASSWORD=kogito-pass
export PGDATABASE=kogito
------
For more information about the full list of supported variables, see https://vertx.io/docs/vertx-pg-client/java/#_environment_variables[vertx-pg-client].
--

==== Enabling PostgreSQL persistence using JDBC driver for {PRODUCT} services

You can enable PostgreSQL persistence for your {PRODUCT} services using JDBC driver.

.Prerequisites
* https://www.postgresql.org/[PostgreSQL] is installed. For information about PostgreSQL installation and configuration, see https://www.postgresql.org/docs/current/[PostgreSQL documentation].

.Procedure
. Add the following dependencies to the `pom.xml` file of your {PRODUCT} project:
+
--
.On Quarkus
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-persistence-jdbc</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-jdbc-postgresql</artifactId>
</dependency>
<dependency>
  <groupId>io.quarkus</groupId>
  <artifactId>quarkus-agroal</artifactId>
</dependency>
----

.On Spring Boot
[source,xml,subs="attributes+,+quotes"]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-persistence-jdbc</artifactId>
  <version>__{PRODUCT_INIT_CAP}_VERSION__</version>
</dependency>
<dependency>
  <groupId>org.postgresql</groupId>
  <artifactId>postgresql</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-jdbc</artifactId>
</dependency>
----
--
. Add the following properties to the `src/main/resources/application.properties` file in your {PRODUCT} project to configure PostgreSQL and to connect to the PostgreSQL server.
+
--
Replace the server host, port, and database information according to your MongoDB Server installation. By default, the database is named `kogito`.

.On Quarkus
[source]
------
kogito.persistence.type=jdbc
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=kogito-user
quarkus.datasource.password=kogito-pass
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/kogito
------

.On Spring Boot
[source]
------
kogito.persistence.type=jdbc
spring.datasource.username=kogito-user
spring.datasource.password=kogito-pass
spring.datasource.url=jdbc:postgresql://localhost:5432/kogito
------
--

For an example {PRODUCT} service with PostgreSQL persistence using reactive and JDBC driver, see the following example applications in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/process-postgresql-persistence-quarkus[`process-postgresql-persistence-quarkus`]: Example on Quarkus
* https://github.com/kiegroup/kogito-examples/tree/stable/process-postgresql-persistence-springboot[`process-postgresql-persistence-springboot`]: Example on SpringBoot

// end::proc-postgresql-persistence-enabling[]

[id="con-data-index-service_{context}"]
== {PRODUCT} Data Index Service

[role="_abstract"]
{PRODUCT} provides a Data Index Service that stores all {PRODUCT} events related to processes, tasks, and domain data. The Data Index Service uses Apache Kafka messaging to consume https://cloudevents.io/[CloudEvents] messages from {PRODUCT} services, and then indexes the returned data for future GraphQL queries and stores the data in the Infinispan persistence store. The Data Index Service is at the core of all {PRODUCT} search, insight, and management capabilities.

.Data Index Service architecture in an example {PRODUCT} service
image::kogito/configuration/data-index-architecture_enterprise.png[Diagram of an example Kogito service using Data Index Service]

The {PRODUCT} Data Index Service has the following key attributes:

* Distinct focus on domain data
* Flexible data structure
* Distributable and cloud-ready format
* Infinispan-based persistence support
* Message-based communication with {PRODUCT} runtime (Apache Kafka, cloud events )
* Powerful querying API using GraphQL

NOTE: The {PRODUCT} Data Index Service is not intended for permanent storage or audit log purposes. The Data Index Service is designed to make business domain data accessible for processes that are currently in progress.

=== Data Index Service workflow in {PRODUCT}

The {PRODUCT} Data Index Service is a Quarkus application, based on https://vertx.io/[Vert.x] with https://smallrye.io/smallrye-reactive-messaging/[Reactive Messaging], that exposes a https://graphql.org[GraphQL] endpoint that client applications use to access business domain-specific data and other information about running process instances.

The Data Index Service uses Apache Kafka messaging to consume https://cloudevents.io/[CloudEvents] messages from {PRODUCT} services, and then indexes the returned data for future GraphQL queries. These events contain information about units of work executed for a process.

Indexed data from the Data Index Service is parsed and pushed into the following Infinispan caches:

* *Domain cache*: Generic cache for each process definition where the process instance variables are pushed as the root content. This cache also includes some process instance metadata, which enables data correlation between domain and process instances. Data is transferred in JSON format to an Infinispan Server.
* *Process instance cache*: Cache for each process instance. This cache contains all process instance information, including all metadata and other detailed information such as executed nodes.
* *User task instance cache*: Cache for each user task instance. This cache contains all task instance information, including all metadata and other detailed information such as data input and output.
* *Jobs cache*: Cache for each scheduled job instance. This cache contains all job instance information, including all detailed information related to a specific job, such as the number of times a job is fired and next time the same job will be fired.

The indexing functionality in the Data Index Service is provided by choosing one of the following persistent providers:

* https://infinispan.org/[Infinispan]
* https://www.mongodb.com/[MongoDB]
* https://www.postgresql.org/[PostgreSQL]

When using Infinispan, https://lucene.apache.org/[Apache Lucene] provides indexing and a protobuf (https://developers.google.com/protocol-buffers/[protocol buffers]) schema and generated marshallers manage the communication between the Data Index Service and Infinispan.

After the data is indexed and stored in a cache, the Data Index Service inspects the process model to update the GraphQL schema and enable a type-checked query system that consumer clients can use to access the data.

.Infinispan indexing
[NOTE]
====

Infinispan also supports data indexing through an embedded Apache Lucene engine. To determine which attributes must be indexed, Inifinispan requires `@Indexed` and `@Field` Hibernate Search parameters that annotate the relevant protobuf file attributes:

.Example indexed model in Infinispan Server configuration
[source]
----
/* @Indexed */
message ProcessInstanceMeta {
    /* @Field(store = Store.YES) */
    optional string id = 1;
}
----

For more information about Infinispan indexing, see https://infinispan.org/docs/stable/titles/developing/developing.html#enable_indexing[Indexing of protobuf encoded entries] in the Infinispan documentation.
====

==== Index the domain-specific data in Data Index Service

You can enable or disable the domain-specific types or custom types indexing in {PRODUCT}. To achieve this, you can change the value of the `kogito.data-index.domain-indexing` property to `true` or `false`, based on the behavior you want to achieve. By default, the domain indexing is enabled on Data Index Service using MongoDB and Infinispan persistence type. When you disable the domain indexing, the Data Index Service stops creating custom types in GraphQL queries, and the GraphQL schema starts exposing queries only for `ProcessInstances`, `UserTaskInstances`, and `Jobs` types.

NOTE: The Data Index PostgreSQL persistence provider does not support indexing for custom types. Therefore, setting the `kogito.data-index.domain-indexing` property to `true` does not have any impact.

[id="proc-data-index-service-using_{context}"]
=== Using the {PRODUCT} Data Index Service to query application data

[role="_abstract"]
{PRODUCT} provides a Data Index Service that stores all {PRODUCT} events related to processes, tasks, and domain data. The Data Index Service uses Kafka messaging to consume https://cloudevents.io/[CloudEvents] messages from {PRODUCT} services, and then indexes the returned data for future GraphQL queries and stores the data in the Infinispan persistence store. The Data Index Service is at the core of all {PRODUCT} search, insight, and management capabilities.

You can use the {PRODUCT} Data Index Service to index, store, and query process data in your {PRODUCT} services.

.Prerequisites
* https://infinispan.org/[Infinispan Server] 13.0.2 or later is installed and running. For information about Infinispan installation and configuration, see the https://infinispan.org/documentation/[Infinispan documentation].
* https://kafka.apache.org/[Apache Kafka] is installed, including required topics, and the Kafka messaging server is running. For information about Kafka installation and configuration, see the https://kafka.apache.org/documentation/[Apache Kafka documentation].
+
--
For a list of configuration options for setting up the Kafka consumer, see https://kafka.apache.org/documentation/#consumerconfigs[Consumer Configs] in the Kafka documentation.

For more information about using Kafka messaging on Quarkus, see https://quarkus.io/guides/kafka[Using Apache Kafka with reactive messaging] in the Quarkus documentation.
--

.Procedure
. Configure your {PRODUCT} project to enable Infinispan persistence and Apache Kafka messaging.
+
--
For instructions on enabling persistence, see xref:proc-infinispan-persistence-enabling_kogito-configuring[].

For instructions on enabling messaging, see xref:proc-messaging-enabling_kogito-configuring[].
--
. Go to the https://repository.jboss.org/org/kie/kogito/data-index-service-infinispan/[`data-index-service-infinispan`] artifacts page, select the latest release of the Data Index Service, and download the `data-index-service-infinispan-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `data-index-service-infinispan-__VERSION__-runner.jar` file and enter the following command to run the Data Index Service with the required Infinispan credentials:
+
--
.Running the Data Index Service
[source,subs="+quotes"]
----
$ java  \
  -Dquarkus.infinispan-client.auth-username=__INFINISPAN_USER_NAME__ \
  -Dquarkus.infinispan-client.auth-password=__INFINISPAN_PASSWORD__ \
  -jar data-index-service-infinispan-__VERSION__-runner.jar
----

For more information about Infinispan authentication on Quarkus, see https://quarkus.io/guides/infinispan-client[Infinispan client] in the Quarkus documentation.

To change the logging level of the Data Index Service, such as for debugging, you can specify the following start-up properties as needed when you run the Data Index Service:

.Modifying Data Index Service logging level for debugging
[source,subs="+quotes"]
----
$ java  \
  -Dquarkus.log.console.level=DEBUG -Dquarkus.log.category.\"org.kie.kogito\".min-level=DEBUG  \
  -Dquarkus.log.category.\"org.kie.kogito\".level=DEBUG  \
  -jar data-index-service-infinispan-__VERSION__-runner.jar
----
--
. In a separate command terminal window, navigate to your {PRODUCT} project and run the project using your preferred run mode, such as development mode:
+
--
.On Quarkus
[source]
----
mvn clean compile quarkus:dev
----

.On Sprint Boot
[source]
----
mvn clean compile spring-boot:run
----

With the Data Index Service and your {PRODUCT} project both configured and running, the Data Index Service starts consuming messages from the defined Kafka topics, such as `kogito-processinstances-events`.
--
. In a web browser, navigate to the `http://__HOST__:__PORT__` location configured for your running {PRODUCT} service, such as `\http://localhost:8080/`, to explore the exposed data model.
+
--
To query the available data using the https://github.com/graphql/graphiql[GraphiQL] interface, navigate to `http://__HOST__:__PORT__/graphql`, such as `\http://localhost:8080/graphql` in this example, and begin executing supported queries to interact with your application data.

.Example query for process instance data
[source]
----
{ ProcessInstances {
  id,
  processId,
  processName,
  state,
  nodes {
    name,
    type,
    enter,
    exit
  }
} }
----

.Example response
image::kogito/openshift/kogito-data-index-graphiql-process-instances.png[Image of GraphQL query and response for process instances]

For available query types, click *Docs* in the upper-right corner of the GraphiQL interface.

For more information about supported queries with the Data Index Service, see xref:ref-data-index-service-queries_kogito-configuring[].

NOTE: As an alternative to enabling the Data Index Service explicitly for {PRODUCT} services, you can use the {PRODUCT} Operator to install the Data Index Service custom resource for the service deployment on OpenShift. For more information about installing the Data Index Service with the {PRODUCT} Operator, see {URL_DEPLOYING_ON_OPENSHIFT}#con-kogito-operator-with-data-index-service_kogito-deploying-on-openshift[_{DEPLOYING_ON_OPENSHIFT}_].
--

[id="ref-data-index-service-queries_{context}"]
=== Supported GraphQL queries with the Data Index Service

[role="_abstract"]
After you configure and run your {PRODUCT} service and the {PRODUCT} Data Index Service, you can query the available data using the https://github.com/graphql/graphiql[GraphiQL] interface displayed at `http://__HOST__:__PORT__/graphql`, such as `\http://localhost:8080/graphql`.

The {PRODUCT} Data Index Service supports GraphQL queries for domain-specific (domain cache) types and for process instances, task instances, and jobs.

==== GraphQL queries for domain-specific types (domain cache)

Use the following GraphQL queries to retrieve data about process definitions. These example queries assume that a `Travels` Business Process Model and Notation (BPMN) process model is running or has been executed.

Retrieve data from process definitions::
+
--
You can retrieve data about a specified process definition from your {PRODUCT} service.

.Example query
[source]
----
{
  Travels {
    visaApplication {
      duration
    }
    flight {
      flightNumber
      gate
    }
    hotel {
      name
      address {
        city
        country
      }
    }
    traveller {
      firstName
      lastName
      nationality
      email
    }
  }
}
----
--

Correlate data using the `metadata` parameter::
+
--
You can use the `metadata` parameter to correlate data from domain-specific types (domain cache) with data from process instances and task instances. This parameter is added to all root models that are deployed in the Data Index Service and enables you to retrieve and filter query data.

.Example query
[source]
----
{
  Travels {
    flight {
      flightNumber
      arrival
      departure
    }
    metadata {
      lastUpdate
      userTasks {
        name
      }
      processInstances {
        processId
      }
    }
  }
}
----
--

Filter query results using the `where` and `metadata` parameters::
+
--
You can use the `where` parameter with multiple combinations to filter query results based on process definition attributes. The attributes available for search depend on the BPMN process model that is deployed, such as a `Travels` process model in this example.

.Example query
[source]
----
{
  Travels(where: {traveller: {firstName: {like: "Cri*"}}}) {
    flight {
      flightNumber
      arrival
      departure
    }
    traveller {
      email
    }
  }
}
----

NOTE: The `like` operator is case sensitive.

You can also use the `metadata` parameter to filter correlated query results from related process instances or tasks.

.Example query
[source]
----
{
  Travels(where: {metadata: {processInstances: {id: {equal: "1aee8ab6-d943-4dfb-b6be-8ea8727fcdc5"}}}}) {
    flight {
      flightNumber
      arrival
      departure
    }
  }
}
----

.Example query
[source]
----
{
  Travels(where: {metadata: {userTasks: {id: {equal: "de52e538-581f-42db-be65-09e8739471a6"}}}}) {
    flight {
      flightNumber
      arrival
      departure
    }
  }
}
----
--

Sort query results using the `orderBy` parameter::
+
--
You can use the `orderBy` parameter to sort query results based on process definition attributes. You can also specify the direction of sorting in ascending `ASC` order or descending `DESC` order. Multiple attributes are applied to the database query in the order they are specified in the query filter.

.Example query
[source]
----
{
  Travels(orderBy: {trip: {begin: ASC}}) {
    flight {
      flightNumber
      arrival
      departure
    }
  }
}
----
--

Limit and offset query results using the `pagination` parameter::
+
--
You can use the `pagination` parameter to specify a `limit` and `offset` for query results.

.Example query
[source]
----
{
  Travels(where: {traveller: {firstName: {like: "Cri*"}}}, pagination: {offset: 0, limit: 10}) {
    flight {
      flightNumber
      arrival
      departure
    }
    traveller {
      email
    }
  }
}
----
--

[id="con-data-index-service-queries_{context}"]
==== GraphQL queries for process instances, user task instances, and jobs

Use the following GraphQL queries to retrieve data about process instances, user task instances, and jobs.

Retrieve data from process instances::
+
--
You can retrieve data about a specified process instance from your process definition.

.Example query
[source]
----
{
  ProcessInstances {
    id
    processId
    state
    parentProcessInstanceId
    rootProcessId
    rootProcessInstanceId
    variables
    nodes {
      id
      name
      type
    }
  }
}
----
--

Retrieve data from user task instances::
+
--
You can retrieve data from a specified user task instance.

.Example query
[source]
----
{
  UserTaskInstances {
    id
    name
    actualOwner
    description
    priority
    processId
    processInstanceId
  }
}
----
--

Retrieve data from jobs::
+
--
You can retrieve data from a specified job instance.

.Example query
[source]
----
{
  Job {
    id
    status
    priority
    processId
    processInstanceId
    executionCounter
  }
}
----
--

Filter query results using the `where` parameter::
+
--
You can use the `where` parameter with multiple combinations to filter query results based on process or task attributes.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}) {
    id
    processId
    processName
    start
    state
    variables
  }
}
----

.Example query
[source]
----
{
  ProcessInstances(where: {id: {equal: "d43a56b6-fb11-4066-b689-d70386b9a375"}}) {
    id
    processId
    processName
    start
    state
    variables
  }
}
----

.Example query
[source]
----
{
  UserTaskInstances(where: {state: {equal: "Ready"}}) {
    id
    name
    actualOwner
    description
    priority
    processId
    processInstanceId
  }
}
----

By default, every filtered attribute is executed as an `AND` operation in queries. You can modify this behavior by combining filters with an `AND` or `OR` operator.

.Example query
[source]
----
{
  ProcessInstances(where: {or: {state: {equal: ACTIVE}, rootProcessId: {isNull: false}}}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----

.Example query
[source]
----
{
  ProcessInstances(where: {and: {processId: {equal: "travels"}, or: {state: {equal: ACTIVE}, rootProcessId: {isNull: false}}}}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----

Depending on the attribute type, the following operators are also available:

* String array argument:
** `contains` : String
** `containsAll`: Array of strings
** `containsAny`: Array of strings
** `isNull`: Boolean (`true` or `false`)

* String argument:
** `in`: Array of strings
** `like`: String
** `isNull`: Boolean (`true` or `false`)
** `equal`: String

* ID argument:
** `in`: Array of strings
** `equal`: String
** `isNull`: Boolean (`true` or `false`)

* Boolean argument:
** `isNull`: Boolean (`true` or `false`)
** `equal`: Boolean (`true` or `false`)

* Numeric argument:
** `in`: Array of integers
** `isNull`: Boolean
** `equal`: Integer
** `greaterThan`: Integer
** `greaterThanEqual`: Integer
** `lessThan`: Integer
** `lessThanEqual`: Integer
** `between`: Numeric range
** `from`: Integer
** `to`: Integer

* Date argument:
** `isNull`: Boolean (`true` or `false`)
** `equal`: Date time
** `greaterThan`: Date time
** `greaterThanEqual`: Date time
** `lessThan`: Date time
** `lessThanEqual`: Date time
** `between`: Date range
** `from`: Date time
** `to`: Date time
--

Sort query results using the `orderBy` parameter::
+
--
You can use the `orderBy` parameter to sort query results based on process or task attributes. You can also specify the direction of sorting in ascending `ASC` order or descending `DESC` order. Multiple attributes are applied to the database query in the order they are specified in the query filter.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}, orderBy: {start: ASC}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----

.Example query
[source]
----
{
  UserTaskInstances(where: {state: {equal: "Ready"}}, orderBy: {name: ASC, actualOwner: DESC}) {
    id
    name
    actualOwner
    description
    priority
    processId
    processInstanceId
  }
}
----
--

Limit and offset query results using the `pagination` parameter::
+
--
You can use the `pagination` parameter to specify a `limit` and `offset` for query results.

.Example query
[source]
----
{
  ProcessInstances(where: {state: {equal: ACTIVE}}, orderBy: {start: ASC}, pagination: {limit: 10, offset: 0}) {
    id
    processId
    processName
    start
    end
    state
  }
}
----
--

[id="proc-data-index-service-runtime-connection_{context}"]
==== Data Index service Gateway API

Data Index incorporates a set of queries or mutations that allow firing operations on runtime services endpoints using GraphQL notation.

.Data Index Gateway API external connections
image::kogito/configuration/data-index-gateway-api.png[Image of data-index external points connections]

NOTE: The xref:con-bpmn-process-management-addon_kogito-developing-process-services[] needs to be enabled for {PRODUCT} runtime service to perform the requested operation.

The Data Index Gateway API enables you to peform the following operations:

Abort a process instance::
+
--
Retrieves a process instance with the ID passed as a parameter and launches the abort operation on related {PRODUCT} service.

.Example mutation for abort operation
[source]
----
mutation {
    ProcessInstanceAbort (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--

Retry a process instance::
+
--
Retrieves a process instance with the id passed as a parameter and launches the retry operation on related {PRODUCT} service.

.Example mutation for retry operation
[source]
----
mutation {
    ProcessInstanceRetry (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--

Skip a process instance::
+
--
Retrieves a process instance with the ID passed as a parameter and launches the skip operation on related {PRODUCT} service.

.Example mutation for skip operation
[source]
----
mutation {
    ProcessInstanceSkip (id:"66e05e9c-eaab-47af-a83e-156498b7096d")
}
----
--

Retrieve a process instance diagram::
+
--
Retrieves a process instance diagram that shows the execution path. When the `diagram` field of a process instance is queried, a call to a specific {PRODUCT} service is generated to retrieve the requested process instance diagram.

.Example query to retrieve a process instance diagram
[source]
----
{ProcessInstances(where: { id: {equal: "1017afb1-5749-440e-8b9b-6b876bb5894d"}}){
  diagram
}}
----
--

NOTE: The  xref:con-bpmn-process-svg-addon_kogito-developing-process-services[] also needs to be enabled for {PRODUCT} runtime service to retrieve the process instance diagram.

Retrieve process instance nodes::
+
--
Retrieves the nodes of a process instance that are coming from the process definition. When the `nodeDefinitions` field of a process instance is queried, a call to a specific {PRODUCT} service is generated to retrieve the requested list of available nodes.

.Example query to retrieve process instance nodes
[source]
----
{ProcessInstances(where: { id: {equal: "1017afb1-5749-440e-8b9b-6b876bb5894d"}}){
  diagram
}}
----
--

Update process instance variables::
+
--
Updates the variables of a process instance using the `id` passed as a parameter. Retrieves a process instance using the `id` passed as a parameter and launches the update operation on related {PRODUCT} service with the new values passed in `variables` parameter.

.Example mutation to update process instance variables
[source]
----
mutation {
    ProcessInstanceUpdateVariables
        (id:"23147fcc-da82-43a2-a577-7a36b26094bd",
         variables:"{\"it_approval\":true,\"candidate\":{\"name\":\"Joe\",\"email\":\"jdoe@ts.com\",\"salary\":30000,\"skills\":\"java\"},\"hr_approval\":true}")
}
----
--

Trigger a node instance::
+
--
Triggers a node instance using the node definition `nodeId`. The `nodeId` is included in the `nodeInstances` of a process instance using the `id` passed as parameter.

.Example mutation to trigger a node instance
[source]
----
mutation{
  NodeInstanceTrigger(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeId:"_B8C4F63C-81AD-4291-9C1B-84967277EEF6")
}
----
--

Retrigger a node instance::
+
--
Retriggers a node instance using the `id`, which is similar to `nodeInstanceId` related to a process instance. The `id` of the process instance is passed as a parameter.

.Example mutation to retrigger a node instance
[source]
----
mutation{
  NodeInstanceRetrigger(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeInstanceId:"01756ba2-ac16-4cf1-9d74-154ae8f2df21")
}
----
--

Cancel a node instance::
+
--
Cancels a node instance with the `id`, which is similar to `nodeInstanceId` related to a process instance. The `id` of the process instance is passed as a parameter.

.Example mutation to cancel a node instance
[source]
----
mutation{
  NodeInstanceCancel(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    nodeInstanceId:"01756ba2-ac16-4cf1-9d74-154ae8f2df21")
}
----
--

Reschedule a job::
+
--
Reschedules a job using the `id`. The job `id` and other information are passed in the `data` parameter.

.Example mutation to reschedule a job
[source]
----
mutation{
  JobReschedule(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7",
    data:"{\"expirationTime\": \"2021-08-27T04:35:54.631Z\",\"retries\": 2}")
}
----
--

Cancel a job::
+
--
Cancels a job using the `id` passed as a parameter.

.Example mutation to cancel a job
[source]
----
mutation{
  JobCancel(
    id: "9674e3ed-8c13-4c3f-870e-7465d9ca7ca7")
}
----
--

Retrieve the user task schema::
+
--
Retrieves the schema of the user task instance using the `taskId`. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes. When the `schema` field of a user task instance is queried, a call to a specific {PRODUCT} service is
generated to retrieve the requested task schema.

.Example query for task instance schema
[source]
----
{UserTaskInstances (where: {id: {equal: "b356d136-e601-43ee-9c2d-ee47198b9303"}}) {
  schema(user:"jdoe" groups:["managers"])
}}
----
--

Update a user task instance::
+
--
Updates the data of a user task instance using the `taskId` and the information passed in the parameters. The parameters passed is only affected with this operation. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to update a task instance
[source]
----
mutation{
    UserTaskInstanceUpdate (
        taskId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        user: "jdoe",
        groups: ["managers", "users", "IT"],
        description: "New task description"
        priority: "High"
        actualOwner: "jdoe"
        adminGroups: ["managers","IT"]
        adminUsers: ["admin"]
        excludedUsers: ["katy"]
        potentialGroups: ["users"]
        potentialUsers: ["alice"]
        inputParams: "{\"it_approval\":true,\"candidate\":{\"name\":\"Joe\",\"email\":\"jdoe@ts.com\",\"salary\":30000,\"skills\":\"java\"},\"hr_approval\":true}"
    )
}
----
--

Add a comment to a user task instance::
+
--
Adds a new comment to the user task instance using the `taskId` and the information passed in the `comment` parameter. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to add a new comment to a user task instance
[source]
----
mutation{
    UserTaskInstanceCommentCreate(
        taskId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        user: "jdoe",
        groups: ["managers", "users", "IT"],
        comment: "New Comment to add"
    )
}
----
--

Update a comment in a user task instance::
+
--
Updates a comment using the `commentId` and the information passed in the `comment` parameter in a user task instance. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to update a user task instance comment
[source]
----
mutation{
    UserTaskInstanceCommentUpdate(
        commentId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        comment: "Comment new content",
        user: "jdoe",
        groups: ["managers", "users", "IT"]
    )
}
----
--

Delete a comment in a user task instance::
+
--
Deletes a comment using the `commentId` in a user task instance. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to delete a user task instance comment
[source]
----
mutation{
    UserTaskInstanceCommentDelete(
        commentId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        user: "jdoe",
        groups: ["managers", "users", "IT"]
    )
}
----
--

Add an attachment to a user task instance::
+
--
Adds a new attachment to the user task instance using the `taskId` and the information passed in the `name` and `uri` parameter. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to add a new attachment to a user task instance
[source]
----
mutation{
    UserTaskInstanceAttachmentCreate(
        taskId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        user: "jdoe",
        groups: ["managers", "users", "IT"],
        name:"Attachment name"
        uri:"https://drive.google.com/file/d/1Z_Lipg2jzY9TNewTaskAttachmentUri"
    )
}
----
--

Update an attachment in a user task instance::
+
--
Updates an attachment using the `attachmentId` and the information passed in the `name` and `uri` parameter in a user task instance. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to update a user task instance attachment
[source]
----
mutation{
    UserTaskInstanceAttachmentUpdate(
        attachmentId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        name:"Attachment new name"
        uri:"https://drive.google.com/file/d/1Z_Lipg2jzY9TAttachmentNewUri",
        user: "jdoe",
        groups: ["managers", "users", "IT"]
    )
}
----
--

Delete an attachment in a user task instance::
+
--
Deletes an attachment using the `attachmentId` in a user task instance. The call allows you to pass the `user` and `groups` parameters even if the parameters are mandatory. If `user` and `groups` parameters are used to run the call to runtimes.

.Example mutation to delete a user task instance attachment
[source]
----
mutation{
    UserTaskInstanceAttachmentDelete(
        attachmentId:"b356d136-e601-43ee-9c2d-ee47198b9303",
        user: "jdoe",
        groups: ["managers", "users", "IT"]
    )
}
----
--

[id="proc-data-index-service-security_{context}"]
=== Enabling {PRODUCT} Data Index Service security with OpenID Connect

[role="_abstract"]
For Quarkus-based {PRODUCT} services, you can use the https://quarkus.io/guides/security-openid-connect[Quarkus OpenID Connect adapter] with the {PRODUCT} Data Index Service to enable security using token authorization. These tokens are issued by OpenID Connect and OAuth 2.0 compliant authorization servers such as https://www.keycloak.org/about.html[Keycloak].

IMPORTANT: This procedure applies only when you are using a locally cloned copy of the https://github.com/kiegroup/kogito-apps/tree/master/data-index[{PRODUCT} Data Index Service] repository in GitHub.

.Prerequisites
* You have cloned the https://github.com/kiegroup/kogito-apps/tree/master/data-index[{PRODUCT} Data Index Service] repository from GitHub.

.Procedure
. In a command terminal, navigate to the local clone of the {PRODUCT} Data Index Service repository and enter the following command to run the application with the required security properties:
+
--
.Run the Data Index Service with security properties
[source]
----
mvn clean compile quarkus:dev  \
  -Dquarkus.profile=keycloak  \
  -Dkogito.protobuf.folder=/home/git/kogito-apps/tree/master/data-index/data-index-service/src/test/resources  \
  -Dkogito.protobuf.watch=true
----

The {PRODUCT} Data Index Service contains a Quarkus profile to encapsulate the security configuration, so if the service requires security, you can specify the `quarkus.profile=keycloak` property at build time to enable the needed security. If the `keycloak` Quarkus profile is not added, the OpenID Connect extension is disabled.
--
. Navigate to the `src/main/resources/application.properties` file of the Data Index Service project and add the following properties:
+
--
.Required security properties in `applications.properties` file
[source]
----
# OpenID Connect configurations
%keycloak.quarkus.oidc.enabled=true
%keycloak.quarkus.oidc.tenant-enabled=true
%keycloak.quarkus.oidc.auth-server-url=http://localhost:8280/auth/realms/kogito
%keycloak.quarkus.oidc.client-id=kogito-service
%keycloak.quarkus.oidc.credentials.secret=secret
%keycloak.quarkus.oidc.application-type=service

%keycloak.quarkus.oidc.web-app-tenant.auth-server-url=http://localhost:8280/auth/realms/kogito
%keycloak.quarkus.oidc.web-app-tenant.client-id=kogito-service
%keycloak.quarkus.oidc.web-app-tenant.credentials.secret=secret
%keycloak.quarkus.oidc.web-app-tenant.application-type=web-app

kogito.data-index.vertx-graphql.ui.path=/graphiql
kogito.data-index.vertx-graphql.ui.tenant=web-app-tenant

# HTTP security configurations
%keycloak.quarkus.http.auth.permission.authenticated.paths=/*
%keycloak.quarkus.http.auth.permission.authenticated.policy=authenticated
----

NOTE: The `quarkus.oidc.enabled` property enables or disables security at build time, while the `quarkus.oidc.tenant-enabled` property enables or disables security at runtime.

Replace any property definitions with those of your specific environment, especially the following properties:

* `quarkus.oidc.auth-server-url`: The base URL of the OpenID Connect (OIDC) server, such as `https://localhost:8280/auth`. All other OIDC server page and service URLs are derived from this URL. If you work with Keycloak OIDC server, ensure that the base URL is in the following format: `https://__HOST__:__PORT__/auth/realms/__KEYCLOAK_REALM__`.
* `quarkus.oidc.client-id`: The client ID of the application. Each application has a client ID that is used to identify the application.
* `quarkus.oidc.credentials.secret`: The client secret for the application.

NOTE: If you are enabling security at runtime using the `quarkus.oidc.tenant-enabled` property, the `quarkus.http.auth.permission` path and policy must specify how authentication is applied. By default, if security is enabled, the user must be authenticated to access any path.

The default configuration provides a multi-tenant configuration so that the {PRODUCT} Data Index Service can use two endpoints with different security https://quarkus.io/guides/security-openid-connect#quarkus-oidc_quarkus.oidc.application-type[`quarkus.oidc.application-type`] configurations:

* The `/graphql` endpoint is configured as a `service` application that enables Bearer token authentication.
* The `/graphiql` interface endpoint, shown in the previous configuration file example, is configured as a `web-app` application that redirects unauthenticated users to the Keycloak login page. You configure this endpoint using the `kogito.data-index.vertx-graphql.ui.path` property.
--
. In the same `application.properties` file, also configure the resources to be exposed and the required permissions for accessing the resources.
+
--
For example, you can enable only users with the role `confidential` to access a single `/graphql` endpoint:

.Example GraphQL security role configuration
[source]
----
%keycloak.quarkus.http.auth.policy.role-policy1.roles-allowed=confidential
%keycloak.quarkus.http.auth.permission.roles1.paths=/graphql
%keycloak.quarkus.http.auth.permission.roles1.policy=role-policy1
----
--
. Stop and restart the {PRODUCT} Data Index Service to ensure that the security changes are applied.

[role="_additional-resources"]
.Additional resources
* https://quarkus.io/guides/security[Security Architecture and Guides]
* https://quarkus.io/guides/security-openid-connect#configuring-using-the-application-properties-file[Configuring using the application.properties file]
* https://quarkus.io/guides/security-openid-connect-multitenancy[Using OpenID Connect multi-tenancy]

[id="proc-data-index-service-mongodb_{context}"]
=== Enabling MongoDB persistence for the {PRODUCT} Data Index Service

[role="_abstract"]
Instead of using the default Infinispan-based persistence in the {PRODUCT} Data Index Service, you can configure the Data Index Service to use https://www.mongodb.com/[MongoDB] persistence storage if needed.

.Prerequisites
* https://www.mongodb.com/try/download[MongoDB Server] 3.6 or later is installed and running. For information about MongoDB installation and configuration, see the https://docs.mongodb.com/manual/installation/[MongoDB documentation].

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/data-index-service-mongodb/[`data-index-service-mongodb`] artifacts page, select the latest release of the Data Index Service, and download the `data-index-service-mongodb-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `data-index-service-mongodb-__VERSION__-runner.jar` file and enter the following command to run the Data Index Service with the required MongoDB connection information:
+
--
.Running the Data Index Service with MongoDB connection information
[source,subs="+quotes"]
----
$ java  \
  -Dquarkus.mongodb.connection-string=__MONGODB_SERVER_CONNECTION_STRING__ \
  -Dquarkus.mongodb.database=__DATABASE_NAME__ \
  -jar data-index-service-mongodb-__VERSION__-runner.jar
----

For more information about MongoDB configuration on Quarkus, see https://quarkus.io/guides/mongodb#quarkus-mongodb_configuration[Using the MongoDB client] in the Quarkus documentation.
--
. After the Data Index Service is running, compile and run your {PRODUCT} project as usual and navigate to the `http://__HOST__:__PORT__` location configured for your running {PRODUCT} service, such as `\http://localhost:8080/`, to explore the exposed data model.

NOTE: Indexes are created on every attribute that is annotated with `@Indexed` and `@Field` in the protobuf files. However, a single MongoDB collection can have no more than 64 indexes, including the default `_id` Index.

[id="proc-data-index-service-postgresql_{context}"]
=== Enabling PostgreSQL persistence for the {PRODUCT} Data Index Service

[role="_abstract"]
Instead of using the default Infinispan-based persistence in the {PRODUCT} Data Index Service, you can configure the Data Index Service to use https://www.postgresql.org/[PostgreSQL] persistence storage if needed.

.Prerequisites
* https://www.postgresql.org/download/[PostgreSQL Server] 13 or later is installed and running. For information about PostgreSQL installation and configuration, see the https://www.postgresql.org/docs/[PostgreSQL documentation].

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/data-index-service-postgresql/[`data-index-service-postgresql`] artifacts page, select the latest release of the Data Index Service, and download the `data-index-service-postgresql-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `data-index-service-postgresql-__VERSION__-runner.jar` file and enter the following command to run the Data Index Service with the required PostgreSQL connection information:
+
--
.Running the Data Index Service with PostgreSQL connection information
[source,subs="+quotes"]
----
$ java  \
  -Dquarkus.datasource.jdbc.url=__POSTGRESQL_SERVER_JDBC_URL__ \
  -Dquarkus.datasource.username=__POSTGRESQL_USERNAME__ \
  -Dquarkus.datasource.password=__POSTGRESQL_PASSWORD__ \
  -jar data-index-service-postgresql-__VERSION__-runner.jar
----

For more information about PostgreSQL configuration on Quarkus, see https://quarkus.io/guides/datasource#jdbc-configuration[JDBC Configuration Reference] in the Quarkus documentation.
--
. After the Data Index Service is running, compile and run your {PRODUCT} project as usual and navigate to the `http://__HOST__:__PORT__` location configured for your running {PRODUCT} service, such as `\http://localhost:8080/`, to explore the exposed data model.

NOTE: Data Index Service with PostgreSQL does not support indexing domain-specific data. The GraphQL schema only exposes queries for `ProcessInstances`, `UserTaskInstances`, and `Jobs`.

[id="con-data-index-dev-service_{context}"]
=== Quarkus Dev Service for {PRODUCT} Data Index Service

When you use the {PRODUCT} Process Quarkus extension, a temporary Data Index Service is automatically provisioned while the Quarkus application is running in development mode. When you use one of the following Quarkus extensions, the Dev Service is set up for immediate use:

.Quarkus extension in {PRODUCT}
[source,xml]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-quarkus</artifactId>
</dependency>
----

.Quarkus extension in {PRODUCT}
[source,xml]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-quarkus-processes</artifactId>
</dependency>
----

When you start your Quarkus project in development mode, an in-memory instance of the Data Index Service is automatically started in the background. This feature is enabled by https://quarkus.io/guides/dev-services[Quarkus Dev Services], and leverages https://www.testcontainers.org/[Testcontainers] to start an image of the Data Index Service.

The {PRODUCT} Process Quarkus extension sets up your Quarkus application to automatically replicate any {PRODUCT} messaging events related to process instances and user tasks into the provisioned Data Index instance.

Once the service is up and running, you can query the GraphQL interface directly using `http://localhost:8180/graphql` or using the Quarkus Dev UI console `http://localhost:8080/q/dev`.

The Data Index GraphQL endpoint can query for `ProcessInstances` and `UserTaskInstances`. For more information about operations and attributes to query, see {URL_CONFIGURING_KOGITO}#con-data-index-service-queries_kogito-configuring[_{PRODUCT} Data Index GraphQL queries_].

NOTE: Quarkus Dev Service for the {PRODUCT} Data Index Service does not support indexing for custom types. The GraphQL schema only exposes queries for `ProcessInstances`, `UserTaskInstances`.

You can share the same Data Index instance across multiple {PRODUCT} services during development. Sharing Data Index instances is enabled by default, therefore, only one Data Index instance is started. This behaviour can be adjusted to start multiple instances using the `quarkus.kogito.devservices.shared` property.

The Quarkus Dev Service also allows further configuration options including:

* To disable Data Index Dev Service, use the `quarkus.kogito.devservices.enabled=false` property.
* To change the port where the Data Index Service runs, use the `quarkus.kogito.devservices.port=8180` property.
* To adjust the provisioned image, use `quarkus.kogito.devservices.imageName=quay.io/kiegroup/kogito-data-index-ephemeral` property.
* To disable sharing the Data Index instance across multiple Quarkus applications, use `quarkus.kogito.devservices.shared=false` property.

For more information about Quarkus Dev Services, see https://quarkus.io/guides/dev-services[Dev Services guide].

[role="_additional-resources"]
.Additional resources
* xref:proc-data-index-service-using_kogito-configuring[]
* xref:con-persistence_kogito-configuring[]

[id="con-jobs-service_{context}"]
== {PRODUCT} Jobs Service

[role="_abstract"]
{PRODUCT} provides a Jobs Service for scheduling Business Process Model and Notation (BPMN) process events that are configured to be executed at a specified time. These time-based events in a process model are known as _jobs_.

By default, {PRODUCT} services use an in-memory timer service to handle jobs defined in your BPMN process models. This default timer service does not cover long time intervals and is only suitable for short delays defined in the process. For advanced use cases where time intervals can be days or weeks or when additional event handling options are required, you can configure your {PRODUCT} project to use the {PRODUCT} Jobs Service as an external timer service.

The Jobs Service does not execute a job, but triggers a callback that might be an HTTP request on an endpoint specified for the job request or any other configured callback. The Jobs Service receives requests for job scheduling and then sends a request at the time specified on the job request.

.Jobs Service architecture
image::kogito/configuration/jobs-service-architecture_enterprise.png[Diagram of the Jobs Service architecture]

NOTE: The {PRODUCT} Jobs Service currently supports only HTTP `POST` requests that are sent to an endpoint specified on the job-scheduling request. The HTTP callback information must be specified in the job-scheduling request.

The main goal of the Jobs Service is to work with only active jobs. The Jobs Service tracks only the jobs that are scheduled and that need to be executed. When a job reaches a final state, the job is removed from the Jobs Service. All job information and transition states are sent to the {PRODUCT} Data Index Service where they can be indexed and made available for GraphQL queries.

The Jobs Service implementation is based on non-blocking APIs and https://smallrye.io/smallrye-reactive-messaging/[Reactive Messaging] on top of Quarkus, which provides effective throughput and resource utilization. The scheduling engine is implemented on top of https://vertx.io/[Vert.x] and the external requests are built using a non-blocking HTTP client based on Vert.x.

=== Supported job states in the {PRODUCT} Jobs Service

The {PRODUCT} Jobs Service uses an internal state control mechanism to manage the job scheduling lifecycle using the following supported job states:

* *Scheduled*
* *Executed*
* *Canceled*
* *Retry*
* *Error*

The Jobs Service workflow through these states is illustrated in the following diagram:

.Jobs Service state control workflow
image::kogito/configuration/jobs-service-state-control_enterprise.png[Diagram of Jobs Service states]

=== Supported job types in the {PRODUCT} Jobs Service

The {PRODUCT} Jobs Service supports the following job types:

* *Time scheduled*: A job that is scheduled at a specified time and executed only once when that point in time is reached. The time must be specified on the job scheduling request and must be in the future.
* *Periodic scheduled*: A job that is scheduled at a specified time and executed after a specified interval, and then executed repeatedly over a specified period of time until a limit of executions is reached. The execution limit and interval must be specified in the job-scheduling request.

=== Supported configuration properties in the {PRODUCT} Jobs Service

The {PRODUCT} Jobs Service supports the following configuration properties. You can set these properties either using the `-D` prefix during Jobs Service start-up or in the `src/main/resources/application.properties` file of the Jobs Service project.

.Supported configuration properties in Jobs Service
[cols="30%,40%,15%,15%"]
|===
|Name |Description |Value |Default

|`kogito.jobs-service.backoffRetryMillis`
|Defines the retry back-off time in milliseconds between job execution attempts, in case the execution fails
|Long type
|`1000`

|`kogito.jobs-service.maxIntervalLimitToRetryMillis`
|Defines the maximum interval in milliseconds when retrying to execute jobs, in case the execution fails
|Long type
|`60000`

|`mp.messaging.outgoing.kogito-job-service-job-status-events.bootstrap.servers`
|Identifies the Kafka bootstrap server address with the port used to publish events
|String
|`localhost:9092`

|`mp.messaging.outgoing.kogito-job-service-job-status-events.topic`
|Defines the name of the Kafka topic where the events are published
|String
|`kogito-jobs-events`
|===

////
// @comment: These endpoints are used internally by Jobs Service and may confuse users who think they need to use them in some way. Excluding for now. (Stetson, 1 Apr 2020)
### Usage

The basic actions on Job Service are made through REST as follow:

#### Schedule a Job

POST

{url-job-service}{jobs-path}

```
{
    "id": "1",
    "priority": "1",
    "expirationTime": "2019-11-29T18:16:00Z",
    "callbackEndpoint": "http://localhost:8080/callback"
}
```

Example:
[subs="attributes"]
 curl -X POST \
  {url-job-service}{jobs-path}/ \
  -H 'Content-Type: application/json' \
  -d '{
	"id": "1",
	"priority": "1",
	"expirationTime": "2019-11-29T18:16:00Z",
	"callbackEndpoint": "http://localhost:8080/callback"
}'

{sp} +

#### Reschedule a Job

POST

{url-job-service}{jobs-path}

```
{
	"id": "1",
	"priority": "1",
	"expirationTime": "2019-11-29T18:19:00Z",
	"callbackEndpoint": "http://localhost:8080/callback"
}
```

Example:
[subs="attributes"]
 curl -X POST \
  {url-job-service}{jobs-path}/ \
  -H 'Content-Type: application/json' \
  -d '{
	"id": "1",
	"priority": "1",
	"expirationTime": "2019-11-29T18:19:00Z",
	"callbackEndpoint": "http://localhost:8080/callback"
}'

{sp} +

#### Cancel a scheduled Job

DELETE

{url-job-service}{jobs-path}/1

Example:
[subs="attributes"]
 curl -X DELETE {url-job-service}{jobs-path}/1

{sp} +

#### Retrieve a scheduled Job

GET

{url-job-service}{jobs-path}/1

Example:
[subs="attributes"]
 curl -X GET {url-job-service}{jobs-path}/1

{sp} +

---
////


////
//@comment: Excluded for now because underlying details that might confuse the user when trying to understand how to actually use it. (Stetson, 1 Apr 2020)
# Kogito Job Service add-ons

Addons are specific classes that provides integration with Kogito Job Service to the runtime services.
This allows to use Job Service as a timer service for process instances.
Whenever there is a need to schedule timer as part of process instance it will be scheduled in the Job Service and the job service will callback the service upon timer expiration.

The general implementation of the add-on is as follows:

* an implementation of `org.kie.kogito.jobs.JobsService` interface that is used by the service to schedule jobs
* REST endpoint registered on `/management/jobs` path

## Configuration properties

Regardless of the runtime being used following are two configuration properties that are expected (and by that are mandatory)

[cols="40%,400%,20%"]
|===
|Name |Description |Example

|`kogito.service.url`
|A URL that identifies where the service is deployed to. Used by runtime events to set the source of the event.
|http://localhost:8080

|`kogito.jobs-service.url`
|An URL that posts to a running Kogito Job Service, it is expected to be in form `scheme://host:port`
|http://localhost:8085
|===

## JobService implementation

A dedicated `org.kie.kogito.jobs.JobsService` implementation is provided based on the runtime being used (either Quarkus or SpringBoot) as it relies on the technology used in these runtime to optimise dependencies and integration.

### Quarkus

For Quarkus based runtimes, there is `org.kie.kogito.jobs.management.quarkus.VertxJobsService` implementation that utilises Vert.x `WebClient` to interact with Job Service over HTTP.

It configures web client by default based on properties found in application.properties.
Though in case this is not enough it supports to provide custom instance of `io.vertx.ext.web.client.WebClient` type that will be used instead to communicate with Job Service.

### Spring Boot

For Spring Boot based runtimes, there is `org.kie.kogito.jobs.management.springboot.SpringRestJobsService` implementation that utilises Spring `RestTemplate` to interact with Job Service over HTTP.

It configures rest template by default based on properties found in application.properties.
Though in case this is not enough it supports to provide custom instance of `org.springframework.web.client.RestTemplate` type that will be used instead to communicate with Job Service.

## REST endpoint for callbacks

The REST endpoint that is provided with the add-on is responsible for receiving the callbacks from Job Service at exact time when the timer was scheduled and by that move the process instance execution forward.

The callback URL is given to the Job Service upon scheduling and as such does provide all the information that are required to move the instance

* process id
* process instance id
* timer instance id

NOTE: Timer instance id is build out of two parts - actual job id (in UUID format) and a timer id (a timer definition id generated by the process engine).
An example of a timer instance id is `62cad2e4-d343-46ac-a89c-3e313a30c1ad_1` where `62cad2e4-d343-46ac-a89c-3e313a30c1ad` is the UUID of the job and `1` is the timer definition id.
Both values are separated with `_`

### API documentation

The current API documentation is based on Swagger, and the service has an embedded UI available at
{url-job-service}/swagger-ui/[{url-job-service}/swagger-ui]
////

[id="proc-jobs-service-using_{context}"]
=== Using the {PRODUCT} Jobs Service as a timer service

[role="_abstract"]
By default, {PRODUCT} services use an in-memory timer service to handle time-based events (jobs) defined in your Business Process Model and Notation (BPMN) process models. This default timer service does not cover long time intervals and is only suitable for short delays defined in the process.

For advanced use cases where time intervals can be days or weeks or when additional event handling options are required, you can configure your {PRODUCT} project to use the {PRODUCT} Jobs Service as an external timer service. Whenever you need to schedule a timer as part of process instance, the timer is scheduled in the Jobs Service and the Jobs Service calls back to the {PRODUCT} service upon timer expiration.

The {PRODUCT} Jobs Service also supports Infinispan, PostgreSQL, or MongoDB persistence that you can enable when you run the Jobs Service so that job data is preserved across application restarts.

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/jobs-service-common[`jobs-service-common`] artifacts page, select the latest release of the Jobs Service, and download the `jobs-service-common-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `jobs-service-common-__VERSION__-runner.jar` file and enter the following command to run the Jobs Service with in-memory persistence enabled:
+
--
.Running the Jobs Service with in-memory persistence enabled
[source,subs="+quotes"]
----
$ java -jar jobs-service-common-__VERSION__-runner.jar
----

If the Jobs Service uses the default in-memory storage, all job information is lost between application restarts.

To change the logging level of the Jobs Service, such as for debugging, you can specify the following start-up properties:

.Modifying Jobs Service logging level for debugging
[source,subs="+quotes"]
----
$ java  \
  -Dquarkus.log.console.level=DEBUG -Dquarkus.log.category.\"org.kie.kogito\".min-level=DEBUG  \
  -Dquarkus.log.category.\"org.kie.kogito\".level=DEBUG  \
  -jar jobs-service-common-__VERSION__-runner.jar
----
--
. In your {PRODUCT} project, add the following dependency to the `pom.xml` file to enable the Jobs Service add-on:
+
--
.On Quarkus
[source, xml]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-quarkus-jobs-management</artifactId>
</dependency>
----

.On Spring Boot
[source, xml]
----
<dependency>
  <groupId>org.kie.kogito</groupId>
  <artifactId>kogito-addons-springboot-jobs-management</artifactId>
</dependency>
----
--
. In your {PRODUCT} project, add the following properties to the `src/main/resources/application.properties` to define the locations of the Jobs Service and the callback to be used when the timer expires:
+
.Configure {PRODUCT} service properties for Jobs Service
[source]
----
kogito.jobs-service.url=http://localhost:8085
kogito.service.url=http://localhost:8080
----
. In a command terminal, navigate to your {PRODUCT} project and run the project using your preferred run mode, such as development mode:
+
--
.On Quarkus
[source]
----
mvn clean compile quarkus:dev
----

.On Sprint Boot
[source]
----
mvn clean compile spring-boot:run
----

With the Jobs Service and your {PRODUCT} project both configured and running, the Jobs Service can receive any job-scheduling requests to function as the external timer service.

By default, the implementation of the Jobs Service uses the following basic components:

* An implementation of the `org.kie.kogito.jobs.JobsService` interface that is used by the service to schedule jobs
* A REST endpoint registered at the path `/management/jobs`

If the default REST clients used by the Jobs Service add-on do not meet your needs, you can configure custom REST clients using the relevant service implementors. The REST client depends on the application type:

* On Quarkus, the Jobs Service uses a Vert.x web client: `io.vertx.ext.web.client.WebClient`
* On Spring Boot, the Jobs Service uses a rest template: `org.springframework.web.client.RestTemplate`

In both cases, you produce an instance of the client to enable detailed setup of the client.
--

NOTE: As an alternative to enabling the Jobs Service explicitly for {PRODUCT} services, you can use the {PRODUCT} Operator to install the Jobs Service custom resource for the service deployment on OpenShift. For more information about installing the Jobs Service with the {PRODUCT} Operator, see {URL_DEPLOYING_ON_OPENSHIFT}#con-kogito-operator-with-jobs-service_kogito-deploying-on-openshift[_{DEPLOYING_ON_OPENSHIFT}_].

[id="proc-jobs-service-persistence_{context}"]
=== Persistence in the {PRODUCT} Jobs Service

[role="_abstract"]
The {PRODUCT} Jobs Service supports the following persistence mechanisms for job data:

* *In-memory persistence*: (Default) Job data is persisted with the Jobs Service in-memory storage during the Jobs Service runtime. If the Jobs Service is restarted, all job information is lost. If no other persistence configuration is set, the Jobs Service uses this persistence mechanism.
* *Infinispan persistence*: Job data is persisted using Infinispan storage so that the data is preserved across application restarts. If the Jobs Service is restarted, the service continues to process any previously scheduled jobs.
* *PostgreSQL persistence*: Job data is persisted using PostgreSQL storage so that the data is preserved across application restarts. If the Jobs Service is restarted, the service continues to process any previously scheduled jobs. Flyway is used to manage database schema.
* *MongoDB persistence*: Job data is persisted using MongoDB storage so that the data is preserved across application restarts. If the Jobs Service is restarted, the service continues to process any previously scheduled jobs.

[id="proc-jobs-service-persistence-infinispan_{context}"]
==== Enabling Infinispan persistence in the {PRODUCT} Jobs Service

You can enable Infinispan persistence in the {PRODUCT} Jobs Service during application start-up.

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/jobs-service-infinispan[`jobs-service-infinispan`] artifacts page, select the latest release of the Jobs Service, and download the `jobs-service-infinispan-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `jobs-service-infinispan-__VERSION__-runner.jar` file and enter the following command to run the Jobs Service with Infinispan persistence enabled:
+
--
.Running the Jobs Service with Infinispan persistence enabled
[source,subs="+quotes"]
----
$ java  \
    -jar jobs-service-infinispan-__VERSION__-runner.jar
----

For more information about Infinispan configuration with Quarkus applications, see https://quarkus.io/guides/infinispan-client[Infinispan client] in the Quarkus documentation.
--

[id="proc-jobs-service-persistence-postgresql_{context}"]
==== Enabling PostgreSQL persistence in the {PRODUCT} Jobs Service

You can enable PostgreSQL persistence in the {PRODUCT} Jobs Service during application start-up.

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/jobs-service-postgresql[`jobs-service-postgresql`] artifacts page, select the latest release of the Jobs Service, and download the `jobs-service-postgresql-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `jobs-service-postgresql-__VERSION__-runner.jar` file and enter the following command to run the Jobs Service with PostgreSQL persistence enabled:
+
--
.Running the Jobs Service with PostgreSQL persistence enabled
[source,subs="+quotes"]
----
$ java  \
    -Dquarkus.datasource.jdbc.url=__DATASOURCE_JDBC_URL__ \
    -Dquarkus.datasource.reactive.url=__DATASOURCE_URL__ \
    -Dquarkus.datasource.username=__DATASOURCE_USERNAME__ \
    -Dquarkus.datasource.password=__DATASOURCE_PASSWORD__ \
    -jar jobs-service-postgresql-__VERSION__-runner.jar
----

[NOTE]
====
By default, https://quarkus.io/guides/flyway[Flyway] is enabled in the {PRODUCT} Jobs Service. According to the `quarkus.datasource.jdbc.url` property, Flyway creates or updates the required database schema to run the Jobs Service, alongside configured user must have permissions to alter the database.

In case you want to manage the database without Flyway automation, set the `-Dquarkus.flyway.migrate-at-start` property to `false` when you start the Jobs Service, otherwise, the service might fail during the initialization.
====

For more information about PostgreSQL configuration with Quarkus applications, see https://quarkus.io/guides/reactive-sql-clients[Quarkus reactive SQL client] and https://quarkus.io/guides/flyway[Flyway] in the Quarkus documentation.
--

[id="proc-jobs-service-persistence-mongodb_{context}"]
==== Enabling MongoDB persistence in the {PRODUCT} Jobs Service

You can enable MongoDB persistence in the {PRODUCT} Jobs Service during application start-up.

.Procedure
. Go to the https://repository.jboss.org/org/kie/kogito/jobs-service-mongodb[`jobs-service-mongodb`] artifacts page, select the latest release of the Jobs Service, and download the `jobs-service-mongodb-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `jobs-service-mongodb-__VERSION__-runner.jar` file and enter the following command to run the Jobs Service with MongoDB persistence enabled:
+
--
.Running the Jobs Service with MongoDB persistence enabled
[source,subs="+quotes"]
----
$ java  \
    -Dquarkus.mongodb.connection-string=__MONGODB_SERVER_CONNECTION_STRING__ \
    -Dquarkus.mongodb.database=__DATABASE_NAME__ \
    -jar jobs-service-mongodb-__VERSION__-runner.jar
----

For more information about MongoDB configuration with Quarkus applications, see https://quarkus.io/guides/mongodb[Using the MongoDB client] in the Quarkus documentation.
--

[id="proc-jobs-service-messaging_{context}"]
=== Enabling Kafka messaging in the {PRODUCT} Jobs Service

[role="_abstract"]
The {PRODUCT} Jobs Service supports Apache Kafka messaging to publish events for each job state transition to a defined Kafka topic. Any application can subscribe to this Kafka topic to receive information about jobs and job state transitions. For example, the {PRODUCT} Data Index Service is subscribed to the Jobs Service Kafka topic so that if you configure and run the Jobs Service, the Data Index Service can begin indexing jobs with their current state.

You can enable Kafka messaging in the {PRODUCT} Jobs Service during application start-up and in the Jobs Service `application.properties` file.

.Procedure
. In the `src/main/resources/application.properties` file in the Jobs Service project, add the following properties to identify the Kafka bootstrap server with the port used to publish events and the Kafka topic where the events are published:
+
.Defining Kafka server and topic in Jobs Service `application.properties`
[source,subs="+quotes"]
----
mp.messaging.outgoing.kogito-job-service-job-status-events.bootstrap.servers=__SERVER_ADDRESS__
mp.messaging.outgoing.kogito-job-service-job-status-events.topic=__TOPIC_NAME__
----
. Run the Jobs Service with the property `-Dquarkus.profile=events-support`:
+
--
.Enabling Kafka messaging during Jobs Service start-up
[source,subs="+quotes"]
----
$ java  \
    -Dquarkus.profile=events-support  \
    -jar jobs-service-common-__VERSION__-runner.jar
----

Alternatively, you can add the environment variable `QUARKUS_PROFILE=events-support`.
--

[id="proc-jobs-service-security_{context}"]
=== Enabling {PRODUCT} Jobs Service security with OpenID Connect

[role="_abstract"]
For Quarkus-based {PRODUCT} services, you can use the https://quarkus.io/guides/security-openid-connect[Quarkus OpenID Connect adapter] with the {PRODUCT} Jobs Service to enable security using bearer token authorization. These tokens are issued by OpenID Connect and OAuth 2.0 compliant authorization servers such as https://www.keycloak.org/about.html[Keycloak].

IMPORTANT: This procedure applies only when you are using a locally cloned copy of the https://github.com/kiegroup/kogito-apps/tree/master/jobs-service[{PRODUCT} Jobs Service] repository in GitHub.

.Prerequisites
* You have cloned the https://github.com/kiegroup/kogito-apps/tree/master/jobs-service[{PRODUCT} Jobs Service] repository from GitHub.

.Procedure
. In a command terminal, navigate to the local clone of the {PRODUCT} Jobs Service repository and enter the following command to run the application with the required security properties:
+
--
.Run the Jobs Service with security properties
[source]
----
mvn clean compile quarkus:dev  -Dquarkus.profile=keycloak
----

The Jobs Service contains a Quarkus profile to encapsulate the security configuration, so if the service requires security, you can specify the `quarkus.profile=keycloak` property at build time to enable the needed security. If the `keycloak` Quarkus profile is not added, the OpenID Connect extension is disabled.
--
. Navigate to the `src/main/resources/application.properties` file of the Jobs Service project and add the following properties:
+
--
.Required security properties in `applications.properties` file
[source]
----
%keycloak.quarkus.oidc.enabled=true
%keycloak.quarkus.oidc.tenant-enabled=true
%keycloak.quarkus.oidc.auth-server-url=http://localhost:8280/auth/realms/kogito
%keycloak.quarkus.oidc.client-id=kogito-jobs-service
%keycloak.quarkus.oidc.credentials.secret=secret
%keycloak.quarkus.http.auth.policy.role-policy1.roles-allowed=confidential
%keycloak.quarkus.http.auth.permission.roles1.paths=/*
%keycloak.quarkus.http.auth.permission.roles1.policy=role-policy1
----

NOTE: The `quarkus.oidc.enabled` property enables or disables security at build time, while the `quarkus.oidc.tenant-enabled` property enables or disables security at runtime.

Replace any property definitions with those of your specific environment, especially the following properties:

* `quarkus.oidc.auth-server-url`: The base URL of the OpenID Connect (OIDC) server, such as `https://localhost:8280/auth`. All other OIDC server page and service URLs are derived from this URL. If you work with Keycloak OIDC server, ensure that the base URL is in the following format: `https://__HOST__:__PORT__/auth/realms/__KEYCLOAK_REALM__`.
* `quarkus.oidc.client-id`: The client ID of the application. Each application has a client ID that is used to identify the application.
* `quarkus.oidc.credentials.secret`: The client secret for the application.
--
. In the same `application.properties` file, also configure the resources to be exposed and the required permissions for accessing the resources.
+
--
NOTE: If you are enabling security at runtime using the `quarkus.oidc.tenant-enabled` property, the `quarkus.http.auth.permission` path and policy must specify how authentication is applied. By default, if security is enabled, the user must be authenticated to access any path.

This example configuration enables only users with role `confidential` to access any endpoint.
--
. Stop and restart the {PRODUCT} Jobs Service to ensure that the security changes are applied.

[role="_additional-resources"]
.Additional resources
* https://quarkus.io/guides/security[Security Architecture and Guides]
* https://quarkus.io/guides/security-openid-connect#configuring-using-the-application-properties-file[Configuring using the application.properties file]
* https://quarkus.io/guides/security-openid-connect-multitenancy[Using OpenID Connect multi-tenancy]

include::{asciidoc-dir}/bpmn/chap-kogito-developing-process-services.adoc[tags=con-bpmn-process-management-addon]

[id="con-trusty-service_{context}"]
== {PRODUCT} Trusty Service and Explainability Service

[role="_abstract"]
{PRODUCT} provides a Trusty Service that stores all {PRODUCT} tracing events related to decisions made in {PRODUCT} services. As an aid to the Trusty Service workflow for storing tracing events, {PRODUCT} also provides a supplemental Explainability Service that provides an explanation for the decisions made in {PRODUCT} services.

The Trusty Service uses Apache Kafka messaging to consume https://cloudevents.io/[CloudEvents] messages from {PRODUCT} services, and then processes the tracing events and stores the data, including any explainability results from the Explainability Service, in the Infinispan persistence store. The Explainability Service likewise uses Apache Kafka messaging to consume CloudEvents messages from the Trusty Service, and then applies explainability algorithms. Some algorithms require the Explainability Service to interact with the {PRODUCT} service that evaluated the decision. This communication is performed with HTTP `POST` requests.

The Trusty Service and Explainability Service are at the core of the TrustyAI metrics monitoring initiative in {PRODUCT}.

.Trusty Service and Explainability Service architecture in an example {PRODUCT} service
image::kogito/configuration/trusty-architecture_enterprise.png[Diagram of an example Kogito service using Trusty Service]

The {PRODUCT} Trusty Service has the following key attributes:

* Focus on decisions
* Flexible data structure
* Distributable and cloud-ready format
* Infinispan-based persistence support
* Message-based communication with {PRODUCT} runtime (Apache Kafka, cloud events )
* Integration with the Explainability Service to retrieve advanced analysis for the decisions

The {PRODUCT} Trusty Service and Explainability Service are Quarkus applications, based on https://vertx.io/[Vert.x] with https://smallrye.io/smallrye-reactive-messaging/[Reactive Messaging]. Tracing data from the Trusty Service is parsed and pushed into the *Decisions* cache for each decision made in a {PRODUCT} service. Each record contains information about all decision inputs, outputs, and errors, if any.

The Trusty Service storage is provided by https://infinispan.org/[Infinispan]. Communication between the Trusty Service and Infinispan is handled through a protobuf (https://developers.google.com/protocol-buffers/[protocol buffers]) schema and generated marshallers.

After the tracing event is analyzed and stored, the Trusty Service exposes the data with a dedicated API.

For information about using the {PRODUCT} Trusty Service and Explainability Service with the {PRODUCT} Operator, see {URL_DEPLOYING_ON_OPENSHIFT}#con-kogito-operator-with-trusty-service_kogito-deploying-on-openshift[_{DEPLOYING_ON_OPENSHIFT}_].

For example {PRODUCT} services that use the Trusty Service and Explainability Service, see the following example applications in GitHub:

* https://github.com/kiegroup/kogito-examples/tree/stable/dmn-tracing-quarkus[`dmn-tracing-quarkus`]: A DMN decision service on Quarkus that uses the `kogito-addons-quarkus-tracing-decision` add-on to generate tracing events that the {PRODUCT} Trusty Service and Explainability Service can consume and expose.
* https://github.com/kiegroup/kogito-examples/tree/stable/trusty-demonstration[`trusty-demonstration`]: A tutorial for deploying the `dmn-tracing-quarkus` example application on Kubernetes as a demonstration of {PRODUCT} Trusty Service and Explainability Service capabilities in a cloud environment.

[id="proc-trusty-service-security_{context}"]
=== Enabling {PRODUCT} Trusty Service security with OpenID Connect

[role="_abstract"]
For Quarkus-based {PRODUCT} services, you can use the https://quarkus.io/guides/security-openid-connect[Quarkus OpenID Connect adapter] with the {PRODUCT} Trusty Service to enable security using token authorization. These tokens are issued by OpenID Connect and OAuth 2.0 compliant authorization servers such as https://www.keycloak.org/about.html[Keycloak].

IMPORTANT: This procedure applies only when you are using a locally cloned copy of the https://github.com/kiegroup/kogito-apps/tree/master/trusty[{PRODUCT} Trusty Service] repository in GitHub.

.Prerequisites
* You have cloned the https://github.com/kiegroup/kogito-apps/tree/master/trusty[{PRODUCT} Trusty Service] repository from GitHub.

.Procedure
. In a command terminal, navigate to the local clone of the {PRODUCT} Trusty Service repository and enter the following command to run the application with the required security properties:
+
--
.Run the trusty Service with security properties
[source]
----
mvn clean compile quarkus:dev -Dquarkus.profile=keycloak
----

The {PRODUCT} Trusty Service contains a Quarkus profile to encapsulate the security configuration, so if the service requires security, you can specify the `quarkus.profile=keycloak` property at build time to enable the needed security. If the `keycloak` Quarkus profile is not added, the OpenID Connect extension is disabled.
--
. Navigate to the `src/main/resources/application.properties` file of the Trusty Service project and add the following properties:
+
--
.Required security properties in `applications.properties` file
[source]
----
# OpenID Connect configurations
%keycloak.quarkus.oidc.enabled=true
%keycloak.quarkus.oidc.tenant-enabled=true
%keycloak.quarkus.oidc.auth-server-url=http://localhost:8280/auth/realms/kogito
%keycloak.quarkus.oidc.client-id=kogito-service
%keycloak.quarkus.oidc.credentials.secret=secret
%keycloak.quarkus.oidc.application-type=service

# HTTP security configurations
%keycloak.quarkus.http.auth.permission.authenticated.paths=/*
%keycloak.quarkus.http.auth.permission.authenticated.policy=authenticated
----

NOTE: The `quarkus.oidc.enabled` property enables or disables security at build time, while the `quarkus.oidc.tenant-enabled` property enables or disables security at runtime.

Replace any property definitions with those of your specific environment, especially the following properties:

* `quarkus.oidc.auth-server-url`: The base URL of the OpenID Connect (OIDC) server, such as `https://localhost:8280/auth`. All other OIDC server page and service URLs are derived from this URL. If you work with Keycloak OIDC server, ensure that the base URL is in the following format: `https://__HOST__:__PORT__/auth/realms/__KEYCLOAK_REALM__`.
* `quarkus.oidc.client-id`: The client ID of the application. Each application has a client ID that is used to identify the application.
* `quarkus.oidc.credentials.secret`: The client secret for the application.

NOTE: If you are enabling security at runtime using the `quarkus.oidc.tenant-enabled` property, the `quarkus.http.auth.permission` path and policy must specify how authentication is applied. By default, if security is enabled, the user must be authenticated to access any path.

--
. Stop and restart the {PRODUCT} Trusty Service to ensure that the security changes are applied.

[id="con-kogito-task-assigning-service_{context}"]
== {PRODUCT} Task Assigning Service

[role="_abstract"]
{PRODUCT} provides a Task Assigning Service that enables you to assign the user tasks created by the {PRODUCT} services to users as part of an optimal solution. This optimal solution is calculated by {PRODUCT_OP}, a lightweight and embeddable planning engine that optimizes planning problems.

You can use {PRODUCT} services to model and execute multiple business processes. A typical business process consists of different nodes including events, gateways, connectors, and tasks to define a process logic. Each node in a business process contains a specific semantic and is instantiated using a process engine when a related process instance is executed. Business processes that require the intervention of a human user are designed using user task nodes.

The following example business process consists of three tasks that require human intervention:

.Example business process
image::kogito/configuration/task-assigning-service/PurchaseOrderProcessDefinition.png[]

When a system executes several process instances, it can have many user task instances waiting for external user action.

.Default task inbox
image::kogito/configuration/task-assigning-service/TaskListWithClaimAvailableActions.png[]

[id="con-bpm-task-assigning-service_{context}"]
=== BPM standard Task Assigning

Generally, the runtime engines assign user tasks to a group of users. The target audience is added as part of the user task configuration. In the following example, you can see how *Review Purchase Order* task is configured with the `PurchaseDepartment` group:

.Example user task configuration
image::kogito/configuration/task-assigning-service/ReviewPurchaseOrderTaskConfiguration.png[]

At runtime, when a new instance of the *Review Purchase Order* task is created, the instance is automatically transitioned to the *Active* phase and assigned to the *PurchaseDepartment* group with the status *Ready*. This makes the *Review Purchase Order* task available in the task inbox for all the users in the *PurchaseDepartment* group. To get ownership of the task, a user must execute the claim operation. The following example shows how a user is claiming `#cbf41` task.

.Example task claim operation
image::kogito/configuration/task-assigning-service/TaskListWithAvailableActionsForSelectedTask.png[]

When a user claims a task, the status of the task is changed to *Reserved* and the task is assigned to the claiming user. The user becomes the actual owner of the task and the task appears only on the user's Task Inbox. The following example shows the task information and available actions for `#cbf41` task after the execution of the claim operation:

.Example execution of a task claim operation
image::kogito/configuration/task-assigning-service/TaskListWithAvailableActionsForReservedTaskWithUser1.png[]

[id="con-optaplanner-driven-task-assigning_{context}"]
=== {PRODUCT_OP} driven task assigning

The Task Assigning Service enables you to assign the user tasks among the users according to an optimal plan, which is calculated by {PRODUCT_OP}. The following example shows the content of Task Inbox for `user1` and `user2` when the Task Assigning Service is running:

.Example tasks assigned by the Task Assigning Service
image::kogito/configuration/task-assigning-service/TasksAssignedToUser1AndUser2ByTheTaskAssigningService.png[]

The previous example shows that user tasks including `#895c6` and `#cbf41` are assigned to `user1` and user tasks `#5aac` and `#ddc34` are assigned to `user2`. The remaining tasks including `#fdcc0`, `#a93d4`, and `#5be2b` are unassigned tasks and will be analyzed later.

The back-end Task Assigning process for the previous example includes the following steps:

. All the available tasks with *Ready* status are consumed by the Task Assigning Service.
. `user1` and `user2` are identified as part of the `PurchaseDeparment` group.
. It is identified that the *Review Purchase Order* task is defined for the `PurchaseDepartment` group.
. An optimized plan is calculated and all the tasks including unassigned tasks are assigned based on their configuration such as group condition and {PRODUCT_OP} optimization rules. The resulting optimized plan consists of the following statements:
+
* Tasks `#895ca` and `#cbf41` must be assigned to `user1`
* Tasks `#5aa2c` and `#ddc34` must be assigned to `user2`

. The Task Assigning Service performs the assignments. Note that the users are not required to execute the `claim` operation. Therefore, the users focus on completing their assigned tasks, and the assignment is performed by {PRODUCT_OP}.

Overall, the Task Assigning Service analyzes the available tasks, calculates an optimized plan using {PRODUCT_OP}, and generates the necessary assignments in the process services. The users rely on the optimized plan and focus on executing their assigned tasks.

[NOTE]
====
In the previous example, only user tasks configuration is considered for the assignments calculation by {PRODUCT_OP}. However, the assignment calculation can also include the business information related to the user tasks.
====

The {PRODUCT} services that are running create many process instances and user tasks, and at the same time the users complete the assigned tasks. This user task creation and completion cycles repeatedly. In this scenario, the Task Assigning Service instructs the {PRODUCT_OP} to re-calculate the plan and generate new assignments to reflect a new optimized plan. {PRODUCT_OP} consists of the ability to react on the changing context. For more information, see the https://docs.optaplanner.org/latestFinal/optaplanner-docs/html_single/index.html[{PRODUCT_OP} documentation].

The continuous task reassignments can have negative impact on the user experience. This problem can be managed by combining different repeated and continuous planning techniques. For more information, see the Repeated Planning section of the https://docs.optaplanner.org/latestFinal/optaplanner-docs/html_single/index.html[{PRODUCT_OP} documentation].

A simple strategy is to introduce the concept of a planning window, which is a defined subset of the tasks that are available and cannot be changed or reassigned when new plans are calculated. Only the planning window is made available to the users, and the tasks in the planning window are known as published tasks.

The previous example contains a planning window with a size of two. This indicates that only two tasks from the optimized plan are assigned in the processes runtime for each user to focus on. When the users complete the assigned tasks, the Task Assigning Service assigns additional tasks based on the optimized plan. Also, {PRODUCT_OP} maintains information about the best assignments for all the tasks.

In the previous example, the assignments are distributed as follows:

* `user1` is assigned to `#895ca` and `#cbf41` tasks, which belong to the planning window
* `user2` is assigned to `#5aa2c` and `#ddc34` tasks, which belong to the planning window

The unassigned tasks do not belong to the planning window, therefore, the unassigned tasks are assigned only internally by {PRODUCT_OP} and not in the process runtime.

[id="con-task-assigning-service-kogito-services_{context}"]
=== Task Assigning Service in {PRODUCT} services

The following figure displays the architecture of a standard {PRODUCT} services installation, in which the Task Assigning Service is optional:

.Task Assigning Service in {PRODUCT} services installation
image::kogito/configuration/task-assigning-service/TaskAssigningServiceArchitecture.png[Task Assigning Service Architecture]

The Task Assigning Service also interacts with the following {PRODUCT} services:

* Apache Kafka messaging service to consume the CloudEvents that are generated by the user tasks
* Data Index Query API to load the tasks status information when required
* {PRODUCT} services runtime API to generate the desired task assignments
* User Service Connector to connect with external user service and receive user information

[id="proc-using-task-assigning-service_{context}"]
=== Using the Task Assigning Service

You can use the Task Assigning Service to assign the user tasks that are created by the {PRODUCT} services to users using an optimal solution. The optimal solution is calculated by {PRODUCT_OP}.

.Prerequisites

* https://kafka.apache.org/[Apache Kafka] is installed, including the {PRODUCT} runtimes required topics, and the Kafka messaging server is running. For information about Kafka installation and configuration, see https://kafka.apache.org/documentation/[Apache Kafka documentation].
* The {PRODUCT} Data Index Service is running. For information about Data Index Service installation and configuration see {URL_CONFIGURING_KOGITO}#con-data-index-service_kogito-configuring[{PRODUCT} Data Index Service].
* {PRODUCT} services are running. The set of {PRODUCT} services with business processes are running and properly configured for generating the user tasks related to CloudEvents and sending them to the Apache Kafka messaging service. For more information about creating and running {PRODUCT} services see {URL_CREATING_RUNNING}#chap-kogito-creating-running[_{CREATING_RUNNING}_].

.Procedure

. Go to the https://repository.jboss.org/org/kie/kogito/task-assigning-service/[`task-assigning-service`] artifacts page, select the latest release of the Task Assigning Service, and download the `task-assigning-service-__VERSION__-runner.jar` file to a local directory.
. In a command terminal, navigate to the directory location of the downloaded `task-assigning-service-__VERSION__-runner.jar` file and enter the following command to run the Task Assigning Service:
+
.Running the Task Assigning Service
[source,subs="+quotes"]
----
$ java -jar task-assigning-service-__VERSION__-runner.jar
----

[id="con-task-assigning-service-config_{context}"]
=== Task Assigning Service configuration

Considering that the Task Assigning Service is a Quarkus application, then all the configuration parameters defined in the next topics can be established by using any of the available Quarkus configuration sources.

For more information about configuring a Quarkus application, see https://quarkus.io/guides/config[Quarkus - Configuring your Application] in the Quarkus documentation.

[NOTE]
====
Generally, most of the parameters are pre-configured with appropriate values. However, a configuration file `application.properties` placed in the `$TASK_ASSIGNING_SERVICE_HOME/config` directory, can be used to configure the Task Assigning Service parameters.
====

[id="ref-global-config-params-task-assigning_{context}"]
==== Global configuration parameters of Task Assigning Service

The Task Assigning Service contains the following global configuration parameters:

.Global configuration parameters of Task Assigning Service
[cols="50%,40%,10%", options="header"]
|===
|Parameter
|Description
|Default value

| `kogito.task-assigning.data-index.server-url`
| URL of the Data Index Service
| http://localhost:8180/graphql

| `kogito.task-assigning.data-index.connect-timeout-duration`
a| Timeout for the established connection when a Data Index Service query is executed

For example, a value of 0 that is `PT0S` represents no timeout. The format is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. For example:

* `PT30S`: configures 30 seconds
* `PT1.500S`: configures 1500 milliseconds
* `PT0.500S`: configures 500 milliseconds

| `PT30S`

| `kogito.task-assigning.data-index.read-timeout-duration`
a| Timeout for the data reading when a Data Index Service query is executed

The value of this parameter must be adjusted according to the amount of expected data and is typically related with the `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/pageSize` property.

The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. For example:

* `PT3M`: configures 3 minutes
* `PT1.500S`: configures 1500 milliseconds
* `PT0.500S`: configures 500 milliseconds

| `PT3M`

| `kogito.task-assigning.process-runtime.connect-timeout-duration`
a| Timeout for the established connection when a {PRODUCT} runtimes operation is executed

For example, a value of 0 that is `PT0S` represents no timeout. The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. For example:

* `PT30S`: configures 30 seconds
* `PT1.500S`: configures 1500 milliseconds
* `PT0.500S`: configures 500 milliseconds

| `PT30S`

| `kogito.task-assigning.process-runtime.read-timeout-duration`
a| Timeout for the data reading when a {PRODUCT} runtimes operation is executed

For example, a value of 0 that is `PT0S` represents no timeout. The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours. For example:

* `PT1M`: configures 1 minute
* `PT1.500S`: configures 1500 milliseconds
* `PT0.500S`: configures 500 milliseconds

| `PT1M`

| `kogito.task-assigning.oidc-client`
| Name of the OidcClient to use when an OpenId Connect compliant authorization server such as Keycloak is being used for securing the access to the Data Index Service and the {PRODUCT} runtimes.

The same OidcClient configuration is used for all the accesses, so you must have the proper permissions for accessing both the Data Index Service and the {PRODUCT} runtimes.

For more information about how to configure this OidcClients, see https://quarkus.io/guides/security-openid-connect-client[Quarkus - Using OpenID Connect Client].

| NA

| `kogito.task-assigning.user`
| User ID for connecting to the Data Index Service and the {PRODUCT} runtimes when the systems are secured using the Basic Authentication mechanism.

| NA

| `kogito.task-assigning.password`
| User password for connecting to the Data Index Service and the {PRODUCT} runtimes when the systems are secured using the Basic Authentication mechanism.
| NA

| `kogito.task-assigning.publish-window-size`
| Maximum number of tasks for each user that are assigned in {PRODUCT} runtimes when an optimized plan is calculated.


The value of this parameter must be low to 2, 3, or 4, as it is expected that the tasks are assigned to the users according to an optimized plan, which changes over the time. The high values for this parameter leads to BPM standard task assigning that can nullify the Task Assigning Service work.

| `2`

| `kogito.task-assigning.wait-for-improved-solution-duration`

| Time interval to improve a solution before the optimized plan is sent to the {PRODUCT} runtimes.

This time interval is applied every time a new set of changes is processed, therefore it must be short. For example, `PT0.500` representing 500 milliseconds.

The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours.

| `PT0S` (no wait)

| `kogito.task-assigning.improve-solution-on-background-duration`

| Time interval for performing background optimization of the current solution after the optimized plan is sent to the {PRODUCT} runtimes.

In case, no changes are produced in the processes and a better solution is calculated during that period, then a new optimized plan is automatically sent to the {PRODUCT} runtimes.

The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours.

| `PT1M`

| `kogito.task-assigning.user-service-connector`
| Name of the User Service Connector for accessing an external user service.
| `PropertiesConnector`

| `kogito.task-assigning.user-service-sync-interval`

a| Time interval for refreshing the user information from an external user service.

The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours.

* `PT2H`: configures 2 hours
* `PT30M`: configures 30 minutes

| `PT2H`

| `quarkus.optaplanner.solver-config-xml`

| Path to classpath resource with the solver configuration when a customized configuration is used.

If the resource is not found or the configuration is incorrect, the Task Assigning Service does not start.

| `taskAssigningDefaultSolverConfig.xml`

| `org.kie.kogito.taskassigning.core.model.planningUserId`

a| This parameter configures the user for being assigned with the tasks that no other user in the system can be assigned to.

For example, if a task has a required skill "astronaut" and no other user can be found with this skill, it is assigned to the `planninguser`. However, this parameter is only restricted to skills.

| `planninguser`
|===

[id="con-task-assigning-kafka-messaging-config_{context}"]
==== Kafka messaging configuration

The Task Assigning Service reacts upon the CloudEvents that are generated by the user tasks in {PRODUCT} services.

The following table describes the set of parameters that are required to configure Apache Kafka messaging. However, based on the target installation, any configuration parameter of the Quarkus Apache Kafka can be used. For more information, see https://quarkus.io/guides/kafka#configuration[Quarkus Apache Kafka Configuration].

.Kafka messaging configuration parameters
[cols="30%,50%,20%",options="header"]
|===
|Parameter
|Description
|Default value

| `kafka.bootstrap.servers`
| Comma-separated list of `host:port` to establish the initial connection to the Kafka cluster.

For more information about Kafka configuration, see https://quarkus.io/guides/kafka#configuring-the-kafka-connector[Configuring the Kafka connector].

| `localhost:9290`

| `mp.messaging.incoming.kogito-usertaskinstances-events.topic`

| Kafka topic where the CloudEvents are published. The CloudEvents are generated by the user tasks in {PRODUCT} runtimes.

| `kogito-usertaskinstances-events`

| `mp.messaging.incoming.kogito-usertaskinstances-events.group.id`

| Kafka group ID for consumption of the CloudEvents. The CloudEvents are generated by the user tasks in {PRODUCT} runtimes.

Generally, the default value must not be changed unless necessary. If the default value is changed, then the changed value must not be used in the same group ID as another service that consumes the CloudEvents, such as Data Index Service.

| `kogito-task-assigning-service`

|===

==== Solution data loading configuration

When you start the Task Assigning Service, it is required to execute a set of initial data loading queries against the Data Index Service and the configured User System Service.

The following table describes the parameters that are required to configure the data loading:

.Data loading configuration parameters
[cols="30%,50%,20%", options="header"]
|===
|Parameter
|Description
|Default value

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/pageSize`

| Page size for the data loading queries of tasks
| `3000`

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/Retry/maxRetries`

a| Number of retries to execute when the data loading operation fails. `-1` represents an infinite number of retries.

NOTE: In case the maximum number of configured retries is reached, the Task Assigning Service fails.

| `-1`

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/Retry/delay`

| Time delay in retries

| `2000`

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/Retry/delayUnit`
| Time unit to measure the delay in retries. The valid values are defined by the `java.time.temporal.ChronoUnit` excluding NANOS and MICROS.

| MILLIS

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/Retry/maxDuration`

a| Maximum amount of time for continuing retrying.

NOTE: In case the maximum duration is reached, the Task Assigning Service fails.

| `15`

| `org.kie.kogito.taskassigning.service.SolutionDataLoader/loadSolutionData/Retry/durationUnit`

| Time unit to measure the max duration. The valid values are defined by the `java.time.temporal.ChronoUnit` excluding NANOS and MICROS.

| MINUTES

|===

The data loading configuration parameters conform with the Quarkus fault tolerance mechanisms. For more information about the fault tolerance mechanism, see https://quarkus.io/guides/smallrye-fault-tolerance[Quarkus - Smallrye Fault Tolerance].

==== Users data synchronization configuration

The following table describes the configuration parameters that are applied when the users data is synchronized from an external user service. The users data synchronization is executed on a regular configurable basis.

.Users data synchronization configuration parameters
[cols="30%,50%,20%", options="header"]
|===
|Parameter
|Description
|Default value

| `kogito.task-assigning.user-service-sync-interval`

a| Time interval for refreshing the user information using a user service

The format of the value is based on the ISO-8601 duration format PnDTnHnMn.nS with days considered to be exactly 24 hours.

* `PT2H`: configures 2 hours
* `PT30M`: configures 30 minutes

| `PT2H`

| `org.kie.kogito.taskassigning.service.UserServiceAdapter/loadUsersData/Retry/maxRetries`

a| Number of retries to execute when the data loading operation fails. The value `-1` represents an infinite number of retries.

NOTE: In case the maximum number of configured retries is reached, the Task Assigning Service fails.

| `-1`

| `org.kie.kogito.taskassigning.service.UserServiceAdapter/loadUsersData/Retry/delay`

a| Time delay when performing retries.

| 2000

| `org.kie.kogito.taskassigning.service.UserServiceAdapter/loadUsersData/Retry/delayUnit`

| Time unit to measure the delay when performing retries. The valid values are defined by the `java.time.temporal.ChronoUnit` excluding NANOS and MICROS.

| MILLIS

| `org.kie.kogito.taskassigning.service.UserServiceAdapter/loadUsersData/Retry/maxDuration`

a| Maximum amount of time to continue retrying without success.

NOTE: In case the maximum duration is reached, the Task Assigning Service fails.

| `5`

| org.kie.kogito.taskassigning.service.UserServiceAdapter/loadUsersData/Retry/durationUnit

| Time unit to measure the maximum duration. The valid values are defined by the `java.time.temporal.ChronoUnit` excluding NANOS and MICROS.

| MINUTES

|===

The configuration parameters of users data synchronization conform with the Quarkus fault tolerance mechanisms. For more information about the fault tolerance mechanism, see https://quarkus.io/guides/smallrye-fault-tolerance[Quarkus - Smallrye Fault Tolerance].

[id="con-user-service-connector_{context}"]
==== User Service Connector

The calculation of an optimal plan for assigning user tasks requires consideration of business information as part of the definitions of the users, for example, using groups, skills related to a user, or affinities in certain topics. For more information about using skills and affinities, see xref:con-skills-affinities_kogito-configuring[].

The business-related information is managed by an external user service and must be provided in each particular installation. The retrieval of this information is delegated to the `UserServiceConnector` component.

A `UserServiceConnector` component must implement the following interface:

.UserServiceConnector API
[source,java]
----
/**
 * User service connector implementations are loaded by CDI and must be qualified with the
 * UserServiceConnectorQualifier in order to configure the particular implementation name.
 * This name is used for configuring the property kogito.task-assigning.user-service-connector
 * in cases where this connector is to be used.
 *
 * e.g.
 *
 *
 *     @ApplicationScoped
 *     @UserServiceConnectorName("MyUserServiceConnector")
 *     public class MyUserServiceConnector implements UserServiceConnector {
 *          ......
 *     }
 */
public interface UserServiceConnector {

    /**
     * Invoked by the task assigning service as part of the initialization procedure and
     * before any other method is invoked.
     */
    void start();

    /**
     * @return the list of all users present in the external user service.
     */
    List<User> findAllUsers();

    /**
     * Get the user information corresponding the user identified by the id.
     *
     * @param id a user identifier.
     * @return the User corresponding to the given identifier, null if no user was found.
     */
    User findUser(String id);

}
----

[id="con-user-service-connector-config_{context}"]
==== User Service Connector configuration

The implementation of User Service Connector is loaded by CDI and must be qualified using the `UserServiceConnectorQualifier` to define a particular name. The defined name can be used for configuring the `kogito.task-assigning.user-service-connector` parameter.

By default, the following configuration is provided:

.Default User Service Connector configuration
[source,subs="+quotes"]
----
kogito.task-assigning.user-service-connector=PropertiesConnector
----

[id="con-properties-connector_{context}"]
====  PropertiesConnector

The `PropertiesConnector` is a basic `UserServiceConnector` implementation that loads the user definitions and attributes from the properties encountered on the different configuration sources that can be used for the Task Assigning Service Quarkus application, for example, the `applicaion.properties` file.

For more information about configuring a Quarkus application, see https://quarkus.io/guides/config[Quarkus - Configuring your Application].

You can define the users using the following examples:

.Example for defining users
[source]
----
kogito.task-assigning.properties-connector.user.poul.groups=interns,managers
kogito.task-assigning.properties-connector.user.poul.attribute.skills=C#,kotlin
kogito.task-assigning.properties-connector.user.poul.attribute.affinities=sports
kogito.task-assigning.properties-connector.user.poul.attribute.name=Poul
kogito.task-assigning.properties-connector.user.poul.attribute.surname=Simon
----

The previous example defines the following user:

.Example user
[source]
----
User {
    id = "poul",
    GroupSet = {"interns", "managers"}
    AttributeMap = {
        {"name", "Poul"}
        {"surname", "Simon"}
        {"skills", "C#,kotlin"}
        {"affinities", "sports"}
}
----

By using the Quarkus profiles configuration mechanism, you can also define users in a per profile basis, for example:

.Example for defining users in per profile basis
[source]
----
%dev.kogito.task-assigning.properties-connector.user.mary.groups=managers
%dev.kogito.task-assigning.properties-connector.user.mary.attribute.skills=javascript,ada
%dev.kogito.task-assigning.properties-connector.user.mary.attribute.affinities=movies
%dev.kogito.task-assigning.properties-connector.user.mary.attribute.name=Mary
%dev.kogito.task-assigning.properties-connector.user.mary.attribute.surname=Poppins
----

In the previous example, user `Mary` is considered only when the `dev` profile is activated. For example, you can use the  Quarkus configuration parameter `-Dquarkus.profile=dev` when the application starts.

[id="ref-default-constraints_{context}"]
==== Default constraints

The Task Assigning Service uses different constraints to generate an optimized plan. Generally, a large set of use cases is covered and no extensions are required to use the constraints. However, you can work with the customized set of constraints provided by users if required. For more information about using customized constraints, see xref:proc-custom-solver-constraints-config_kogito-configuring[].

To generate the optimized solutions, a `BendableLongScore` with two levels of hard constraints and six levels of soft constraints is used. You can customize these constraints by following a set of restrictions.

.Default constraints for Task Assigning Service
[cols="30%,15%,55%", options="header"]
|===
|Constraint
|Level
|Description

| Required Potential Owner
| Hard constraint 0 (required)
| Determines whether a task must be assigned to one of its Potential Owners or the Planning User in case no Potential Owner is found.

The customization provided by users must include this constraint as a first-level hard constraint, otherwise, the business process semantics is not considered by the Task Assigning Service. For example, the tasks are assigned to the users who are not Potential Owners.

In case the `Required Potential Owner` constraint is customized, the task must be assigned to the Planning User when no other user is suitable for the customized condition.

| Required Skills
| Hard constraint 1
| Determines that a task can only be assigned to a user, who has all the configured skills. For more information about skills, see xref:con-skills-affinities_kogito-configuring[]. In case, no user is found with the configured skills of the task, then the task is assigned to the Planning User. However, if the task does not have configured skills, the constraint contains no effect.

In case the `Required Skills` constraint is customized, the task must be assigned to the Planning User when no other user is suitable for the customized condition.

| `PlanningUser` assignment
| Soft constraint 0 (required)
| Penalizes the Planning User assignment. This constraint enforces the minimization of the Planning User assignment and ensures that the Planning User is assigned as the last available option. This constraint cannot be customized.

| High level priority
| Soft constraint 1
| Enforces the assignment of higher priority tasks first when possible.

| Desired affinities
| Soft constraint 2
| Assigns the tasks according to the configured affinities. For more information about affinities, see xref:con-skills-affinities_kogito-configuring[]. If a task has configured affinities, the task is assigned to the user with most of the related affinities.

In case, a task does not have configured affinities, the constraint does not affect.

| Minimize makespan
| Soft constraint 3 (required)
| Reduces the time to complete all the tasks. This constraint must be included.

| Medium level priority
| Soft constraint 4
| Ensures that medium level priority tasks are assigned after higher priority tasks when possible.

| Low level priority
| Soft constraint 5
| Ensures that low level priority tasks are assigned at last when possible.

|===

[NOTE]
====
The `TaskAssigningSolution` is based on the `BendableLongScore` scoring function, therefore any potential user provided set of constraints must be based on the same scoring function. However, the scoring function and the core model classes can be changed in a future release.
====

The following code shows the implementation of default constraints:

.Default constraints implementation
[source,java]
----
public class DefaultTaskAssigningConstraints {

    public static Constraint requiredPotentialOwner(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> !TaskAssigningConditions.userMeetsPotentialOwnerOrPlanningUserCondition(taskAssignment.getTask(), taskAssignment.getUser()))
                .penalize("Required Potential Owner", constraintWeight);
    }

    public static Constraint requiredSkills(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> !TaskAssigningConditions.userMeetsRequiredSkillsOrPlanningUserCondition(taskAssignment.getTask(), taskAssignment.getUser()))
                .penalize("Required Skills", constraintWeight);
    }

    public static Constraint planningUserAssignment(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> ModelConstants.IS_PLANNING_USER.test(taskAssignment.getUser().getId()))
                .penalize("PlanningUser assignment", constraintWeight);
    }

    public static Constraint highLevelPriority(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> PriorityHelper.isHighLevel(taskAssignment.getTask().getPriority()))
                .penalize("High level priority",
                        constraintWeight,
                        TaskAssignment::getEndTimeInMinutes);
    }

    public static Constraint desiredAffinities(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> taskAssignment.getUser().isEnabled())
                .reward("Desired Affinities",
                        constraintWeight,
                        taskAssignment -> TaskHelper.countMatchingLabels(taskAssignment.getTask(), taskAssignment.getUser(), DefaultLabels.AFFINITIES.name()));
    }

    public static Constraint minimizeMakespan(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> taskAssignment.getNextElement() == null)
                .penalize("Minimize makespan",
                        constraintWeight,
                        taskAssignment -> taskAssignment.getEndTimeInMinutes() * taskAssignment.getEndTimeInMinutes());
    }

    public static Constraint mediumLevelPriority(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> PriorityHelper.isMediumLevel(taskAssignment.getTask().getPriority()))
                .penalize("Medium level priority",
                        constraintWeight,
                        TaskAssignment::getEndTimeInMinutes);
    }

    public static Constraint lowLevelPriority(ConstraintFactory constraintFactory, Score<?> constraintWeight) {
        return constraintFactory.from(TaskAssignment.class)
                .filter(taskAssignment -> PriorityHelper.isLowLevel(taskAssignment.getTask().getPriority()))
                .penalize("Low level priority",
                        constraintWeight,
                        TaskAssignment::getEndTimeInMinutes);
    }

    public static BendableLongScore hardLevelWeight(int hardLevel, long hardScore) {
        return BendableLongScore.ofHard(HARD_LEVELS_SIZE, SOFT_LEVELS_SIZE, hardLevel, hardScore);
    }

    public static BendableLongScore softLevelWeight(int softLevel, long softScore) {
        return BendableLongScore.ofSoft(HARD_LEVELS_SIZE, SOFT_LEVELS_SIZE, softLevel, softScore);
    }
}
----

The following code shows the implementation of constraint provider used by the default Solver configuration:

.Default constraint provider
[source,java]
----
public class DefaultTaskAssigningConstraintProvider implements ConstraintProvider {

    @Override
    public Constraint[] defineConstraints(ConstraintFactory constraintFactory) {
        return new Constraint[] {
                requiredPotentialOwner(constraintFactory),
                requiredSkills(constraintFactory),
                planningUserAssignment(constraintFactory),
                highLevelPriority(constraintFactory),
                desiredAffinities(constraintFactory),
                minimizeMakespan(constraintFactory),
                mediumLevelPriority(constraintFactory),
                lowLevelPriority(constraintFactory)
        };
    }

    protected Constraint requiredPotentialOwner(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.requiredPotentialOwner(constraintFactory, hardLevelWeight(0, 1));
    }

    protected Constraint requiredSkills(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.requiredSkills(constraintFactory, hardLevelWeight(1, 1));
    }

    protected Constraint planningUserAssignment(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.planningUserAssignment(constraintFactory, softLevelWeight(0, 1));
    }

    protected Constraint highLevelPriority(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.highLevelPriority(constraintFactory, softLevelWeight(1, 1));
    }

    protected Constraint desiredAffinities(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.desiredAffinities(constraintFactory, softLevelWeight(2, 1));
    }

    protected Constraint minimizeMakespan(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.minimizeMakespan(constraintFactory, softLevelWeight(3, 1));
    }

    protected Constraint mediumLevelPriority(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.mediumLevelPriority(constraintFactory, softLevelWeight(4, 1));
    }

    protected Constraint lowLevelPriority(ConstraintFactory constraintFactory) {
        return DefaultTaskAssigningConstraints.lowLevelPriority(constraintFactory, softLevelWeight(5, 1));
    }
}
----

[NOTE]
====
Users providing customization of the Solver configuration can use the `org.kie.kogito.taskassigning.core.model.solver.DefaultTaskAssigningConstraints` and `org.kie.kogito.taskassigning.core.model.solver.DefaultTaskAssigningConstraintProvider` classes as a start point. However, these classes are not part of the public API and therefore, might change in a future release.
====

For more information about constraints building and configuration see https://docs.optaplanner.org/latestFinal/optaplanner-docs/html_single/index.html[{PRODUCT_OP} documentation].

[id="con-skills-affinities_{context}"]
==== Skills and affinities

The skills and affinities in the Task Assigning Service implement the ability to declare business-related information. The business-related information is considered by the default constraints or the customized set of constraints, and introduce additional decision elements that can be combined with the group-based assignment semantics defined in the business process to assign the user tasks.

Internally, this mechanism is based on the ability to generate calculated attributes that are automatically set to the task and user representations used by OptaPlanner. These attributes are generated by the `TaskAttributesProcessor` and the `UserAttributesProcessor`.

You can link business-related information to a user task using the input configuration of the same user task. The following is an example of input configuration:

.Example skills and affinities configuration.
image::kogito/configuration/task-assigning-service/SkillsAndAffinitiesConfigurationExample.png[]

In the previous example, a process variable `variableWithTheSkills` is linked with the user task input name `skills`, and the corresponding value is processed using the `DefaultTaskAttributesProcessor` to calculate the `SKILLS` attribute in the internal model, which is managed by {PRODUCT_OP}.

[id="con-task-attributes-processors_{context}"]
==== Task attributes processors

The task attributes processors generate task attributes in the internal model and execute according to the following procedure:

. A user task is created or modified in the {PRODUCT} services, and the Task Assigning Service detects the change.

. All the configured `TaskAttributesProcessors` are executed for the same task.

. The `TaskAttributesProcessors` can transform the user task information into a task attribute in the internal model.

. The default constraints consider these attributes.

For example, the input data `skills` with the value `"skill1, skill2"` resulted in the attribute `SKILLS` with `{"skill1", "skill2"}` set of values.

.Task attributes processor
image::kogito/configuration/task-assigning-service/TaskAttributesProcessor.png[]

[id="con-user-attributes-processors_{context}"]
==== User attributes processors

The user attributes processors generate user attributes in the internal model and execute according to the following procedure:

. The information for a user is retrieved from the external users service, for example, when the Task Assigning Service starts or when the users are synchronized.

. All the configured `UserAttributesProcessors` processors are executed for the same user.

. The `UserAttributesProcessors` can transform the user information into a user attribute in the internal model.

. The default constraints consider these attributes.

For example, an external user attribute `affinities` with the value `"news"` resulted in the attribute `AFFINITIES` with the `{"news"}` values.

.User attributes processor
image::kogito/configuration/task-assigning-service/UserAttributesProcessor.png[]

[id="ref-default-attributes-processors_{context}"]
==== Default attributes processors

{PRODUCT} provides the following default attributes processors to manage the `SKILLS` and `AFFINITIES` attributes:

* `DefaultTaskAttributeProcessor`: Processes the user tasks `skills` and `affinities` input values as a string of comma-separated values and creates two Java objects set with tokenized string values for each attribute. The resulting set of values is assigned to the calculated attributes `SKILLS` and `AFFINITIES` respectively.
+
For example, the input value `"english, finance"` of `skills` is extracted as a set with the `{"english", "finance"}` values and assigned to the `SKILLS` attribute.
+
You can customize the `DefaultTaskAttributeProcessor` processor using the following configuration parameters:

** `kogito.task-assigning.default-task-attributes-processor.enabled` parameter to enable or disable the processing. The default value of this parameter is `true`.
** `kogito.task-assigning.default-task-attributes-processor.priority` parameter to configure the execution order for the `DefaultTaskAttributeProcessor` processor. The default value of this parameter is `0`. Lower numbers execute first.
** `kogito.task-assigning.default-task-attributes-processor.skills` parameter to change the name of the input value of the user task, from which the `SKILLS` attribute is calculated. The default value of this parameter is `skills`.
** `kogito.task-assigning.default-task-attributes-processor.affinities` parameter to change the name of the input value of the user task, form which the `AFFINITIES` attribute is calculated. The default value of this parameter is `affinities`.

* `DefaultUserAttributesProcessor`: Calculates the `SKILLS` and `AFFINITIES` attributes for a user. You can customize this processor using the following parameters:

** `kogito.task-assigning.default-user-attributes-processor.enabled` parameter is used to enable or disable the processing. The default value of this parameter is `true`.
** `kogito.task-assigning.default-user-attributes-processor.priority` parameter configures the execution order for the `DefaultUserAttributesProcessor` processor. The default value of this parameter is `0`. Lower numbers execute first.
** `kogito.task-assigning.default-user-attributes-processor.skills` parameter to change the name of the external user attribute, from which the `SKILLS` attribute is calculated. The default value of this parameter is `skills`.
** `kogito.task-assigning.default-user-attributes-processor.affinities` parameter to change the name of the external user attribute, from which the `AFFINITIES` attribute is calculated. The default value of this parameter is `affinities`.

[id="con-task-assigning-service-health-checks_{context}"]
=== Task Assigning Service health checks

The Task Assigning Service implements the https://github.com/eclipse/microprofile-health[MicroProfile Health] compatible readiness and liveness health checks. For more information about adding health checks to your Quarkus application, see https://quarkus.io/guides/smallrye-health[Quarkus SmallRye Health].

Readiness::

The following example shows the results of executing the readiness health check for the Task Assigning Service URL `http://localhost:8280/q/health/ready`:

.Example readiness health check execution
[source,json]
----
{
  "status": "UP",
  "checks": [
    {
      "name": "Task Assigning Service - readiness check",
      "status": "UP",
      "data": {
        "service-status": "READY"
      }
    },
    {
      "name": "SmallRye Reactive Messaging - readiness check",
      "status": "UP",
      "data": {
        "kogito-usertaskinstances-events": "[OK]"
      }
    }
  ]
}
----

Liveness::

The following example shows the results of executing the liveness health check for the Task Assigning Service URL `http://localhost:8280/q/health/live`:

.Example liveness health check execution
[source, json]
----
{
  "status": "UP",
  "checks": [
    {
      "name": "SmallRye Reactive Messaging - liveness check",
      "status": "UP",
      "data": {
        "kogito-usertaskinstances-events": "[OK]"
      }
    },
    {
      "name": "Task Assigning Service - liveness check",
      "status": "UP",
      "data": {
        "service-status": "READY"
      }
    }
  ]
}
----

[id="con-extensions-task-assigning_{context}"]
=== Task Assigning Service extensions in {PRODUCT}

The Task Assigning Service can be extended by using any of the following alternatives:

* Use a customized xref:proc-custom-solver-constraints-config_kogito-configuring[].
* Add user provided xref:con-user-service-connector_kogito-configuring[].
* Add user provided xref:proc-create-custom-task-attributes-processors_kogito-configuring[] and xref:proc-create-custom-user-attributes-processors_kogito-configuring[].

==== Using Task Assigning Service extensions

To extend the Task Assigning Service, you can use the following procedure:

.Procedure

. Create a Quarkus application and add the following dependencies to include the Task Assigning Service:
+
.Dependencies to include the Task Assigning Service
[source, xml]
----
<?xml version="1.0"?>
<project xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns="http://maven.apache.org/POM/4.0.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <modelVersion>4.0.0</modelVersion>
  <groupId>org.kie.kogito</groupId>
  <artifactId>task-assigning-service-extension-example</artifactId>
  <version>1.0.0-SNAPSHOT</version>

  <properties>
    <!-- any other Quarkus application properties  -->

    <!-- Kogito version corresponding to the task assigning service base to extend -->
    <kogito.version>KOGITO-VERSION-OF-YOUR-CHOICE</kogito.version>

    <!-- Quarkus version required by the task assigning service -->
    <quarkus.platform.version>REQUIRED-QUARKUS-VERSION</quarkus.platform.version>

    <!--
      If the kogito.version is aligned with the version of the kogito components
      delivered in the quarkus.platform.version the quarkus-universe-bom can be
      used, otherwise use the quarkus-bom.
    -->
    <!--<quarkus.platform.artifact-id>quarkus-universe-bom</quarkus.platform.artifact-id>-->
    <quarkus.platform.artifact-id>quarkus-bom</quarkus.platform.artifact-id>
    <quarkus.platform.group-id>io.quarkus</quarkus.platform.group-id>

  </properties>

  <dependencyManagement>
    <dependencies>
      <dependency>
        <groupId>${quarkus.platform.group-id}</groupId>
        <artifactId>${quarkus.platform.artifact-id}</artifactId>
        <version>${quarkus.platform.version}</version>
        <type>pom</type>
        <scope>import</scope>
      </dependency>
    </dependencies>
  </dependencyManagement>

  <dependencies>

    <!--
      The assigning service integration api defines the contracts for implementing user
      customized TaskAttributesProcessor, UserAttributesProcessor and UserServiceConnectors
    -->
    <dependency>
      <groupId>org.kie.kogito</groupId>
      <artifactId>task-assigning-integration-api</artifactId>
      <version>${kogito.version}</version>
    </dependency>

    <!-- Incorporates the task assigning service -->
    <dependency>
      <groupId>org.kie.kogito</groupId>
      <artifactId>task-assigning-service</artifactId>
      <version>${kogito.version}</version>
    </dependency>

    <!-- any other required dependency -->

  </dependencies>

   ...

</project>
----
+
For more information about how to create a Quarkus application, see https://quarkus.io/get-started/[Quarkus - Get Started].

. Add the extension components.
+
For more information about extension components, see xref:proc-custom-solver-constraints-config_kogito-configuring[], xref:con-user-service-connector_kogito-configuring[], xref:proc-create-custom-task-attributes-processors_kogito-configuring[], and xref:proc-create-custom-user-attributes-processors_kogito-configuring[].

. Build and start your Quarkus application.

[NOTE]
====
You can add the extensions as part of the existing application or other Maven modules, in which the modules must be added as dependencies.
====

[id="proc-custom-solver-constraints-config_{context}"]
==== Configuring custom solver and constraints

You can configure the custom solver and constraints in the Task Assigning Service.

.Procedure

. Create a solver configuration file, such as `myExampleSolverConfig.xml` in the project directory.
+
.Solver configuration file
[source,java]
----
src/main/resources/META-INF/myExampleSolverConfig.xml
----

. Create a customized `org.optaplanner.core.api.score.stream.ConstraintProvider` implementation, such as `org.kie.kogito.solver.ExtendedConstraintProvider` and configure the solver as follows:
+
.Solver configuration
[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<solver xmlns="https://www.optaplanner.org/xsd/solver"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xsi:schemaLocation="https://www.optaplanner.org/xsd/solver https://www.optaplanner.org/xsd/solver/solver.xsd">

  <!-- the Solver must execute in daemon mode -->
  <daemon>true</daemon>
  <moveThreadCount>AUTO</moveThreadCount>

  <!-- the following solution and entities configurations must be used -->
  <solutionClass>org.kie.kogito.taskassigning.core.model.TaskAssigningSolution</solutionClass>
  <entityClass>org.kie.kogito.taskassigning.core.model.ChainElement</entityClass>
  <entityClass>org.kie.kogito.taskassigning.core.model.TaskAssignment</entityClass>

  <scoreDirectorFactory>
    <!-- configure the customized ConstraintProvider -->
    <constraintProviderClass>org.kie.kogito.solver.ExtendedConstraintProvider</constraintProviderClass>
  </scoreDirectorFactory>

  <constructionHeuristic>
    <changeMoveSelector>
      <!-- in general it's recommended to keep this ChangeMoveFilter -->
      <filterClass>org.kie.kogito.taskassigning.core.model.solver.filter.TaskByGroupAndSkillsChangeMoveFilter</filterClass>
    </changeMoveSelector>
  </constructionHeuristic>

  <localSearch>
    <termination>
      <!-- NOTE: please keep this termination period inside the localSearch configuration. -->
      <unimprovedSpentLimit>PT2M</unimprovedSpentLimit>
    </termination>
  </localSearch>
</solver>
----

. To use the solver configuration, set the following parameter in your application `src/resources/META-INF/application.properties` file:
+
.Example parameter setting in the application file
[source]
----
quarkus.optaplanner.solver-config-xml=myExampleSolverConfig.xml
----

For more information on writing OptaPlanner solver configurations and `ConstraintProviders`, see https://docs.optaplanner.org/latestFinal/optaplanner-docs/html_single/index.html[{PRODUCT_OP} documentation].

For information about which constraints are mandatory for the Task Assigning Service, see xref:ref-default-constraints_kogito-configuring[].

[id="proc-create-custom-user-service-connector_{context}"]
==== Creating custom User Service Connector

You can create a custom User Service Connector for the Task Assigning Service.

.Procedure

. To create your custom User Service Connector, implement the `org.kie.kogito.taskassigning.user.service.UserServiceConnector` interface as follows:
+
.User Service Connector interface implementation
[source,java]
----
/**
* Give a name to the UserServiceConnector implementation by using the
* @UserServiceConnectorQualifier annotation.
* This name is used for configuring the kogito.task-assigning.user-service-connector
* parameter to use this connector.
*
* note: while it is a recommended practice the selected name does not necessary
* have to be the same as the class name.
*/
@ApplicationScoped
@UserServiceConnectorQualifier("MyExampleUserServiceDBConnector")
public class MyExampleUserServiceDBConnector implements UserServiceConnector {

    /**
    * Invoked by the task assigning service as part of the initialization procedure and
    * before any other method is invoked.
    */
    @Override
    public void start() {
        // execute any required initialization procedure
    }

    /**
    * @return the list of all users present in the external user service.
    */
    @Override
    public List<User> findAllUsers() {
        // query the DB and return the list of users
        return Collections.emptyList();
    }

    /**
    * Get the user information corresponding to the user identified by the id.
    * @param userId a user identifier.
    * @return the user corresponding to the given identifier, null if no user was found.
    */
    @Override
    public User findUser(String userId) {
        // query the DB and return the user corresponding or null if not found.
        return null;
    }
}
----
+
[NOTE]
====
The implementations of the User Service Connector are loaded using CDI and configured with the name provided by the `UserServiceConnectorQualifier`. Ensure that your implementation contains the following annotations:
`@ApplicationScoped`
`@UserServiceConnectorQualifier("MyExampleUserServiceDBConnector")`
====

. Set the following parameter in your `application.properties` file:
+
.Parameter for `application.properties` file
[source]
----
kogito.task-assigning.user-service-connector=MyExampleUserServiceDBConnector
----

The defined User Service Connector is automatically loaded and available when your application starts.

[id="proc-create-custom-task-attributes-processors_{context}"]
==== Creating custom task attributes processors

The xref:con-task-attributes-processors_kogito-configuring[] are CDI loaded components, which are invoked when a new user task is created or an existing user task is changed. The task attributes processors define the calculated attributes in an internal task model, which is managed by {PRODUCT_OP}.

The task attributes processors are invoked in the following scenarios:

* When a new user task is created in the {PRODUCT} runtimes, and a corresponding new task is added to the data model, which is managed by {PRODUCT_OP}.
* When an existing user task is changed and the changes added to the data model are detected.

You can create a custom task attributes processor for the Task Assigning Service.

.Procedure

To create your custom task attributes processor, implement the `org.kie.kogito.taskassigning.model.processing.TaskAttributesProcessor` interface as follows:

.Task attribute processor implementation
[source,java]
----
@ApplicationScoped
public class MyExampleTaskAttributeProcessor implements TaskAttributesProcessor {

    /**
     * Indicates the priority of this processor when multiple task attributes processors
     * are applied, lower numbers executes first.
     */
    @Override
    public int getPriority() {
        return 50;
    }

    /**
     * Indicates if the processor is enabled. Disabled processors are not applied.
     */
    @Override
    public boolean isEnabled() {
        return true;
    }

    /**
     * Executed when a new human task is created in the Kogito runtimes, or changes
     * in the TaskInfo information for an existing human task are detected.
     * The targetAttributes will be assigned to the internal Task counterpart
     * managed by OptaPlanner.
     *
     * @param taskInfo Information about the Kogito runtimes human task that was created
     * or modified.
     * @param targetAttributes Attributes to assign to the Task counterpart
     * managed by OptaPlanner.
     */
    @Override
    public void process(TaskInfo taskInfo, Map<String, Object> targetAttributes) {
        // custom attribute calculated by using the TaskInfo information or any other procedure.
        Object myCustomAttributeValue = new Object();
        targetAttributes.put("myCustomAttribute", myCustomAttributeValue);
    }
}
----

The `org.kie.kogito.taskassigning.model.processing.TaskInfo` interface represents the user task information that is available to the task attributes processor:

.Example `TaskInfo` interface
[source,java]
----
public interface TaskInfo {

    String getTaskId();

    String getName();

    String getDescription();

    String getReferenceName();

    String getPriority();

    String getProcessInstanceId();

    String getProcessId();

    String getRootProcessInstanceId();

    String getRootProcessId();

    Set<String> getPotentialUsers();

    Set<String> getPotentialGroups();

    Set<String> getAdminUsers();

    Set<String> getAdminGroups();

    Set<String> getExcludedUsers();

    ZonedDateTime getStarted();

    Map<String, Object> getInputs();

    String getEndpoint();
}
----

The custom task attributes processor is loaded and available when your application starts.

[id="proc-create-custom-user-attributes-processors_{context}"]
==== Creating custom user attributes processors

The xref:con-user-attributes-processors_kogito-configuring[] are CDI loaded components that are invoked when user information is refreshed using an external user service. The user attributes processors set the calculated attributes in an internal user model, which is managed by {PRODUCT_OP}.

[NOTE]
====
You can configure the user information refresh interval using the `kogito.task-assigning.user-service-sync-interval` parameter.
====

You can create a custom user attributes processor for the Task Assigning Service.

.Procedure

To create your custom user attributes processor, implement the `org.kie.kogito.taskassigning.model.processing.UserAttributesProcessor` interface as follows:

.User attributes processor interface implementation
[source,java]
----
@ApplicationScoped
public class MyExampleUserAttributesProcessor implements UserAttributesProcessor {

    /**
     * Indicates the priority of this processor when multiple user attributes processors
     * are applied, lower numbers executes first.
     */
    @Override
    public int getPriority() {
        return 30;
    }

    /**
     * Indicates if the processor is enabled. Disabled processors are not applied.
     */
    @Override
    public boolean isEnabled() {
        return true;
    }

    /**
     * Executed when the user information is refreshed from the external user service.
     *
     * @param externalUser User instance returned by the external user service query.
     * @param targetAttributes Attributes to assign to the User counterpart
     * managed by OptaPlanner.
     */
    @Override
    public void process(User externalUser, Map<String, Object> targetAttributes) {
        // custom attribute calculated by using the User information or any other procedure.
        Object myCustomAttributeValue = new Object();
        targetAttributes.put("myCustomAttribute", myCustomAttributeValue);
    }
}
----

The `org.kie.kogito.taskassigning.user.service.User` interface represents the user information that is returned by the User Service Connector when the user information is refreshed from an external user service.

.Example `org.kie.kogito.taskassigning.user.service.User` interface
[source,java]
----
public interface User {

    String getId();

    Set<Group> getGroups();

    Map<String, Object> getAttributes();

}
----

The user attributes processors is automatically loaded and available when your application starts.


[role="_additional-resources"]
.Additional resources
* https://quarkus.io/guides/security[Security Architecture and Guides]
* https://quarkus.io/guides/security-openid-connect#configuring-using-the-application-properties-file[Configuring using the application.properties file]
* https://quarkus.io/guides/security-openid-connect-multitenancy[Using OpenID Connect multi-tenancy]

ifdef::KOGITO-ENT[]
[role="_additional-resources"]
== Additional resources
* {URL_CREATING_RUNNING}[_{CREATING_RUNNING}_]
* {URL_DEPLOYING_ON_OPENSHIFT}[_{DEPLOYING_ON_OPENSHIFT}_]
* {URL_DECISION_SERVICES}[_{DECISION_SERVICES}_]
* {URL_PROCESS_SERVICES}[_{PROCESS_SERVICES}_]
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
