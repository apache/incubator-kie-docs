[id='test-scenarios-create-proc']
= Creating and running a test scenario

You can create test scenarios in {CENTRAL} to test the functionality of business rule data before deployment. A basic test scenario must have at least the following data:

* Related data objects
* *GIVEN* facts
* *EXPECT* results

With this data, the test scenario can validate the expected and actual results for that rule instance based on the defined facts. You can also add a *CALL METHOD* and any available *globals* to a test scenario, but these scenario settings are optional.

.Procedure
. In {CENTRAL}, go to *Menu* -> *Design* -> *Projects* and click the project name.
. Click *Add Asset* -> *Test Scenario (Legacy)*.
. Enter an informative *Test Scenario* name and select the appropriate *Package*. The package that you specify must be the same package where the required rule assets have been assigned or will be assigned. You can import data objects from any package into the asset's designer.
. Click *Ok* to create the test scenario.
+
The new test scenario is now listed in the *Test Scenarios* panel of the *Project Explorer*,
+
. Click the *Data Objects* tab to verify that all data objects required for the rules that you want to test are listed. If not, click *New item* to import the needed data objects from other packages, or
ifeval::["{context}" == "test-scenarios"]
xref:data-objects-create-proc_test-scenarios[create data objects]
endif::[]
ifeval::["{context}" == "chap-test-scenarios"]
xref:data-objects-create-proc_chap-data-models[create data objects]
endif::[]
within your package.
. After all data objects are in place, return to the *Model* tab of the test scenarios designer and define the *GIVEN* and *EXPECT* data for the scenario, based on the available data objects.
+
.The test scenarios designer
image::project-data/test-scenario-edit.png[]
+
The *GIVEN* section defines the input facts for the test. For example, if an `Underage` rule in the project declines loan applications for applicants under the age of 21, then the *GIVEN* facts in the test scenario could be `Applicant` with `age` set to some integer less than 21.
+
The *EXPECT* section defines the expected results based on the *GIVEN* input facts. That is, *GIVEN* the input facts, *EXPECT* these other facts to be valid or entire rules to be activated. For example, with the given facts of an applicant under the age of 21 in the scenario, the *EXPECT* results could be `LoanApplication` with `approved` set to `false` (as a result of the underage applicant), or could be the activation of the `Underage` rule as a whole.
+
. Optionally, add a *CALL METHOD* and any *globals* to the test scenario:
+
--
* *CALL METHOD:* Use this to invoke a method from another fact when the rule execution is initiated. Click *CALL METHOD*, select a fact, and click image:project-data/6187.png[] to select the method to invoke. You can invoke any Java class methods (such as methods from an ArrayList) from the Java library or from a JAR that was imported for the project (if applicable).
* *globals:* Use this to add any global variables in the project that you want to validate in the test scenario. Click *globals* to select the variable to be validated, and then in the test scenarios designer, click the global name and define field values to be applied to the global variable. If no global variables are available, then they must be created as new assets in {CENTRAL}. Global variables are named objects that are visible to the {DECISION_ENGINE} but are different from the objects for facts. Changes in the object of a global do not trigger the re-evaluation of rules.
--
+
. Click *More* at the bottom of the test scenarios designer to add other data blocks to the same scenario file as needed.
. After you have defined all *GIVEN*, *EXPECT*, and other data for the scenario, click *Save* in the test scenarios designer to save your work.
. Click *Run scenario* in the upper-right corner to run this `.scenario` file, or click *Run all scenarios* to run all saved `.scenario` files in the project package (if there are multiple). Although the *Run scenario* option does not require the individual `.scenario` file to be saved, the *Run all scenarios* option does require all `.scenario` files to be saved.
+
If the test fails, address any problems described in the *Alerts* message at the bottom of the window, review all components in the scenario, and try again to validate the scenario until the scenario passes.
+
. Click *Save* in the test scenarios designer to save your work after all changes are complete.

For more details about adding GIVEN facts to test scenarios, see xref:test-scenarios-GIVEN-proc[].

For more details about adding EXPECT results to test scenarios, see xref:test-scenarios-EXPECT-proc[].
