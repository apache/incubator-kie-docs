
= Services

On top of RuntimeManager API a set of high level services has been provided from jBPM version 6.2.
These services are meant to be the easiest way to embed (j)BPM capabilities into custom application.
A complete set of modules are delivered as part of these services.
They are partitioned into several modules to ease thier adoptions in various environments.


* jbpm-services-api
+
contains only api classes and interfaces

* jbpm-kie-services
+
rewritten code implementation of services api - pure java, no framework dependencies

* jbpm-services-cdi
+
CDI wrapper on top of core services implementation

* jbpm-services-ejb-api 
+
extension to services api for ejb needs

* jbpm-services-ejb-impl 
+
EJB wrappers on top of core services implementation

* jbpm-services-ejb-timer
+
scheduler service based on EJB TimerService to support time based operations e.g.
timer events, deadlines, etc

* jbpm-services-ejb-client
+
EJB remote client implementation - currently only for JBoss
 Service modules are grouped with its framework dependencies, so developers are free to choose which one is suitable for them and use only that.



== Deployment Service

As the name suggest, its primary responsibility is to deploy (and undeploy) units.
Deployment unit is kjar that brings in business assets (like processes, rules, forms, data model) for execution.
Deployment services allow to query it to get hold of available deployment units and even their RuntimeManager instances.

[NOTE]
====
there are some restrictions on EJB remote client to do not expose RuntimeManager as it won't make any sense on client side (after it was serialized).
====

So typical use case for this service is to provide dynamic behavior into your system so multiple kjars can be active at the same time and be executed simultaneously.
[source,java]
----
// create deployment unit by giving GAV
DeploymentUnit deploymentUnit = new KModuleDeploymentUnit(GROUP_ID, ARTIFACT_ID, VERSION);
// deploy        
deploymentService.deploy(deploymentUnit);
// retrieve deployed unit        
DeployedUnit deployed = deploymentService.getDeployedUnit(deploymentUnit.getIdentifier());
// get runtime manager
RuntimeManager manager = deployed.getRuntimeManager();
----

Complete DeploymentService interface is as follows:
[source,java]
----
public interface DeploymentService {

    void deploy(DeploymentUnit unit);
    
    void undeploy(DeploymentUnit unit);
    
    RuntimeManager getRuntimeManager(String deploymentUnitId);
    
    DeployedUnit getDeployedUnit(String deploymentUnitId);
    
    Collection<DeployedUnit> getDeployedUnits();
    
    void activate(String deploymentId);
    
    void deactivate(String deploymentId);
    
    boolean isDeployed(String deploymentUnitId);
}
----

== Definition Service

Upon deployment, every process definition is scanned using definition service that parses the process and extracts valuable information out of it.
These information can provide valuable input to the system to inform users about what is expected.
Definition service provides information about:

* process definition - id, name, description
* process variables - name and type
* reusable subprocesses used in the process (if any)
* service tasks (domain specific activities)
* user tasks including assignment information
* task data input and output information

So definition service can be seen as sort of supporting service that provides quite a few information about process definition that are extracted directly from BPMN2.

[source,java]
----
String processId = "org.jbpm.writedocument";
         
Collection<UserTaskDefinition> processTasks = 
bpmn2Service.getTasksDefinitions(deploymentUnit.getIdentifier(), processId);
         
Map<String, String> processData = 
bpmn2Service.getProcessVariables(deploymentUnit.getIdentifier(), processId);
         
Map<String, String> taskInputMappings = 
bpmn2Service.getTaskInputMappings(deploymentUnit.getIdentifier(), processId, "Write a Document" );
----

While it usually is used with combination of other services (like deployment service) it can be used standalone as well to get details about process definition that do not come from kjar.
This can be achieved by using buildProcessDefinition method of definition service.

[source,java]
----
public interface DefinitionService {
	
    ProcessDefinition buildProcessDefinition(String deploymentId, String bpmn2Content,
			ClassLoader classLoader, boolean cache) throws IllegalArgumentException;

    ProcessDefinition getProcessDefinition(String deploymentId, String processId);
    
    Collection<String> getReusableSubProcesses(String deploymentId, String processId);
    
    Map<String, String> getProcessVariables(String deploymentId, String processId);
    
    Map<String, String> getServiceTasks(String deploymentId, String processId);
    
    Map<String, Collection<String>> getAssociatedEntities(String deploymentId, String processId);
    
    Collection<UserTaskDefinition> getTasksDefinitions(String deploymentId, String processId);
    
    Map<String, String> getTaskInputMappings(String deploymentId, String processId, String taskName);
    
    Map<String, String> getTaskOutputMappings(String deploymentId, String processId, String taskName);
	
}
----



== Process Service

Process service is the one that usually is of the most interest.
Once the deployment and definition service was already used to feed the system with something that can be executed.
Process service provides access to execution environment that allows:

* start new process instance
* work with existing one - signal, get details of it, get variables, etc
* work with work items

At the same time process service is a command executor so it allows to execute commands (essentially on ksession) to extend its capabilities. 

Important to note is that process service is focused on runtime operations so use it whenever there is a need to alter (signal, change variables, etc) process instance and not for read operations like show available process instances by looping though given list and invoking getProcessInstance method.
For that there is dedicated runtime data service that is described below.

An example on how to deploy and run process can be done as follows:

[source,java]
----
KModuleDeploymentUnit deploymentUnit = new KModuleDeploymentUnit(GROUP_ID, ARTIFACT_ID, VERSION);
         
deploymentService.deploy(deploymentUnit);
 
long processInstanceId = processService.startProcess(deploymentUnit.getIdentifier(), "customtask");
      
ProcessInstance pi = processService.getProcessInstance(processInstanceId);
----

As you can see start process expects deploymentId as first argument.
This is extremely powerful to enable service to easily work with various deployments, even with same processes but coming from different versions - kjar versions.
[source,java]
----
public interface ProcessService {
	
    Long startProcess(String deploymentId, String processId);

    Long startProcess(String deploymentId, String processId, Map<String, Object> params);

    void abortProcessInstance(Long processInstanceId);
    
    void abortProcessInstances(List<Long> processInstanceIds);

    void signalProcessInstance(Long processInstanceId, String signalName, Object event);
    
    void signalProcessInstances(List<Long> processInstanceIds, String signalName, Object event);
    
    ProcessInstance getProcessInstance(Long processInstanceId);

    void setProcessVariable(Long processInstanceId, String variableId, Object value);
    
    void setProcessVariables(Long processInstanceId, Map<String, Object> variables);
    
    Object getProcessInstanceVariable(Long processInstanceId, String variableName);

    Map<String, Object> getProcessInstanceVariables(Long processInstanceId);
    
    Collection<String> getAvailableSignals(Long processInstanceId);
    
    void completeWorkItem(Long id, Map<String, Object> results);

    void abortWorkItem(Long id);
    
    WorkItem getWorkItem(Long id);

    List<WorkItem> getWorkItemByProcessInstance(Long processInstanceId);
    
    public <T> T execute(String deploymentId, Command<T> command);
    
    public <T> T execute(String deploymentId, Context<?> context, Command<T> command);

}
----



== Runtime Data Service

Runtime data service as name suggests, deals with all that refers to runtime information:

* started process instances
* executed node instances
* executed node instances
* and more

Use this service as main source of information whenever building list based UI - to show process definitions, process instances, tasks for given user, etc.
This service was designed to be as efficient as possible and still provide all required information.

Some examples:

* get all process definitions
+

[source,java]
----
Collection definitions = runtimeDataService.getProcesses(new QueryContext());
----

* get active process instances 
+
[source,java]
----
Collection<processinstancedesc> instances = runtimeDataService.getProcessInstances(new QueryContext());
----

* get active nodes for given process instance
+
[source,java]
----
Collection<nodeinstancedesc> instances = runtimeDataService.getProcessInstanceHistoryActive(processInstanceId, new QueryContext());
----

* get tasks assigned to john
+
[source,java]
----
List<tasksummary> taskSummaries = runtimeDataService.getTasksAssignedAsPotentialOwner("john", new QueryFilter(0, 10));
----

There are two important arguments that the runtime data service operations supports:

* QueryContext
* QueryFilter - extension of QueryContext

These provide capabilities for efficient management result set like pagination, sorting and ordering (QueryContext). Moreover additional filtering can be applied to task queries to provide more advanced capabilities when searching for user tasks.

[source,java]
----
public interface RuntimeDataService {
  
    // Process instance information
    
    Collection<ProcessInstanceDesc> getProcessInstances(QueryContext queryContext);
   
    Collection<ProcessInstanceDesc> getProcessInstances(List<Integer> states, String initiator, QueryContext queryContext);
   
    Collection<ProcessInstanceDesc> getProcessInstancesByProcessId(List<Integer> states, String processId, String initiator, QueryContext queryContext);
   
    Collection<ProcessInstanceDesc> getProcessInstancesByProcessName(List<Integer> states, String processName, String initiator, QueryContext queryContext);
    
    Collection<ProcessInstanceDesc> getProcessInstancesByDeploymentId(String deploymentId, List<Integer> states, QueryContext queryContext);
    
    ProcessInstanceDesc getProcessInstanceById(long processInstanceId);
    
    Collection<ProcessInstanceDesc> getProcessInstancesByProcessDefinition(String processDefId, QueryContext queryContext);
    
    Collection<ProcessInstanceDesc> getProcessInstancesByProcessDefinition(String processDefId, List<Integer> states, QueryContext queryContext);

    
    // Node and Variable instance information
   
    NodeInstanceDesc getNodeInstanceForWorkItem(Long workItemId);

    Collection<NodeInstanceDesc> getProcessInstanceHistoryActive(long processInstanceId, QueryContext queryContext);

    Collection<NodeInstanceDesc> getProcessInstanceHistoryCompleted(long processInstanceId, QueryContext queryContext);

    Collection<NodeInstanceDesc> getProcessInstanceFullHistory(long processInstanceId, QueryContext queryContext);
    
    Collection<NodeInstanceDesc> getProcessInstanceFullHistoryByType(long processInstanceId, EntryType type, QueryContext queryContext);

    Collection<VariableDesc> getVariablesCurrentState(long processInstanceId);

    Collection<VariableDesc> getVariableHistory(long processInstanceId, String variableId, QueryContext queryContext);

    
    // Process information
  
    Collection<ProcessDefinition> getProcessesByDeploymentId(String deploymentId, QueryContext queryContext);   
    
    Collection<ProcessDefinition> getProcessesByFilter(String filter, QueryContext queryContext);

    Collection<ProcessDefinition> getProcesses(QueryContext queryContext);
   
    Collection<String> getProcessIds(String deploymentId, QueryContext queryContext);
   
    ProcessDefinition getProcessById(String processId);
  
    ProcessDefinition getProcessesByDeploymentIdProcessId(String deploymentId, String processId);
    
	// user task query operations

    UserTaskInstanceDesc getTaskByWorkItemId(Long workItemId);

    UserTaskInstanceDesc getTaskById(Long taskId);

    List<TaskSummary> getTasksAssignedAsBusinessAdministrator(String userId, QueryFilter filter);
	
    List<TaskSummary> getTasksAssignedAsBusinessAdministratorByStatus(String userId, List<Status> statuses, QueryFilter filter);

    List<TaskSummary> getTasksAssignedAsPotentialOwner(String userId, QueryFilter filter);
	
    List<TaskSummary> getTasksAssignedAsPotentialOwner(String userId, List<String> groupIds, QueryFilter filter);

    List<TaskSummary> getTasksAssignedAsPotentialOwnerByStatus(String userId, List<Status> status, QueryFilter filter);
	
    List<TaskSummary> getTasksAssignedAsPotentialOwner(String userId, List<String> groupIds, List<Status> status, QueryFilter filter);
	
    List<TaskSummary> getTasksAssignedAsPotentialOwnerByExpirationDateOptional(String userId, List<Status> status, Date from, QueryFilter filter);
	
    List<TaskSummary> getTasksOwnedByExpirationDateOptional(String userId, List<Status> strStatuses, Date from, QueryFilter filter);

    List<TaskSummary> getTasksOwned(String userId, QueryFilter filter);

    List<TaskSummary> getTasksOwnedByStatus(String userId, List<Status> status, QueryFilter filter);

    List<Long> getTasksByProcessInstanceId(Long processInstanceId);

    List<TaskSummary> getTasksByStatusByProcessInstanceId(Long processInstanceId, List<Status> status, QueryFilter filter);
        
    List<AuditTask> getAllAuditTask(String userId, QueryFilter filter);
	    
}
----



== User Task Service

User task service covers complete life cycle of individual task so it can be managed from start to end.
It explicitly eliminates queries from it to provide scoped execution and moves all query operations into runtime data service.
Besides lifecycle operations user task service allows:

* modification of selected properties
* access to task variables
* access to task attachments
* access to task comments

On top of that user task service is a command executor as well that allows to execute custom task commands.

Complete example with start process and complete user task done by services:
[source,java]
----
long processInstanceId = 
processService.startProcess(deployUnit.getIdentifier(), "org.jbpm.writedocument");
 
List<Long> taskIds = 
runtimeDataService.getTasksByProcessInstanceId(processInstanceId);
 
Long taskId = taskIds.get(0);
      
userTaskService.start(taskId, "john");
UserTaskInstanceDesc task = runtimeDataService.getTaskById(taskId);
      
Map<String, Object> results = new HashMap<String, Object>();
results.put("Result", "some document data");
userTaskService.complete(taskId, "john", results);
----



[NOTE]
====
The most important thing when working with services is that there is no more need to create your own implementations of Process service that simply wraps runtime manager, runtime engine, ksession usage.
Services make use of RuntimeManager API best practices and thus eliminate various risks when working with that API.
====

== Quartz-based Timer Service

jBPM provides a cluster-ready timer service via Quartz, allowing you to dispose or load your knowledge session at any time.  In order to fire each timer appropriately, this service can be utilized to manage how long a kie session should be active.  

A base Quartz configuration file in the case of a clustered environment is provided as an example below:

[source,xml]
----
#============================================================================
# Configure Main Scheduler Properties  
#============================================================================

org.quartz.scheduler.instanceName = jBPMClusteredScheduler
org.quartz.scheduler.instanceId = AUTO

#============================================================================
# Configure ThreadPool  
#============================================================================

org.quartz.threadPool.class = org.quartz.simpl.SimpleThreadPool
org.quartz.threadPool.threadCount = 5
org.quartz.threadPool.threadPriority = 5

#============================================================================
# Configure JobStore  
#============================================================================

org.quartz.jobStore.misfireThreshold = 60000

org.quartz.jobStore.class=org.quartz.impl.jdbcjobstore.JobStoreCMT
org.quartz.jobStore.driverDelegateClass=org.quartz.impl.jdbcjobstore.StdJDBCDelegate
org.quartz.jobStore.useProperties=false
org.quartz.jobStore.dataSource=managedDS
org.quartz.jobStore.nonManagedTXDataSource=nonManagedDS
org.quartz.jobStore.tablePrefix=QRTZ_
org.quartz.jobStore.isClustered=true
org.quartz.jobStore.clusterCheckinInterval = 20000

#============================================================================
# TODO: Configure Datasources
#============================================================================
#org.quartz.dataSource.managedDS.jndiURL=
#org.quartz.dataSource.nonManagedDS.jndiURL=
----

For more information on configuring a Quartz scheduler, please see the documentation for the 1.8.5 distribution archive.


== QueryService

QueryService provides advanced search capabilities that are based on Dashbuilder DataSets.
The concept behind it is that users are given control over how to retrieve data from underlying data store.
This includes complex joins with external tables such as JPA entities tables, custom systems data base tables etc.

QueryService is build around two parts:

* Management operations
+

*** register query definition
*** replace query definition
*** unregister (remove) query definition
*** get query definition
*** get all registered query definitions
* Runtime operations
+

*** query - with two flavors
+

**** simple based on QueryParam as filter provider
**** advanced based on QueryParamBuilder as filter provider

DashBuilder DataSets provide support for multiple data sources (CSV, SQL, elastic search, etc) while jBPM - since its backend is RDBMS based - focuses on SQL based data sets.
So jBPM QueryService is a subset of DashBuilder DataSets capabilities to allow efficient queries with simple API.

*Terminology*



* QueryDefinition - represents definion of the data set which consists of unique name, sql expression (the query) and source - JNDI name of the data source to use when performing queries
* QueryParam - basic structure that represents individual query parameter - condition - that consists of: column name, operator, expected value(s)
* QueryResultMapper - responsible for mapping raw data set data (rows and columns) into object representation
* QueryParamBuilder - responsible for building query filters that will be applied on the query definition for given query invocation 

While QueryDefinition and QueryParam is rather straight forward, QueryParamBuilder and QueryResultMapper is bit more advanced and require slightly more attention to make use of it in right way, and by that take advantage of their capabilities. 



*QueryResultMapper*

Mapper as the name suggest, maps data taken out from data base (from data set) into object representation.
Much like ORM providers such as hibernate maps tables to entities.
Obviously there might be many object types that could be used for representing data set results so it's almost impossible to provide them out of the box.
Mappers are rather powerful and thus are pluggable, you can implement your own that will transform the result into whatever type you like.
jBPM comes with following mappers out of the box:

* org.jbpm.kie.services.impl.query.mapper.ProcessInstanceQueryMapper
+

*** registered with name - ProcessInstances
* org.jbpm.kie.services.impl.query.mapper.ProcessInstanceWithVarsQueryMapper
+

*** registered with name - ProcessInstancesWithVariables
* org.jbpm.kie.services.impl.query.mapper.ProcessInstanceWithCustomVarsQueryMapper
+

*** registered with name - ProcessInstancesWithCustomVariables
* org.jbpm.kie.services.impl.query.mapper.UserTaskInstanceQueryMapper
+

*** registered with name - UserTasks
* org.jbpm.kie.services.impl.query.mapper.UserTaskInstanceWithVarsQueryMapper
+

*** registered with name - UserTasksWithVariables
* org.jbpm.kie.services.impl.query.mapper.UserTaskInstanceWithCustomVarsQueryMapper
+

*** registered with name - UserTasksWithCustomVariables
* org.jbpm.kie.services.impl.query.mapper.TaskSummaryQueryMapper
+

*** registered with name - TaskSummaries
* org.jbpm.kie.services.impl.query.mapper.RawListQueryMapper
+

*** registered with name - RawList

Each QueryResultMapper is registered under given name to allow simple look up by name instead of referencing its class name - especially important when using EJB remote flavor of services where we want to reduce number of dependencies and thus not relying on implementation on client side.
So to be able to reference QueryResultMapper by name, NamedQueryMapper should be used which is part of jbpm-services-api.
That acts as delegate (lazy delegate) as it will look up the actual mapper when the query is actually performed.

[source,java]
----
queryService.query("my query def", new NamedQueryMapper<Collection<ProcessInstanceDesc>>("ProcessInstances"), new QueryContext());
----

*QueryParamBuilder*

QueryParamBuilder that provides more advanced way of building filters for our data sets.
By default when using query method of QueryService that accepts zero or more QueryParam instances (as we have seen in above examples) all of these params will be joined with AND operator meaning all of them must match.
But that's not always the case so that's why QueryParamBuilder has been introduced for users to build their on builders which will provide filters at the time the query is issued.

There is one QueryParamBuilder available out of the box and it is used to cover default QueryParams that are based on so called core functions.
These core functions are SQL based conditions and includes following

* IS_NULL
* NOT_NULL
* EQUALS_TO
* NOT_EQUALS_TO
* LIKE_TO
* GREATER_THAN
* GREATER_OR_EQUALS_TO
* LOWER_THAN
* LOWER_OR_EQUALS_TO
* BETWEEN
* IN
* NOT_IN

QueryParamBuilder is simple interface that is invoked as long as its build method returns non null value before query is performed.
So you can build up a complex filter options that could not be simply expressed by list of QueryParams.
Here is basic implementation of QueryParamBuilder to give you a jump start to implement your own - note that it relies on DashBuilder Dataset API.

[source,java]
----
public class TestQueryParamBuilder implements QueryParamBuilder<ColumnFilter> {
 
    private Map<String, Object> parameters;
    private boolean built = false;
    public TestQueryParamBuilder(Map<String, Object> parameters) {
        this.parameters = parameters;
    }
     
    @Override
    public ColumnFilter build() {
        // return null if it was already invoked
        if (built) {
            return null;
        }
         
        String columnName = "processInstanceId";
         
        ColumnFilter filter = FilterFactory.OR(
                FilterFactory.greaterOrEqualsTo((Long)parameters.get("min")),
                FilterFactory.lowerOrEqualsTo((Long)parameters.get("max")));
        filter.setColumnId(columnName);
        
        built = true;
        return filter;
    }
 
}
----

Once you have query param builder implemented you simply use its instance when performing query via QueryService

[source,java]
----
queryService.query("my query def", ProcessInstanceQueryMapper.get(), new QueryContext(), paramBuilder);
----

*Typical usage scenario*

First thing user needs to do is to define data set - view of the data you want to work with - so called QueryDefinition in services api. 

[source,java]
----
SqlQueryDefinition query = new SqlQueryDefinition("getAllProcessInstances", "java:jboss/datasources/ExampleDS");
query.setExpression("select * from processinstancelog");
----

This is the simplest possible query definition as it can be:

* constructor takes
+

*** a unique name that identifies it on runtime
*** data source JNDI name used when performing queries on this definition - in other words source of data
* expression - the most important part - is the sql statement that builds up the view to be filtered when performing queries

Once we have the sql query definition we can register it so it can be used later for actual queries.


[source,java]
----
queryService.registerQuery(query);
----

From now on, this query definition can be used to perform actual queries (or data look ups to use terminology from data sets). Following is the basic one that collects data as is, without any filtering


[source,java]
----
Collection<ProcessInstanceDesc> instances = queryService.query("getAllProcessInstances", ProcessInstanceQueryMapper.get(), new QueryContext());
----

Above query was very simple and used defaults from QueryContext - paging and sorting.
So let's take a look at one that changes the defaults of the paging and sorting


[source,java]
----
QueryContext ctx = new QueryContext(0, 100, "start_date", true);
         
Collection<ProcessInstanceDesc> instances = queryService.query("getAllProcessInstances", ProcessInstanceQueryMapper.get(), ctx);
----

Now let's take a look at how to do data filtering 

[source,java]
----
// single filter param
Collection<ProcessInstanceDesc> instances = queryService.query("getAllProcessInstances", ProcessInstanceQueryMapper.get(), new QueryContext(), QueryParam.likeTo(COLUMN_PROCESSID, true, "org.jbpm%"));
 
// multiple filter params (AND)
Collection<ProcessInstanceDesc> instances = queryService.query("getAllProcessInstances", ProcessInstanceQueryMapper.get(), new QueryContext(),
 QueryParam.likeTo(COLUMN_PROCESSID, true, "org.jbpm%"),
 QueryParam.in(COLUMN_STATUS, 1, 3));
----

With that end user is put in driver seat to define what data and how they should be fetched.
Not being limited by JPA provider nor anything else.
Moreover this promotes use of tailored queries for your environment as in most of the case there will be single data base used and thus specific features of that data base can be used to increase performance.

Further examples can be found http://mswiderski.blogspot.com/2016/01/advanced-queries-in-jbpm-64.html[here].

== ProcessInstanceMigrationService

ProcessInstanceMigrationService provides administrative utility to move given process instance(s) from one deployment to another or one process definition to another. It's main responsibility is to allow basic upgrade of process definition behind given process instance. That might include mapping of currently active nodes to other nodes in new definition.

Migration does not deal with process or task variables, they are not affected by migration. Essentially process instance migration means a change of underlying process definition process engine uses to move on with process instance.

Even though process instance migration is available it's recommended to let active process instances finish and then start new instances with new version whenever possible. In case that approach can't be used, migration of active process instance needs to be carefully planned before its execution as it might lead to unexpected issues.Most important to take into account is:


* is new process definition backward compatible?
* are there any data changes (variables that could affect process instance decisions after migration)?
* is there need for node mapping?

Answers to these question might save a lot of headache and production problems after migration. Best is to always stick with backward compatible processes - like extending process definition rather than removing nodes. Though that's not always possible and in some cases there is a need to remove certain nodes from process definition. In that situation, migration needs to be instructed how to map nodes that were removed in new definition in case active process instance is at the moment in such a node.


Node mapping is given as a map of node ids (UniqueIds that are set in the definition) where key is the source node id (from process definition used by process instance) to target node id (in new process definition).

[NOTE]

Node mapping can only be used to map same type of nodes e.g. user task to user task.

Again, process or task variables are not affected by process instance migration at the moment.

ProcessInstanceMigrationService comes with several flavors of migrate operation:

[source,java]

ProcessInstanceMigrationService
----
public interface ProcessInstanceMigrationService {
 /**
 * Migrates given process instance that belongs to source deployment, into target process id that belongs to target deployment.
 * Following rules are enforced:
 * <ul>
 * <li>source deployment id must be there</li>
 * <li>process instance id must point to existing and active process instance</li>
 * <li>target deployment must exist</li>
 * <li>target process id must exist in target deployment</li>
 * </ul>
 * Migration returns migration report regardless of migration being successful or not that needs to be examined for migration outcome.
 * @param sourceDeploymentId deployment that process instance to be migrated belongs to
 * @param processInstanceId id of the process instance to be migrated
 * @param targetDeploymentId id of deployment that target process belongs to
 * @param targetProcessId id of the process process instance should be migrated to
 * @return returns complete migration report
 */
 MigrationReport migrate(String sourceDeploymentId, Long processInstanceId, String targetDeploymentId, String targetProcessId);
 /**
 * Migrates given process instance (with node mapping) that belongs to source deployment, into target process id that belongs to target deployment.
 * Following rules are enforced:
 * <ul>
 * <li>source deployment id must be there</li>
 * <li>process instance id must point to existing and active process instance</li>
 * <li>target deployment must exist</li>
 * <li>target process id must exist in target deployment</li>
 * </ul>
 * Migration returns migration report regardless of migration being successful or not that needs to be examined for migration outcome.
 * @param sourceDeploymentId deployment that process instance to be migrated belongs to
 * @param processInstanceId id of the process instance to be migrated
 * @param targetDeploymentId id of deployment that target process belongs to
 * @param targetProcessId id of the process process instance should be migrated to
 * @param nodeMapping node mapping - source and target unique ids of nodes to be mapped - from process instance active nodes to new process nodes
 * @return returns complete migration report
 */
 MigrationReport migrate(String sourceDeploymentId, Long processInstanceId, String targetDeploymentId, String targetProcessId, Map<String, String> nodeMapping);
 /**
 * Migrates given process instances that belong to source deployment, into target process id that belongs to target deployment.
 * Following rules are enforced:
 * <ul>
 * <li>source deployment id must be there</li>
 * <li>process instance id must point to existing and active process instance</li>
 * <li>target deployment must exist</li>
 * <li>target process id must exist in target deployment</li>
 * </ul>
 * Migration returns list of migration report - one per process instance, regardless of migration being successful or not that needs to be examined for migration outcome.
 * @param sourceDeploymentId deployment that process instance to be migrated belongs to
 * @param processInstanceIds list of process instance id to be migrated
 * @param targetDeploymentId id of deployment that target process belongs to
 * @param targetProcessId id of the process process instance should be migrated to
 * @return returns complete migration report
 */
 List<MigrationReport> migrate(String sourceDeploymentId, List<Long> processInstanceIds, String targetDeploymentId, String targetProcessId);
 /**
 * Migrates given process instances (with node mapping) that belong to source deployment, into target process id that belongs to target deployment.
 * Following rules are enforced:
 * <ul>
 * <li>source deployment id must be there</li>
 * <li>process instance id must point to existing and active process instance</li>
 * <li>target deployment must exist</li>
 * <li>target process id must exist in target deployment</li>
 * </ul>
 * Migration returns list of migration report - one per process instance, regardless of migration being successful or not that needs to be examined for migration outcome.
 * @param sourceDeploymentId deployment that process instance to be migrated belongs to
 * @param processInstanceIds list of process instance id to be migrated
 * @param targetDeploymentId id of deployment that target process belongs to
 * @param targetProcessId id of the process process instance should be migrated to
 * @param nodeMapping node mapping - source and target unique ids of nodes to be mapped - from process instance active nodes to new process nodes
 * @return returns list of migration reports one per each process instance
 */
 List<MigrationReport> migrate(String sourceDeploymentId, List<Long> processInstanceIds, String targetDeploymentId, String targetProcessId, Map<String, String> nodeMapping);
}
----

Migration can either be performed for single process instance or multiple process instances at the same time. Multiple process instances migration is a utility method on top of single instance, instead of calling it multiple times, users call it once and then service will take care of the migration of individual process instances.

[NOTE]

Multi instance migration does migrate each instance in separation (transaction) to secure that one won't affect the other and then produces dedicated migration reports for each process instance


=== Migration report

Migration is always comcluded with migration report that is per each process instance. That migration report provides following information:

* start and end date of the migration

* outcome of the migration - success or failure

* complete log entry - all steps performed during migration, entry can be INFO, WARN or ERROR - in case of ERROR there will be at most one as they are causing migration to be immedietely terminated.

=== Example

Following is an example of how to invoke the migration


[source, java]

 protected static final String MIGRATION_ARTIFACT_ID = "test-migration";
 protected static final String MIGRATION_GROUP_ID = "org.jbpm.test";
 protected static final String MIGRATION_VERSION_V1 = "1.0.0";
 protected static final String MIGRATION_VERSION_V2 = "2.0.0";
 // first deploy both versions
 deploymentUnitV1 = new KModuleDeploymentUnit(MIGRATION_GROUP_ID, MIGRATION_ARTIFACT_ID, MIGRATION_VERSION_V1);
 deploymentService.deploy(deploymentUnitV1);
 // ... version 2
 deploymentUnitV2 = new KModuleDeploymentUnit(MIGRATION_GROUP_ID, MIGRATION_ARTIFACT_ID, MIGRATION_VERSION_V2);
 deploymentService.deploy(deploymentUnitV2);
 // next start process instance in version 1
 long processInstanceId = processService.startProcess(deploymentUnitV1.getIdentifier(), "processID-V1");
// and once the instance is active it can be migrated
MigrationReport report = migrationService.migrate(deploymentUnitV1.getIdentifier(), processInstanceId, deploymentUnitV2.getIdentifier(), "processID-V2");
// as last step check if the migration finished successfully
report.isSuccessful()


=== Known limitations


* When a new or modified task requires inputs which are not available in the migrated v2 process instance.

* Modifying the tasks prior to the active task where the changes have an impact on the further processing.

* Removing a human task which is currently active (can only be replaced - requires to be mapped to another human task)

* Adding a new task parallel to the single active task (all branches in AND gateway are not activated - process will stuck)

* Changing or removing the active recurring timer events (won’t be changed in DB)

* Fixing or updating inputs and outputs in an active task (task data aren’t migrated)

* Node mapping updates only the task node name and description! (other task fields won’t be mapped including the TaskName variable)


== Working with deployments

Deployment Service provides convinient way to put business assets to an execution environment but there are cases that requires some additional management to make them available in right context. 

*Activation and Deactivation of deployments*

Imagine situation where there are number of processes already running of given deployment and then new version of these processes comes into the runtime environment.
With that administrator can decide that new instances of given process definition should be using new version only while already active instances should continue with the previous version. 

To help with that deployment service has been equipped with following methods:

* activate
+
allows to activate given deployment so it can be available for interaction meaning will show its process definition and allow to start new process instances of that project's processes

* deactivate
+
allows to deactivate deployment which will disable option to see or start new process instances of that project's processes but will allow to continue working with already active process instances, e.g.
signal, work with user task etc


This feature allows smooth transition between project versions whitout need of process instance migration.

*Deployment synchronization*

Prior to jBPM 6.2, jbpm services did not have deployment store by default.
When embedded in jbpm-console/kie-wb they utilized sistem.git VFS repository to preserve deployed units across server restarts.
While that works fine, it comes with some drawbacks:

* not available for custom systems that use services
* requires complex setup in cluster - zookeeper and helix

With version 6.2 jbpm services come with deployment synchronizer that stores available deployments into data base, including its deployment descriptor.
At the same time it constantly monitors that table to keep it in sync with other installations that might be using same data source.
This is especially important when running in cluster or when jbpm console runs next to custom application and both should be able to operate on the same artifacts.

By default synchronization must be configured (when runing as core services while it is automatically enabled for ejb and cdi extensions). To configure synchronization following needs to be configured:
[source,java]
----
TransactionalCommandService commandService = new TransactionalCommandService(emf);

DeploymentStore store = new DeploymentStore();
store.setCommandService(commandService);

DeploymentSynchronizer sync = new DeploymentSynchronizer();
sync.setDeploymentService(deploymentService);
sync.setDeploymentStore(store);

DeploymentSyncInvoker invoker = new DeploymentSyncInvoker(sync, 2L, 3L, TimeUnit.SECONDS);
invoker.start();
....
invoker.stop();
----

With this, deployments will be synchronized every 3 seconds with initial delay of two seconds.

*Invoking latest version of project's processes*

In case there is a need to always work with latest version of project's process, services allow to interact with various operations using deployment id with latest keyword.
Let's go over an example to better understand the feature.

Initially deployed unit is org.jbpm:HR:1.0 which has the first version of an hiring process.
After several weeks, new version is developed and deployed to the execution server - org.jbpm:HR.2.0 with version 2 of the hiring process.

To allow callers of the services to interact without being worried if they work with latest version, they can use following deployment id:

[source]
----
org.jbpm.HR:latest
----

this will alwyas find out latest available version of project that is identified by:

* groupId: org.jbpm
* artifactId: HR

version comparizon is based on Maven version numbers and relies on Maen based algorithm to find the latest one.

[NOTE]
====
This is only supported when process identifier remains the same in all project versions
====

Here is a complete example with deployment of multiple versions and interacting always with the latest:
[source,java]
----
KModuleDeploymentUnit deploymentUnitV1 = new KModuleDeploymentUnit("org.jbpm", "HR", "1.0");
deploymentService.deploy(deploymentUnitV1);

long processInstanceId = processService.startProcess("org.jbpm:HR:LATEST", "customtask");
ProcessInstanceDesc piDesc = runtimeDataService.getProcessInstanceById(processInstanceId); 

// we have started process with project's version 1
assertEquals(deploymentUnitV1.getIdentifier(), piDesc.getDeploymentId());

// next we deploy version 1
KModuleDeploymentUnit deploymentUnitV2 = new KModuleDeploymentUnit("org.jbpm", "HR", "2.0");
deploymentService.deploy(deploymentUnitV2);

processInstanceId = processService.startProcess("org.jbpm:HR:LATEST", "customtask");
piDesc = runtimeDataService.getProcessInstanceById(processInstanceId); 

// this time we have started process with project's version 2
assertEquals(deploymentUnitV2.getIdentifier(), piDesc.getDeploymentId());
----

As illustrated this provides very powerful feature when interacting with frequently chaning environment that allows to always be up to date when it comes to use of process definitions.

[NOTE]
====
This feature is also available in REST interface so whenever sending request with deployment id, it's enough to replace concrete version with LATEST keyword to make use of this feature.
====
