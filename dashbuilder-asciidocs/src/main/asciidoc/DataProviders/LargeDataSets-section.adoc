[[_sect_dashbuilder_dataproviders_large_datasets]]
= Dealing with high volume databases


The previous sections showed how data could be loaded from plain text like CSV files or query from a database connection.
When data is small enough, Dashbuilder can handle pretty well the small data sets in memory as far as it doesn't exceed the 2MB size limit.
However, must of the time, our data sets are bigger and we can't upload all the data for Dashbuilder to handle it by its own.
Is in these cases where database backed queries can help us to implement nice drill down reports and charts without preloading all the data. 

Imagine a database containing two tables: 

.Stock trade tables
image::DataProviders/StockTradeDBTables.png[align="center"]


Now, let's take as an example a very simple example of a stock exchange dashboard which is fed from the two tables above.
The dashboard displays some indicators about several companies from several countries selling their shares at a given price on every day closing date.
The dashboard displays 4 KPIs as you can see in the following screenshot: 

.Stock trade dashboard
image::DataProviders/StockTradeDashboard.png[align="center"]


All the indicators are displaying data coming from the two database tables defined above. 

* Bar chart - Average price per company
* Area chart - Sales price evolution
* Pie chart - Companies per country
* Table report - Stock prices at closing date

What we're going to start discussing next is the two strategies we can use for building a dashboard.
This is an important aspect to consider, specially if we're facing big data scenarios. 

== The _in-memory_ strategy


This strategy consists in creating a data provider which load all the data set rows by executing a single SQL query over the two tables. 

[source,sql]
----
    SELECT C.NAME, C.COUNTRY, S.PRICE_PER_SHARE, S.CLOSING_DATE
    FROM COMPANY C JOIN STOCK S ON (C.ID=S.ID_COMPANY)
----


Every single indicator on the dashboard share the same data set.
When filters are executed from the UI no further SQLs are executed since all the calculations are done over the data set in memory. 

Pros: 

* Data integration logic keeps very simple
* Only a single data provider is needed
* Faster configuration of KPIs since all the data set properties are available at design time
* Multiple indicators from a single data provider

Cons: 

* Can't be applied on medium/large data sets due to poor performance


== The _native_ strategy


The native approach consists in having a data provider for every indicator in the dashboard instead of loading an handling all the data set in memory.
Every KPI is told what data has to display.
Every time the user filters on the dashboard, the SQLs are parsed, injected with the filter values and re-executed.
No data is hold in memory, the dashboard is always asking the DB for the data. 

The SQL data providers are the following:  

* Bar chart - Average price per company
+

[source,sql]
----
          SELECT C.NAME, AVG(S.PRICE_PER_SHARE)
          FROM COMPANY C JOIN STOCK S ON (C.ID=S.ID_COMPANY)
          WHERE {sql_condition, optional, c.country, country}
          AND {sql_condition, optional, c.name, name}
          GROUP BY C.NAME
----
* Area chart - Sales price evolution
+

[source,sql]
----
          SELECT S.CLOSING_DATE, AVG(S.PRICE_PER_SHARE)
          FROM COMPANY C JOIN STOCK S ON (C.ID=S.ID_COMPANY)
          WHERE {sql_condition, optional, c.country, country}
          AND {sql_condition, optional, c.name, name}
          GROUP BY CLOSING_DATE
----
* Pie chart - Companies per country
+

[source,sql]
----
          SELECT COUNTRY, COUNT(ID)
          FROM COMPANY
          WHERE {sql_condition, optional, country, country}
          AND {sql_condition, optional, name, name}
          GROUP BY COUNTRY
----
* Table report
+

[source,sql]
----
          SELECT C.NAME, C.COUNTRY, S.PRICE_PER_SHARE, S.CLOSING_DATE
          FROM COMPANY C JOIN STOCK S ON (C.ID=S.ID_COMPANY)
          WHERE {sql_condition, optional, c.country, country}
          AND {sql_condition, optional, c.name, name}
----

As you can see every KPI is delegating the filter & group by operations to the database.
The filter magic happens thanks to the *{sql_condition}* statements.
Every time a filter occurs in the UI the dashbuilder core gets all the SQL data providers referenced by the KPIs and it parses/injects into those SQLs the current filter selections made by the user.
The signature of the _sql_condition_ clause is the following: 

[source]
----
      {sql_condition, [optional | required], [db column], [filter property]}
----


where:

* optional: if no filter exists for the given property then the condition is ignored.
* required: if no filter is present  then the SQL returns no data.
* db column: the db column where the current filter is applied.
* filter property: the UI property which selected values are taken.


Pros: 

* Support for high volumes of data. The database tables need to be properly indexed though. 

Cons: 

* The set up of the data providers is a little bit more tricky  as it requires to create SQL queries with the required filter, group by and sort operations for every KPI. 

When designing a dashboard never forget of thinking thoroughly about the origin, type and the volume of the data we want to display in order to go for the right strategy. 
