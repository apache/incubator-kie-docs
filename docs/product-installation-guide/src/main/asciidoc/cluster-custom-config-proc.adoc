[id='cluster-custom-config-proc']

= Clustering the deployment ZIP installation

When using {EAP} clustering, a single {EAP} domain controller exists with other {EAP} slaves connecting to it as management users. You can deploy {CENTRAL} and dashbuilder as a management user on a domain controller, and the WAR deployments will be distributed to other members of the {EAP} cluster.

To configure clustering on {EAP} 7.1, do the following:

. Configure ZooKeeper and Helix according to <<_setting_up_a_cluster>>.

ifdef::BPMS[]
. Configure Quartz according to <<_setting_up_quartz>>.
endif::BPMS[]

ifdef::BPMS[]
		. Install the JDBC driver. See the https://access.redhat.com/documentation/en-US/JBoss_Enterprise_Application_Platform/6.4/html/Administration_and_Configuration_Guide/sect-JDBC_Drivers.html#Install_a_JDBC_Driver_with_the_Management_Console[Install a JDBC Driver with the Management Console] chapter of the {EAP} Administration and Configuration Guide_.
endif::BPMS[]

ifdef::BPMS[]
. Configure the data source for the server. Based on the mode you use, open `domain.xml` or `standalone.xml`, located at `_EAP_HOME_/_MODE_/configuration`.
.  Locate the `full` profile, and do the following:
+
.. Add the definition of the main data source used by {PRODUCT}.
ifdef::BPMS[]
+
.PostgreSQL Data Source Defined as Main {PRODUCT} Data Source
====
[source,xml]
----
<datasource jndi-name="java:jboss/datasources/psbpmsDS"
            pool-name="postgresDS" enabled="true" use-java-context="true">
  <connection-url>jdbc:postgresql://localhost:5432/jbpm</connection-url>
  <driver>postgres</driver>
  <security>
    <user-name>bpms</user-name>
    <password>bpms</password>
  </security>
</datasource>
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.PostgreSQL Data Source Defined as Main JBoss BRMS Data Source
====
[source,xml]
----
<datasource jndi-name="java:jboss/datasources/psbrmsDS"
            pool-name="postgresDS" enabled="true" use-java-context="true">
  <connection-url>jdbc:postgresql://localhost:5432/jbrm</connection-url>
  <driver>postgres</driver>
  <security>
    <user-name>brms</user-name>
    <password>brms</password>
  </security>
</datasource>
----
====
endif::BRMS[]
.. Add the definition of the data source for the Quartz service.
ifdef::BPMS[]
+
.PostgreSQL Data Source Defined as Quartz Data Source
====
[source,xml]
----
<datasource jta="false" jndi-name="java:jboss/datasources/quartzNotManagedDS"
            pool-name="quartzNotManagedDS" enabled="true" use-java-context="true">
  <connection-url>jdbc:postgresql://localhost:5432/jbpm</connection-url>
  <driver>postgres</driver>
  <security>
    <user-name>bpms</user-name>
    <password>bpms</password>
  </security>
</datasource>
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.PostgreSQL Data Source Defined as Quartz Data Source
====
[source,xml]
----
<datasource jta="false" jndi-name="java:jboss/datasources/quartzNotManagedDS"
            pool-name="quartzNotManagedDS" enabled="true" use-java-context="true">
  <connection-url>jdbc:postgresql://localhost:5432/jbrm</connection-url>
  <driver>postgres</driver>
  <security>
    <user-name>brms</user-name>
    <password>brms</password>
  </security>
</datasource>
----
====
endif::BRMS[]
.. Define the data source driver.
+
.PostgreSQL Driver Definition
====
[source,xml]
----
<driver name="postgres" module="org.postgresql">
  <xa-datasource-class>org.postgresql.xa.PGXADataSource</xa-datasource-class>
</driver>
----
====

.. If you are deploying {PRODUCT} on {EAP} 7.1, ensure that the data sources contain schemas. To create the data source schemas, you can use the DDL scripts located in `jboss-bpmsuite-brms-{PRODUCT_VERSION}-supplementary-tools.zip`. If your data source does not contain schemas, ensure your nodes start one at a time.
+
Additionally, when deploying on Red Hat {EAP} 7.1, open `_EAP_HOME_/domain/business-central.war/WEB-INF/classes/META-INF/persistence.xml` and change the property `hibernate.hbm2ddl.auto="update"` to `hibernate.hbm2ddl.auto="none"`.
endif::BPMS[]
. Configure individual server nodes in the `main-server-group` element in the `_EAP_HOME_/domain/configuration/host.xml` file with properties defined in
ifdef::BPMS[]
<<_cluster_properties_BPMS>>.
endif::BPMS[]
ifdef::BRMS[]
<<_cluster_properties_BRMS>>.
endif::BRMS[]
+
When configuring a {EAP} cluster with Apache ZooKeeper, a different number of {EAP} nodes than Apache ZooKeeper nodes is possible. However, having the same node count for both ZooKeeper and {EAP} is considered best practice.
ifdef::BPMS[]
+
[id='_cluster_properties_BPMS']
.Cluster Node Properties
[cols="1,1,2", frame="all", options="header"]
|===
|Property Name
|Value
|Description

|jboss.node.name
|nodeOne
|Node name unique within the cluster.

|org.quartz.properties
|/bpms/quartz-definition.properties
|Absolute path to the Quartz configuration file.

|org.uberfire.cluster.id
|bpms-cluster
|Helix cluster name.

|org.uberfire.cluster.local.id
|nodeOne_12345
|Unique ID of the Helix cluster node. Note that `:` is replaced with `_`.

|org.uberfire.cluster.vfs.lock
|vfs-repo
|Name of the resource defined on the Helix cluster.

|org.uberfire.cluster.zk
|server1:2181
|ZooKeeper location.

|org.uberfire.metadata.index.dir
|/home/jbpm/node[N]/index
|Location where the index for search is to be created (maintained by Apache Lucene).

|org.uberfire.nio.git.daemon.host
|nodeOne
|The name of the daemon host machine in a physical cluster.

|org.uberfire.nio.git.daemon.port
|9418
|Port used by the VFS repo to accept client connections. The port must be unique for each cluster member.

|org.uberfire.nio.git.dir
|/home/jbpm/node[N]/repo
|Git (VFS) repository location on node[N].

|org.uberfire.nio.git.ssh.host
|nodeOne
|The name of the SSH host machine in a physical cluster.

|org.uberfire.nio.git.ssh.port
|8003
|The unique port number for ssh access to the GIT repo for a cluster running on physical machines.

|org.uberfire.nio.git.ssh.hostport and org.uberfire.nio.git.daemon.hostport
|8003 and 9418
|In a virtualized environment, the outside port to be used.
|===
endif::BPMS[]
ifdef::BRMS[]
+
[id='_cluster_properties_BRMS']
.Cluster Node Properties
[cols="1,1,2", frame="all", options="header"]
|===
|Property Name
|Value
|Description

|org.uberfire.nio.git.dir
|/home/jbrm/node[N]/repo
|Git (VFS) repository location on node[N].

|jboss.node.name
|nodeOne
|Node name unique within the cluster.

|org.uberfire.cluster.id
|brms-cluster
|Helix cluster name.

|org.uberfire.cluster.zk
|server1:2181
|ZooKeeper location.

|org.uberfire.cluster.local.id
|nodeOne_12345
|Unique ID of the Helix cluster node. Note that `:` is replaced with `_`.

|org.uberfire.cluster.vfs.lock
|vfs-repo
|Name of the resource defined on the Helix cluster.

|org.uberfire.nio.git.daemon.port
|9418
|Port used by the VFS repo to accept client connections. The port must be unique for each cluster member.

|org.uberfire.metadata.index.dir
|/home/jbrm/node[N]/index
|Location where the index for search is to be created (maintained by Apache Lucene).

|org.uberfire.nio.git.ssh.port
|8003
|The unique port number for ssh access to the Git repo for a cluster running on physical machines.

|org.uberfire.nio.git.daemon.host
|nodeOne
|The name of the daemon host machine in a physical cluster.

|org.uberfire.nio.git.ssh.host
|nodeOne
|The name of the SSH host machine in a physical cluster.

|org.uberfire.nio.git.ssh.hostport and org.uberfire.nio.git.daemon.hostport
|8003 and 9418
|In a virtualized environment, the outside port to be used.
|===
endif::BRMS[]

ifdef::BPMS[]
+
.Cluster nodeOne Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/bpms/nodeone"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeOne" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="bpms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2182,server3:2183" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeOne_12345"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeOne"/>
  <property name="org.uberfire.nio.git.daemon.port" value="9418" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9418"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.hostport" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.host" value="nodeOne"/>
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbpm/nodeone"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodeone"
            boot-time="false"/>
  <property name="org.quartz.properties"
            value="/tmp/jbpm/quartz/quartz-db-postgres.properties" boot-time="false"/>
</system-properties>
----
====
endif::BPMS[]
ifdef::BPMS[]
+
.Cluster nodeTwo Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/bpms/nodetwo"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeTwo" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="bpms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2182,server3:2183" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeTwo_12346"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeTwo" />
  <property name="org.uberfire.nio.git.daemon.port" value="9419" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9419"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8004" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.hostport" value="8004" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.host" value="nodeTwo" />
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbpm/nodetwo"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodetwo"
            boot-time="false"/>
  <property name="org.quartz.properties"
            value="/tmp/jbpm/quartz/quartz-db-postgres.properties" boot-time="false"/>
</system-properties>
----
====
endif::BPMS[]
ifdef::BPMS[]
+
.Cluster nodeThree Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/bpms/nodethree"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeThree" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="bpms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2182,server3:2183" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeThree_12347"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeThree" />
  <property name="org.uberfire.nio.git.daemon.port" value="9420" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9420"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8005" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.hostport" value="8005" boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.host" value="nodeThree" />
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbpm/nodethree"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodethree"
            boot-time="false"/>
  <property name="org.quartz.properties"
            value="/tmp/jbpm/quartz/quartz-db-postgres.properties" boot-time="false"/>
</system-properties>
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.Cluster nodeOne Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/brms/nodeone"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeOne" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="brms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2181,server3:2181" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeOne_12345"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.port" value="9418" boot-time="false"/>
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbrm/nodeone"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodeone"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeOne" />
  <property name="org.uberfire.nio.git.ssh.host" value="nodeOne" />
  <property name="org.uberfire.nio.git.ssh.hostport" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9418"
            boot-time="false"/>
</system-properties>
----
====
endif::BRMS[]
ifdef::BRMS[]
+
.Cluster nodeTwo Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/brms/nodetwo"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeTwo" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="brms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2182,server3:2183" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeTwo_12346"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.port" value="9418" boot-time="false"/>
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbrm/nodetwo"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodetwo"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeTwo" />
  <property name="org.uberfire.nio.git.ssh.host" value="nodeTwo" />
  <property name="org.uberfire.nio.git.ssh.hostport" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9418"
            boot-time="false"/>
</system-properties>
----
====
endif::BRMS[]
ifdef::BRMS[]
+
.Cluster nodeThree Configuration
====
[source,xml]
----
<system-properties>
  <property name="org.uberfire.nio.git.dir" value="/tmp/brms/nodethree"
            boot-time="false"/>
  <property name="jboss.node.name" value="nodeThree" boot-time="false"/>
  <property name="org.uberfire.cluster.id" value="brms-cluster" boot-time="false"/>
  <property name="org.uberfire.cluster.zk"
            value="server1:2181,server2:2182,server3:2183" boot-time="false"/>
  <property name="org.uberfire.cluster.local.id" value="nodeThree_12347"
            boot-time="false"/>
  <property name="org.uberfire.cluster.vfs.lock" value="vfs-repo" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.port" value="9418" boot-time="false"/>
  <property name="org.uberfire.metadata.index.dir" value="/tmp/jbrm/nodethree"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.cert.dir" value="/tmp/jbpm/nodethree"
            boot-time="false"/>
  <property name="org.uberfire.nio.git.ssh.port" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.host" value="nodeThree" />
  <property name="org.uberfire.nio.git.ssh.host" value="nodeThree" />
  <property name="org.uberfire.nio.git.ssh.hostport" value="8003" boot-time="false"/>
  <property name="org.uberfire.nio.git.daemon.hostport" value="9418"
            boot-time="false"/>
</system-properties>
----
====
endif::BRMS[]

. Add management users as instructed in the _Administration and Configuration Guide_ for {EAP} and application users as instructed in _{PRODUCT} Administration and Configuration Guide_.
// change on purpose - cd = change directory
. Change to `_EAP_HOME_/bin` and start the application server in domain mode:
+
On UNIX systems:
+
----
./domain.sh
----
+
On Windows:
+
----
./domain.bat
----

. Check that the nodes are available.

Deploy the {CENTRAL} application to your servers:

ifdef::BPMS[]
. Change the predefined persistence of the application to the required database (PostgreSQL): in `persistence.xml` change the following:
+
.. `jta-data-source` name to the source defined on the application server (`java:jboss/datasources/psbpmsDS`).
.. Hibernate dialect to be match the data source dialect (`org.hibernate.dialect.PostgreSQLDialect`).

. Log in as the management user to the server *Administration* console of your domain and add the new deployments using the *Runtime view* of the console. Once the deployment is added to the domain, assign it to the correct server group (`main-server-group`).
endif::BPMS[]

ifdef::BRMS[]
* Log in as the management user to the server *Administration* console of your domain and add the new deployments using the *Runtime view* of the console. Once the deployment is added to the domain, assign it to the correct server group (`main-server-group`).
endif::BRMS[]

[NOTE]
====
It is important users explicitly check deployment unit readiness with every cluster member.

When a deployment unit is created on a cluster node, it takes some time before it is distributed among all cluster members. Deployment status can be checked using the UI and REST, however, if the query goes to the node where the deployment was originally issued, the answer is `deployed`. Any request targeting this deployment unit sent to a different cluster member fails with `DeploymentNotFoundException`.
====

ifdef::BPMS[]
[id='_exec_server']
=== Clustering the Intelligent Process Server
endif::BPMS[]
ifdef::BRMS[]
[id='_exec_server']
=== Clustering the Realtime {KIE_SERVER}
endif::BRMS[]

The
ifdef::BPMS[]
Intelligent Process Server
endif::BPMS[]
ifdef::BRMS[]
Realtime {KIE_SERVER}
endif::BRMS[]
is a lightweight and scalable component. Clustering it provides many benefits. For example:

* You can partition your resources based on deployed containers.
* You can scale individual instances independently from each other.
* You can distribute the cluster across network and manage it by a single controller.
** The controller can be clustered into a ZooKeeper ensemble.
* No further components are required.

The basic runtime cluster consists of:

* Multiple {EAP} instances with
ifdef::BPMS[]
Intelligent Process Server
endif::BPMS[]
ifdef::BRMS[]
Realtime {KIE_SERVER}
endif::BRMS[]
* A controller instance with {CENTRAL}

image::kieserver-arch.png[]

This section describes how to start
ifdef::BPMS[]
Intelligent Process Server
endif::BPMS[]
ifdef::BRMS[]
Realtime {KIE_SERVER}
endif::BRMS[]
cluster on {EAP} 6.4.

ifdef::BPMS[]
.Creating an Intelligent Process Server Cluster
endif::BPMS[]
ifdef::BRMS[]
.Creating a Realtime {KIE_SERVER} Cluster
endif::BRMS[]
. Change into `_CONTROLLER_HOME_/bin`.
. Add a user with the `kie-server` role:
+
[source,bash]
----
$ ./add-user.sh -a --user kieserver --password kieserver1! --role kie-server
----
. Start your controller:
+
[source,bash]
----
$ ./standalone.sh
----
. Change into `_SERVER_1_HOME_`.
. Deploy `kie-server.war`. Clustered servers do not need `business-central.war` or other applications.
. See the `<servers>` part of the following `host.xml` as an example of required properties:
+
[source,xml]
----
<server name="server-one" group="main-server-group">
 <system-properties>
  <property name="org.kie.server.location" value="http://localhost:8180/kie-server/services/rest/server"></property> <1>
  <property name="org.kie.server.controller" value="http://localhost:8080/business-central/rest/controller"></property> <2>
  <property name="org.kie.server.controller.user" value="kieserver"></property> <3>
  <property name="org.kie.server.controller.pwd" value="kieserver1!"></property> <4>
  <property name="org.kie.server.id" value="HR"></property> <5>
 </system-properties>
 <socket-bindings port-offset="100"/>
</server>

<server name="server-two" group="main-server-group" auto-start="true">
 <system-properties>
  <property name="org.kie.server.location" value="http://localhost:8230/kie-server/services/rest/server"></property>
  <property name="org.kie.server.controller" value="http://localhost:8080/business-central/rest/controller"></property>
  <property name="org.kie.server.controller.user" value="kieserver"></property>
  <property name="org.kie.server.controller.pwd" value="kieserver1!"></property>
  <property name="org.kie.server.id" value="HR"></property>
 </system-properties>
 <socket-bindings port-offset="150"/>
</server>
----
<1> `org.kie.server.location`: URL of the server instance.
<2> `org.kie.server.controller`: Comma-separated list of the controller URL(s).
<3> `org.kie.server.controller.user`: Username you created for controller authentication. Uses `kieserver` by default.
<4> `org.kie.server.controller.pwd`: Password for controller authentication. Uses `kieserver1!` by default.
<5> `org.kie.server.id`: Server identifier that corresponds to template ID defined by the controller instance. Give the same ID to multiple server instances that represent one template.
+
The example above is defined for {EAP} domain mode. For further list of bootstrap switches, see section
ifdef::BPMS[]
https://access.qa.redhat.com/documentation/en/red-hat-jboss-bpm-suite/6.4/paged/administration-and-configuration-guide/chapter-3-intelligent-process-server#bootstrap_switches[Bootstrap Switches] of the _{PRODUCT} Administration and Configuration Guide_.
endif::BPMS[]
ifdef::BRMS[]
https://access.qa.redhat.com/documentation/en/red-hat-jboss-brms/6.4/paged/administration-and-configuration-guide/chapter-3-realtime-decision-server#bootstrap_switches[Bootstrap Switches] of _{PRODUCT} Administration and Configuration Guide_.
endif::BRMS[]
. Repeat the previous step for as many servers as you need. To start {EAP} in the domain mode, execute:
[source,bash]
----
$ ./SERVER_HOME/bin/domain.sh
----

After connecting the servers to your controller, check the controller log:
[source]
----
13:54:40,315 INFO  [org.kie.server.controller.impl.KieServerControllerImpl] (http-localhost/127.0.0.1:8080-1) Server http://localhost:8180/kie-server/services/rest/server connected to controller
13:54:40,331 INFO  [org.kie.server.controller.impl.KieServerControllerImpl] (http-localhost/127.0.0.1:8080-2) Server http://localhost:8230/kie-server/services/rest/server connected to controller
13:54:40,348 INFO  [org.kie.server.controller.rest.RestKieServerControllerImpl] (http-localhost/127.0.0.1:8080-1) Server with id 'HR' connected
13:54:40,348 INFO  [org.kie.server.controller.rest.RestKieServerControllerImpl] (http-localhost/127.0.0.1:8080-2) Server with id 'HR' connected
----

Alternatively, to verify in controller {CENTRAL}:

. Log into the controller {CENTRAL}.
. Click *Deploy* -> *{KIE_SERVER}s*.
. View the remote servers connected to each template.


[id='_generic_bundle_clustering_setup']
== Generic Bundle Clustering

[id='_setting_up_a_cluster']
=== Setting a Cluster

[NOTE]
====
If you do not use {CENTRAL}, skip this section.
====

To cluster your Git (VFS) repository in {CENTRAL}:

. Download the `jboss-bpmsuite-brms-_VERSION_-supplementary-tools.zip`, which contains Apache ZooKeeper, Apache Helix, and Quartz DDL scripts.
. Unzip the archive: the `ZooKeeper` directory (`_ZOOKEEPER_HOME_`) and the `Helix` directory (`_HELIX_HOME_`) are created.

. Configure Apache ZooKeeper:
+
.. In the ZooKeeper directory, change to `conf` and execute:
+
[source]
----
cp zoo_sample.cfg zoo.cfg
----
.. Edit `zoo.cfg`:
+
[source]
----
# The directory where the snapshot is stored.
dataDir=$ZOOKEEPER_HOME/data/

# The port at which the clients connects.
clientPort=2181

# Defining ZooKeeper ensemble.
# server.{ZooKeeperNodeID}={server}:{port:range}
server.1=localhost:2888:3888
server.2=localhost:2889:3889
server.3=localhost:2890:3890
----
+
NOTE: Multiple ZooKeeper nodes are not required for clustering.
+

Make sure the `dataDir` location exists and is accessible.
.. Assign a node ID to each member that will run ZooKeeper. For example, use `1`, `2`, and `3` for node 1, node 2 and node 3 respectively.
+
The ZooKeeper node ID is specified in a field called `myid` under the data directory of ZooKeeper on each node. For example, on node 1, execute:
+
[source]
----
echo "1" > /zookeeper/data/myid
----

. Provide further ZooKeeper configuration if necessary.
+
. Change to `_ZOOKEEPER_HOME_/bin/` and start ZooKeeper:
+
[source]
----
./zkServer.sh start
----
+
You can check the ZooKeeper log in the `_ZOOKEEPER_HOME_/bin/zookeeper.out` file. Check this log to ensure that the ensemble (cluster) is formed successfully. One of the nodes should be elected as leader with the other two nodes following it.

. Once the ZooKeeper ensemble is started, configure and start Helix. Helix needs to be configured from a single node only. The configuration is then stored by the ZooKeeper ensemble and shared as appropriate.
+
Configure the cluster with the ZooKeeper server as the master of the configuration:
+
.. Create the cluster by providing the ZooKeeper Host and port as a comma-separated list:
+
[source]
----
$HELIX_HOME/bin/helix-admin.sh --zkSvr ZOOKEEPER_HOST:ZOOKEEPER_PORT --addCluster <clustername>
----
.. Add your nodes to the cluster:
+
[source]
----
HELIX_HOME/bin/helix-admin.sh --zkSvr ZOOKEEPER_HOST:ZOOKEEPER_PORT --addNode <clustername>:<name_uniqueID>
----
ifdef::BPMS[]
+
.Adding Three Cluster Nodes
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode bpms-cluster nodeOne:12345
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode bpms-cluster nodeTwo:12346
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode bpms-cluster nodeThree:12347
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.Adding Two Cluster Nodes
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode brms-cluster nodeOne:12345
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode brms-cluster nodeTwo:12346
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addNode brms-cluster nodeThree:12347
----
====
endif::BRMS[]

. Add resources to the cluster.
+
[source]
----
helix-admin.sh --zkSvr ZOOKEEPER_HOST:ZOOKEEPER_PORT  --addResource <clustername> <resourceName> <numPartitions> <stateModelName>
----
+
Learn more about state machine configuration at http://helix.apache.org/0.6.5-docs/tutorial_state.html[Helix Tutorial: State Machine Configuration].
ifdef::BPMS[]
+
.Adding vfs-repo as Resource
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addResource bpms-cluster vfs-repo 1 LeaderStandby AUTO_REBALANCE
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.Adding vfs-repo as Resource
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --addResource brms-cluster vfs-repo 1 LeaderStandby AUTO_REBALANCE
----
====
endif::BRMS[]

. Rebalance the cluster with the three nodes.
+
[source]
----
helix-admin.sh --zkSvr ZOOKEEPER_HOST:ZOOKEEPER_PORT --rebalance <clustername> <resourcename> <replicas>
----
+
Learn more about rebalancing at http://helix.apache.org/0.6.5-docs/tutorial_rebalance.html[Helix Tutorial: Rebalancing Algorithms].
ifdef::BPMS[]
+
.Rebalancing bpms-cluster
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --rebalance bpms-cluster vfs-repo 3
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.Rebalancing brms-cluster
====
[source]
----
./helix-admin.sh --zkSvr server1:2181,server2:2182,server3:2183 --rebalance brms-cluster vfs-repo 3
----
====
endif::BRMS[]
+
In this command, `3` stands for three ZooKeeper nodes.

. Start the Helix controller in all the nodes in the cluster.
ifdef::BPMS[]
+
.Starting Helix Controller
====
[source]
----
./run-helix-controller.sh --zkSvr server1:2181,server2:2182,server3:2183 --cluster bpms-cluster 2>&1 > ./controller.log &
----
====
endif::BPMS[]
ifdef::BRMS[]
+
.Starting Helix Controller
====
[source]
----
./run-helix-controller.sh --zkSvr server1:2181,server2:2182,server3:2183 --cluster brms-cluster 2>&1 > ./controller.log &
----
====
endif::BRMS[]

NOTE: In case you decide to cluster ZooKeeper, add an odd number of instances in order to recover from failure. After a failure, the remaining number of nodes still need to be able to form a majority. For example a cluster of five ZooKeeper nodes can withstand loss of two nodes in order to fully recover. One ZooKeeper instance is still possible, replication will work, however no recover possibilities are available if it fails.


=== Starting and Stopping a Cluster

To start your cluster, see COMMENT: add xref starting_cluster>>. To stop your cluster, see COMMENT: add xref.

ifdef::BPMS[]

[id='_setting_up_quartz']
=== Setting Quartz

NOTE: If you are not using Quartz (timers) in your business processes, or if you are not using the
ifdef::BPMS[]
Intelligent Process Server,
endif::BPMS[]
ifdef::BRMS[]
Realtime {KIE_SERVER},
endif::BRMS[]
skip this section. If you want to replicate timers in your business process, use the Quartz component.

Before you can configure the database on your application server, you need to prepare the database for Quartz to create Quartz tables, which will hold the timer data, and the Quartz definition file.

To configure Quartz:

. Configure the database. Make sure to use one of the supported non-JTA data sources. Since Quartz needs a non-JTA data source, you cannot use the {CENTRAL} data source. In the example code, PostgreSQL with the user
ifdef::BPMS[]
`bpms`
endif::BPMS[]
ifdef::BRMS[]
`brms`
endif::BRMS[]
and password
ifdef::BPMS[]
`bpms`
endif::BPMS[]
ifdef::BRMS[]
`brms`
endif::BRMS[]
is used.
ifdef::BPMS[]
The database must be connected to your application server.
endif::BPMS[]

. Create Quartz tables on your database to allow timer events synchronization. To do so, use the DDL script for your database, which is available in the extracted supplementary ZIP archive in `_QUARTZ_HOME_/docs/dbTables`.

. Create the Quartz configuration file `quartz-definition.properties` in `_JBOSS_HOME_/_MODE_/configuration/` directory and define the Quartz properties.
ifdef::BPMS[]
+
.Quartz Configuration File for PostgreSQL Database
====
[source]
----
#============================================================================
# Configure Main Scheduler Properties
#============================================================================

org.quartz.scheduler.instanceName = jBPMClusteredScheduler
org.quartz.scheduler.instanceId = AUTO

#============================================================================
# Configure ThreadPool
#============================================================================

org.quartz.threadPool.class = org.quartz.simpl.SimpleThreadPool
org.quartz.threadPool.threadCount = 5
org.quartz.threadPool.threadPriority = 5

#============================================================================
# Configure JobStore
#============================================================================

org.quartz.jobStore.misfireThreshold = 60000

org.quartz.jobStore.class=org.quartz.impl.jdbcjobstore.JobStoreCMT
org.quartz.jobStore.driverDelegateClass=org.quartz.impl.jdbcjobstore.PostgreSQLDelegate
org.quartz.jobStore.useProperties=false
org.quartz.jobStore.dataSource=managedDS
org.quartz.jobStore.nonManagedTXDataSource=notManagedDS
org.quartz.jobStore.tablePrefix=QRTZ_
org.quartz.jobStore.isClustered=true
org.quartz.jobStore.clusterCheckinInterval = 20000

#============================================================================
# Configure Datasources
#============================================================================
org.quartz.dataSource.managedDS.jndiURL=jboss/datasources/psbpmsDS
org.quartz.dataSource.notManagedDS.jndiURL=jboss/datasources/quartzNotManagedDS
----
Note the configured data sources that will accommodate the two Quartz schemes at the very end of the file.
====
endif::BPMS[]
ifdef::BRMS[]
+
.Quartz Configuration File for PostgreSQL Database
====
[source]
----
#============================================================================
# Configure Main Scheduler Properties
#============================================================================

org.quartz.scheduler.instanceName = jBPMClusteredScheduler
org.quartz.scheduler.instanceId = AUTO

#============================================================================
# Configure ThreadPool
#============================================================================

org.quartz.threadPool.class = org.quartz.simpl.SimpleThreadPool
org.quartz.threadPool.threadCount = 5
org.quartz.threadPool.threadPriority = 5

#============================================================================
# Configure JobStore
#============================================================================

org.quartz.jobStore.misfireThreshold = 60000

org.quartz.jobStore.class=org.quartz.impl.jdbcjobstore.JobStoreCMT
org.quartz.jobStore.driverDelegateClass=org.quartz.impl.jdbcjobstore.PostgreSQLDelegate
org.quartz.jobStore.useProperties=false
org.quartz.jobStore.dataSource=managedDS
org.quartz.jobStore.nonManagedTXDataSource=notManagedDS
org.quartz.jobStore.tablePrefix=QRTZ_
org.quartz.jobStore.isClustered=true
org.quartz.jobStore.clusterCheckinInterval = 20000

#============================================================================
# Configure Datasources
#============================================================================
org.quartz.dataSource.managedDS.jndiURL=jboss/datasources/psbrmsDS
org.quartz.dataSource.notManagedDS.jndiURL=jboss/datasources/quartzNotManagedDS
----
Note the configured datasources that will accommodate the two Quartz schemes at the very end of the file.
====
endif::BRMS[]
+
[IMPORTANT]
.Cluster Node Check Interval
====
The recommended interval for cluster discovery is 20 seconds and is set in the `org.quartz.jobStore.clusterCheckinInterval` of the `quartz-definition.properties` file. Depending on your set up consider the performance impact and modify the setting as necessary.
====
+
The `org.quartz.jobStore.driverDelegateClass` property that defines the database dialect. If you use Oracle, set it to `org.quartz.impl.jdbcjobstore.oracle.OracleDelegate`.

. Provide the absolute path to your `quartz-definition.properties` file in the `org.quartz.properties` property. For further details, see <<_cluster_properties_BPMS>>.
endif::BPMS[]
